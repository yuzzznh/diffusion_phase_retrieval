# My Project: LatentDAPSë¡œ Langevin Dynamics sampling + TDP-style íƒìƒ‰ìœ¼ë¡œ 0Â° 180Â° ì°¾ê¸° + ë§¨ ë§ˆì§€ë§‰ hard data consistency ì ìš©

## ì‹¤í—˜ë³„ ëª…ë ¹ì–´

``` bash
# ============================================================
# GPU (CUDA) ëª…ë ¹ì–´ - commands_gpu/ í´ë”
# ============================================================
# ì‹¤í—˜ 0
bash commands_gpu/exp0_baseline.sh --1           # 1 image sanity check -> A10ì—ì„œ 150/150W 12292MiB 100% 15min. í•˜ì§€ë§Œ ë¡œê·¸ì—” Peak Memory: 10116.54 MB ë¼ê³  ê¸°ë¡ë¨.
bash commands_gpu/exp0_baseline.sh --10          # 10 images -> A10ì—ì„œ 2.5ì‹œê°„ ê±¸ë¦´ ë“¯.
bash commands_gpu/exp0_baseline.sh --90          # 90 images (10~99) -> --10ê³¼ í•©ì³ì„œ 100ê°œ. A10ì—ì„œ 150/150W 12536MB 100% 15min per image ëœ¸. ë‹¤ ëŒë¦¬ë©´ 22.5ì‹œê°„ ì˜ˆìƒ.
bash commands_gpu/exp0_baseline.sh --1 --10      # 1 + 10 images ìˆœì°¨ ì‹¤í–‰

# ì‹¤í—˜ 1~4
bash commands_gpu/exp1_repulsion.sh --1 --10 --90
bash commands_gpu/exp2_pruning.sh --1 --10 --90
bash commands_gpu/exp3_2particle.sh --10 --90     # (1 image ì—†ìŒ)
bash commands_gpu/exp4_optimization.sh --1 --10 --90

# ì‹¤í—˜ 5
bash commands_gpu/exp5_final.sh --imagenet        # ImageNet 100
bash commands_gpu/exp5_final.sh --ffhq            # FFHQ 100
bash commands_gpu/exp5_final.sh --imagenet --ffhq # ë‘˜ ë‹¤

# ============================================================
# ì¸ì ì—†ì´ ì‹¤í–‰í•˜ë©´ ì‚¬ìš©ë²• ì¶œë ¥:
# ============================================================
$ bash commands_gpu/exp0_baseline.sh
# ì‚¬ìš©ë²•: bash exp0_baseline.sh [--1] [--10] [--90]
# --1   : 1 image sanity check (ì´ë¯¸ì§€ 0)
# --10  : 10 images main experiment (ì´ë¯¸ì§€ 0~9)
# --90  : 90 images final eval (ì´ë¯¸ì§€ 10~99, --10ê³¼ í•©ì³ì„œ 100ê°œ)

## ì‹¤í—˜ ì§„í–‰ ë° êµ¬í˜„ ê³¼ì • ì„¤ê³„

### [ë°ì´í„°] imagenet 10ì¥ìœ¼ë¡œ method ë¹„êµ, ë§ˆì§€ë§‰ evalì€ ffhq imagenet 100ì¥ì”©ìœ¼ë¡œ í•˜ëŠ”ê±¸ ëª©í‘œë¡œ, ì—¬ê±´ ì•ˆë˜ë©´ ffhqëŠ” ë²„ë¦¬ê¸° / ì‹œë“œ ê³ ì • (ì´ë¯¸ DAPSì—ì„œëŠ” 42)

### [ì‹¤í—˜ 0] LatentDAPS ë…¼ë¬¸ì— eval ë°ì´í„°ëŠ” 100 imageì—ë§Œ ë‚˜ì™€ìˆìœ¼ë‹ˆê¹Œ ë¹„êµë¥¼ ìœ„í•´ LatentDAPS(with Langevin Dynamic)ì˜ imagenet first 10 imageì— ëŒ€í•œ phase retrieval ì„±ëŠ¥ ì¸¡ì •.
- ~~ë‹¨, ì´ë•Œ imageë³„ë¡œ ì „ë¶€ ëŒì•„ê°„ ë’¤ ë‹¤ìŒ runì´ ì‹¤í–‰ë˜ëŠ” êµ¬ì¡°ë¡œ 4 runì´ êµ¬í˜„ë¼ìˆëŠ”ë°, ì´í›„ ì‹¤í—˜ë“¤ê³¼ì˜ ì›í™œí•œ ë¹„êµë¥¼ ìœ„í•´ eval ëª…ë ¹ì–´ë¥¼ 4 batch = 4 run êµ¬ì¡°ë¡œ ë³€ê²½í•´ì•¼ í•¨.~~
- ~~time logging: diffusion timestep Të¥¼ êµ¬ê°„ê°œìˆ˜ë¡œ í•˜ì—¬ **timestepë³„ ì†Œìš” ì‹œê°„**ì„ ì¸¡ì •. ì´í›„ ì‹¤í—˜ì—ì„œ pruning/optimization ì‹œì  ì „í›„ ì‹œê°„ ë¹„êµì— í™œìš©. sanity check ì°¨ì›ì—ì„œ 1 image 4 sample ëª…ë ¹ì–´ë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸í•  ê²ƒ.~~ â†’ **ì™„ë£Œ**: `sampler.py`ì˜ `LatentDAPS.sample()`ì—ì„œ stepë³„ ì‹œê°„ ì¸¡ì • (`self.timing_info`ì— ì €ì¥), `posterior_sample.py`ì—ì„œ ì´ë¯¸ì§€ë³„ timing ì§‘ê³„ í›„ `metrics.json`ì— ì €ì¥.
- ~~GPU VRAM logging: ì‹¤í—˜ 0ì—ì„œëŠ” phase êµ¬ë¶„ ì—†ì´ **ì „ì²´ êµ¬ê°„ì˜ peak VRAM**ë§Œ ì¸¡ì •. `torch.cuda.max_memory_allocated()` í™œìš©.~~ â†’ **ì™„ë£Œ**: `posterior_sample.py`ì—ì„œ `torch.cuda.reset_peak_memory_stats()` í›„ `torch.cuda.max_memory_allocated()` ì¸¡ì •, `metrics.json`ì˜ `metadata.gpu.peak_vram_mb`ì— ì €ì¥. (phaseë³„ êµ¬ê°„ ë¶„ë¦¬ëŠ” ì‹¤í—˜ 2, 4ì—ì„œ pruning/optimization ì¶”ê°€ ì‹œ êµ¬í˜„)
- ~~ëª…ë ¹ì–´ ìë™ê¸°ë¡ ë©”ì»¤ë‹ˆì¦˜ì´ ì´ë¯¸ ìˆëŠ”ê±¸ë¡œ ì•„ëŠ”ë°, ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ íŒŒì•…í•˜ê³ , ìš°ë¦¬ ì‹¤í—˜ 0~5ì˜ ê°ì¢… argument ì„¸íŒ…ì´ ì˜ ê¸°ë¡ë˜ë„ë¡ ì½”ë“œë¥¼ ìˆ˜ì •í•  ê²ƒ.~~ â†’ **Hydra ê¸°ë°˜ config ìë™ê¸°ë¡ í™•ì¸ ì™„ë£Œ**: `posterior_sample.py`ì—ì„œ `OmegaConf.to_container(args)`ë¥¼ í†µí•´ ëª¨ë“  configê°€ mergeëœ ìµœì¢… ê²°ê³¼ë¥¼ `results/<name>/config.yaml`ì— ìë™ ì €ì¥í•¨. sh ëª…ë ¹ì–´ì—ì„œ overrideí•œ ëª¨ë“  argumentê°€ ê¸°ë¡ë¨.

### [ì‹¤í—˜ 1] 4-Particle Full Run (Repulsion vs. Independence) â†’ **êµ¬í˜„ ì™„ë£Œ**

#### Sampler ì½”ë“œ ë¶„ì„ (ì¤€ë¹„)
| í•­ëª©         | ì´ ì½”ë“œ                  |
|--------------|--------------------------|
| íŒŒë¼ë¯¸í„°í™”   | EDM Ïƒ (sigma)            |
| ì˜ˆì¸¡ íƒ€ê²Ÿ    | xâ‚€-prediction (denoiser) |
| Îµ-prediction | âŒ ì•„ë‹˜                  |

- Forward diffusion: `x_t = x_0 + ÏƒÂ·Îµ` (EDM í˜•íƒœ)
- `DiffusionPFODE.derivative()`ì—ì„œ `model.score()` í˜¸ì¶œ â†’ score = (D(x;Ïƒ) - x) / ÏƒÂ² ë³€í™˜ ì‚¬ìš©
- ë³€ìˆ˜ëª… `x0hat`, `z0hat`ì´ xâ‚€ ì˜ˆì¸¡ì„ì„ ëª…ì‹œ

#### êµ¬í˜„ ì™„ë£Œ ì‚¬í•­ (RLSD â†’ LatentDAPS ì´ì‹)
- ~~`repulsion.py` ëª¨ë“ˆ ìƒì„±~~: **ì™„ë£Œ**
  - `DinoFeatureExtractor`: DINO-ViT ëª¨ë¸ lazy loading ë° feature ì¶”ì¶œ
  - `compute_repulsion_gradient()`: SVGD-style repulsion gradient ê³„ì‚° (RBF kernel + median heuristic bandwidth)
  - `RepulsionModule`: High-level repulsion ê´€ë¦¬ (scale decay, metrics tracking)
  - **N=2 ë²„ê·¸ ìˆ˜ì •**: `h = median(dist)^2 / max(log(N), eps)` ì‚¬ìš© (RLSDì˜ `log(N-1)` ëŒ€ì‹ )
- ~~`DiffusionPFODE` ìˆ˜ì •~~: **ì™„ë£Œ**
  - `set_repulsion(repulsion, scale)` ë©”ì„œë“œ ì¶”ê°€
  - `derivative()`ì—ì„œ score-level injection: `score' = score + scale * repulsion`
- ~~`LatentDAPS.sample()` ìˆ˜ì •~~: **ì™„ë£Œ**
  - Repulsion module ì´ˆê¸°í™” ë° ê° annealing stepì—ì„œ repulsion ê³„ì‚°
  - pfodeì— repulsion ì „ë‹¬ ë° metrics ìˆ˜ì§‘
- ~~Config ì—…ë°ì´íŠ¸~~: **ì™„ë£Œ**
  - `repulsion_scale`, `repulsion_sigma_break`, `repulsion_schedule`, `repulsion_dino_model`
- ~~Shell scripts ì—…ë°ì´íŠ¸~~: **ì™„ë£Œ**
  - `exp1_repulsion.sh`, `exp3_2particle.sh`ì— ìƒˆ repulsion íŒŒë¼ë¯¸í„° ë°˜ì˜

* ì„¤ì •: ì…ì 4ê°œ, ì²˜ìŒë¶€í„° ëê¹Œì§€($T \to 0$) ìœ ì§€.
* ë¹„êµ: Ours (Repulsion ON) vs. DAPS Baseline (Repulsion OFF, Independent)
* í™•ì¸í•  ì§€í‘œ:
    * Max PSNR: 4ê°œ ì¤‘ ê°€ì¥ ì˜ ë‚˜ì˜¨ ë†ˆì˜ ì ìˆ˜. (ìš°ë¦¬ê°€ ë” ë†’ê±°ë‚˜ ë¹„ìŠ·í•´ì•¼ í•¨)
    * Std / Mode Coverage: 4ê°œê°€ 0ë„, 180ë„, í˜¹ì€ ë‹¤ë¥¸ Local Minimaë¡œ ì–¼ë§ˆë‚˜ ì˜ í©ì–´ì¡ŒëŠ”ê°€?
        * DAPS: ìš´ ë‚˜ì˜ë©´ 4ê°œ ë‹¤ 0ë„ë¡œ ì ë¦¼.
        * Ours: 0ë„, 180ë„ ê³¨ê³ ë£¨ ë‚˜ì™€ì•¼ ì„±ê³µ.
* ê¸°ëŒ€ ê²°ë¡ : "ë‹¨ìˆœíˆ ì—¬ëŸ¬ ë²ˆ ëŒë¦¬ëŠ” ê²ƒ(DAPS)ë³´ë‹¤, ì„œë¡œ ë°€ì–´ë‚´ë©° ëŒë¦¬ëŠ” ê²ƒ(Ours)ì´ ì •ë‹µ(Global Optima)ì„ ì°¾ì„ í™•ë¥ (Success Rate)ì´ í›¨ì”¬ ë†’ë‹¤."
* ì—¬ê¸°ì—ì„  particle guidanceë¥¼ ì˜ ì½”ë”©í•˜ê³  repulsion ê°•ë„ ë“± hyperparameter ê°’ì„ ì ì ˆí•˜ê²Œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ê´€ê±´. 
* ì´ì— ëŒ€í•œ sanity check ë° ê°€ì¥ ê¸°ë³¸ì ì¸ ê²½í–¥ì„± ì²´í¬ë¥¼ ìœ„í•´ 1 image 4 (particle) run ëª…ë ¹ì–´ë¥¼ ì ê·¹ í™œìš©í•œ ë’¤ ë””ë²„ê¹… ì™„ë£Œëœ ì½”ë“œë² ì´ìŠ¤ì—ì„œ í•©ë¦¬ì ì¸ hyperparameter setìœ¼ë¡œ 10 image ì‹¤í—˜ì„ ëŒë¦¬ì.
âš ï¸ ì£¼ì˜í•  ì  (Manifold):
* Repulsionì„ ìœ„í•´ z.gradë¥¼ ì¡°ì‘í•  ë•Œ, ë„ˆë¬´ ê°•í•˜ê²Œ ë°€ë©´ Latentê°€ í•™ìŠµëœ ë¶„í¬ ë°–(Off-manifold)ìœ¼ë¡œ íŠ•ê²¨ ë‚˜ê°€ ì´ë¯¸ì§€ê°€ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* ì´ˆë°˜ì—ëŠ” ê°•í•˜ê²Œ, í›„ë°˜($t \to 0$)ìœ¼ë¡œ ê°ˆìˆ˜ë¡ 0ì— ìˆ˜ë ´í•˜ë„ë¡ Decay Scheduleì„ ê¼­ ë„£ìœ¼ì„¸ìš”.
ğŸ’¡ íŒ (Sanity Check):
* 1 Image ì‹¤í—˜ ì‹œ, 4ê°œì˜ Latent Vector ê°„ì˜ **í‰ê·  ê±°ë¦¬(Average Pairwise Distance)**ë¥¼ ë§¤ ìŠ¤í… ë¡œê¹…í•˜ì„¸ìš”.
* Baseline(ë…ë¦½ ì‹¤í–‰)ë³´ë‹¤ ì´ ê±°ë¦¬ê°€ í™•ì‹¤íˆ ì»¤ì•¼ ì„±ê³µì…ë‹ˆë‹¤.

### [ì‹¤í—˜ 2] 4 â†’ 2 Pruning (Efficiency Verification)
* ì„¤ì •: 4ê°œë¡œ ì‹œì‘ $\to$ $t=200$ì—ì„œ 2ê°œë¡œ ì••ì¶• $\to$ ë.
* ë¹„êµ: Exp 2 (Pruning) vs. Exp 1 (Full Run)
* í™•ì¸í•  ì§€í‘œ:
    * Max PSNR ìœ ì§€ ì—¬ë¶€: Exp 1ê³¼ ê²°ê³¼ê°€ ê±°ì˜ ë˜‘ê°™ì•„ì•¼ í•¨. (ë–¨ì–´ì§€ë©´ Pruning ë¡œì§ ì‹¤íŒ¨)
    * Time / Memory: ì‹œê°„ì´ ì–¼ë§ˆë‚˜ ë‹¨ì¶•ë˜ì—ˆëŠ”ê°€? (ì´ê²Œ ë…¼ë¬¸ì˜ ì„¸ì¼ì¦ˆ í¬ì¸íŠ¸)
* ê¸°ëŒ€ ê²°ë¡ : "ì´ˆë°˜ íƒìƒ‰ í›„ ê°€ë§ ì—†ëŠ” ë†ˆì„ ë²„ë ¤ë„ ì„±ëŠ¥ ì†ì‹¤ì€ ì—†ë‹¤. ì¦‰, Exp 1ì²˜ëŸ¼ ëê¹Œì§€ 4ê°œë¥¼ ëŒê³  ê°€ëŠ” ê±´ ìì› ë‚­ë¹„ë‹¤."
* pruning ì„ê³„ê°’ ë° timestepê³¼ ê°™ì€ hyperparameter ê°’ì„ ì ì ˆí•˜ê²Œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ê´€ê±´. ì´ì— ëŒ€í•œ sanity check ë° ê°€ì¥ ê¸°ë³¸ì ì¸ ê²½í–¥ì„± ì²´í¬ë¥¼ ìœ„í•´ 1 image 4 (particle) run ëª…ë ¹ì–´ë¥¼ ì ê·¹ í™œìš©í•œ ë’¤ ë””ë²„ê¹… ì™„ë£Œëœ ì½”ë“œë² ì´ìŠ¤ì—ì„œ í•©ë¦¬ì ì¸ hyperparameter setìœ¼ë¡œ 10 image ì‹¤í—˜ì„ ëŒë¦¬ì.
âš ï¸ ì£¼ì˜í•  ì  (Indexing Hell):
* ë°°ì¹˜ ì‚¬ì´ì¦ˆê°€ 4ì—ì„œ 2ë¡œ ì¤„ì–´ë“¤ ë•Œ, zë¿ë§Œ ì•„ë‹ˆë¼ optimizerì˜ state, schedulerì˜ step, measurement y ë“± ê´€ë ¨ëœ ëª¨ë“  ë³€ìˆ˜ë¥¼ ê°™ì´ ì¤„ì—¬ì•¼(Slicing) ì—ëŸ¬ê°€ ì•ˆ ë‚©ë‹ˆë‹¤.
* í—·ê°ˆë¦¬ë©´ ê·¸ëƒ¥ 4ê°œ ìœ ì§€ë¥¼ í•˜ë˜, íƒˆë½í•œ 2ê°œì— ëŒ€í•´ì„œëŠ” Gradient ê³„ì‚°ì„ ë„ëŠ” ë§ˆìŠ¤í‚¹(Masking) ì²˜ë¦¬ë§Œ í•´ë„ ì—°ì‚°ëŸ‰ ì´ë“ì€ ì¦ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ë©”ëª¨ë¦¬ ì´ë“ì€ ì—†ì§€ë§Œ êµ¬í˜„ì€ ì‰¬ì›€) $\rightarrow$ í•˜ì§€ë§Œ ì§„ì§œ ë©”ëª¨ë¦¬ ì´ë“ì„ ìœ„í•´ Slicingì„ ì¶”ì²œí•©ë‹ˆë‹¤.
ğŸ“Š GPU VRAM ì¸¡ì • êµ¬ê°„ ë¶„ë¦¬ (êµ¬í˜„ í•„ìš”):
* Pruning ì¶”ê°€ ì‹œ, VRAM ì¸¡ì •ì„ **pruning ì „/í›„ ë‘ êµ¬ê°„**ìœ¼ë¡œ ìª¼ê°œì•¼ í•¨.
* `torch.cuda.reset_peak_memory_stats()`ë¥¼ pruning ì‹œì ì— í˜¸ì¶œí•˜ì—¬ ê° êµ¬ê°„ë³„ peakë¥¼ ë…ë¦½ ì¸¡ì •.
* metrics.jsonì— `vram.pre_pruning_peak_mb`, `vram.post_pruning_peak_mb` í˜•íƒœë¡œ ê¸°ë¡.

### [ì‹¤í—˜ 3] 2-Particle Full Run (Justification for '4')
* ì„¤ì •: ì²˜ìŒë¶€í„° 2ê°œë§Œ ë„ì›Œì„œ ëê¹Œì§€($T \to 0$) ìœ ì§€.
* ë¹„êµ: Exp 2 (4 $\to$ 2 Pruning) vs. Exp 3 (Just 2)
* í•µì‹¬ ì§ˆë¬¸: "ê·¸ëƒ¥ ì²˜ìŒë¶€í„° 2ê°œë§Œ ëŒë¦¬ë©´ ì•ˆ ë¼? êµ³ì´ 4ê°œë¡œ ì‹œì‘í•´ì„œ ì¤„ì—¬ì•¼ í•´?" (ë¦¬ë·°ì–´ë“¤ì´ ë¬´ì¡°ê±´ ë¬¼ì–´ë³¼ ì§ˆë¬¸)
* í™•ì¸í•  ì§€í‘œ:
    * Success Rate (ì„±ê³µë¥ ): Exp 3ì€ ê°€ë” ë‘˜ ë‹¤ ì‹¤íŒ¨(Local Minima)í•˜ëŠ” ê²½ìš°ê°€ ìƒê²¨ì•¼ í•¨. ë°˜ë©´ Exp 2ëŠ” 4ê°œ ì¤‘ ê³¨ëìœ¼ë¯€ë¡œ ì„±ê³µë¥ ì´ ë” ë†’ì•„ì•¼ í•¨.
* ê¸°ëŒ€ ê²°ë¡ : "ì²˜ìŒë¶€í„° 2ê°œë§Œ ì“°ë©´(Exp 3) ë¶ˆì•ˆì •í•˜ë‹¤. 4ê°œë¡œ ë„“ê²Œ íƒìƒ‰í•˜ê³  ì¤„ì´ëŠ” ê²ƒ(Exp 2)ì´ ì•ˆì •ì„±(Stability) ì¸¡ë©´ì—ì„œ í›¨ì”¬ ìš°ì›”í•˜ë‹¤."
* ì „ëµ: ì—¬ê¸°ì„œ ì‹¤íŒ¨ ì‚¬ë¡€(0ë„/180ë„ ëª¨ë‘ ëª» ì°¾ê³  Local Minima ë¹ ì§)ê°€ ë‹¨ í•˜ë‚˜ë¼ë„ ë‚˜ì˜¤ë©´ ë‹˜ì˜ ë…¼ë¦¬ëŠ” ì™„ë²½í•´ì§‘ë‹ˆë‹¤.
* ì‚¬ì‹¤ ì—¬ê¸°ì„  ì•ì„  ì‹¤í—˜ë“¤ì—ì„œ ì¶”ê°€ë˜ëŠ” hyperparameterê°€ ì—†ìœ¼ë©°, sampleë“¤ ì¤‘ ì‹¤íŒ¨í•˜ëŠ” ê²ƒë“¤ì˜ ë¹„ìœ¨ì„ ì œëŒ€ë¡œ ì¬ëŠ” ê²ƒì´ ê´€ê±´ì´ë¯€ë¡œ 1 image ì‹¤í—˜ì´ ì˜ë¯¸ê°€ ì—†ë‹¤. ìµœì†Œí•œ 10 image, ì—¬ê±´ì´ ë˜ë©´ 100 image ì‹¤í—˜ì„ ëŒë¦¬ì.

### [ì‹¤í—˜ 4] ì‹¤í—˜ 1~3 ì¤‘ ê°€ì¥ ì˜ ë‚˜ì˜¨ ì„¸íŒ…ì— ëŒ€í•´ ReSampleì˜ hard data consistency in latent space optimizationì„ ëŒë¦¬ì
- ì •í™•í•œ íšŸìˆ˜ ë° ê¸°ì¤€ì€ ReSample ê³µì‹ ë ˆí¬ì˜ êµ¬í˜„ì—ì„œ ì‹¤ì œ ëª‡ ë²ˆì˜ optimizationì´ ì´ë£¨ì–´ì§€ëŠ”ì§€ë¥¼ ì°¸ê³ í•´ì„œ ê²°ì •í•˜ì. hyperparameter íŠœë‹ì— 1 image ì‹¤í—˜ì„ í™œìš©í•˜ì.
- optimization íšŸìˆ˜ ë° ì†Œìš”ì‹œê°„ì„ ë³´ê³ í•˜ì. batch element ê°„ optimization ë° terminationì´ independentí•´ì•¼ í•¨ì— ìœ ì˜í•˜ì (ReSample ê³µì‹ ë ˆí¬ëŠ” ê·¸ë ‡ì§€ ì•Šì•˜ìŒ!)
ğŸ“Š GPU VRAM ì¸¡ì • êµ¬ê°„ ë¶„ë¦¬ (êµ¬í˜„ í•„ìš”):
* Optimization ì¶”ê°€ ì‹œ, VRAM ì¸¡ì •ì„ **optimization ì „/í›„ ë‘ êµ¬ê°„**ìœ¼ë¡œ ë¶„ë¦¬í•´ì•¼ í•¨.
* `torch.cuda.reset_peak_memory_stats()`ë¥¼ optimization ì‹œì‘ ì‹œì ì— í˜¸ì¶œí•˜ì—¬ ê° êµ¬ê°„ë³„ peakë¥¼ ë…ë¦½ ì¸¡ì •.
* metrics.jsonì— `vram.pre_optimization_peak_mb`, `vram.optimization_peak_mb` í˜•íƒœë¡œ ê¸°ë¡.
* ë§Œì•½ ì‹¤í—˜ 2ì˜ pruningê³¼ í•¨ê»˜ ì‚¬ìš© ì‹œ, 3êµ¬ê°„ìœ¼ë¡œ ë¶„ë¦¬: `pre_pruning`, `post_pruning_pre_optimization`, `optimization`.

### [ì‹¤í—˜ 5] ê²°ê³¼ë¥¼ ë³´ê³  ì œì¼ ì˜ ë‚˜ì˜¨ ì„¸íŒ…ì— ëŒ€í•´ 100 image ì‹¤í—˜ì„ ëŒë¦¬ì. 
- ì´í›„ particle guidance, ìœ ì „ì•Œê³ ë¦¬ì¦˜ì  ê´€ì ì˜ ì„¤ëª…, phase retrieval with 2 oversamplingì´ë¼ëŠ” 2-mode task ìì²´ì˜ íŠ¹ìˆ˜ì„±, DAPSì™€ ReSampleê³¼ì˜ ì‹¤í–‰ì‹œê°„ ë° GPU ë° ì—°ì‚°ëŸ‰ ë¹„êµ
- ëª‡ particleì´ í•„ìš”í–ˆê³  pruning ë° hard data consistency optimizationì´ ì–¼ë§ˆë‚˜ ë„ì›€ì´ ëëŠ”ì§€ì— ëŒ€í•œ ë³´ê³ 
- ê°€ëŠ¥í•˜ë©´ ffhq 100 imageì— ëŒ€í•´ì„œë„ evalì„ ì§„í–‰í•˜ì—¬ table ë§Œë“¤ê¸°.
* FFHQ 100ì¥: ì‹œê°„ì´ ë‚¨ìœ¼ë©´ ëŒë¦¬ë˜, ì•ˆ ë˜ë©´ "ImageNetì´ ë” ìƒìœ„ í˜¸í™˜(Superset) ë¬¸ì œì´ë¯€ë¡œ ìƒëµí–ˆë‹¤"ê³  í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤.
* ìŠ¤í† ë¦¬í…”ë§: "ìœ ì „ ì•Œê³ ë¦¬ì¦˜ì  ê´€ì "ê³¼ "TDPì˜ Planning ê´€ì "ì„ ì„ì–´ì„œ ì„¤ëª…í•˜ë©´, ë‹¨ìˆœí•œ ì—”ì§€ë‹ˆì–´ë§ì´ ì•„ë‹ˆë¼ **'ìƒì„± ëª¨ë¸ì„ ìœ„í•œ íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜ ì œì•ˆ'**ìœ¼ë¡œ ê²©ìƒë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.



## êµ¬í˜„ ì˜ˆì‹œ. êµ¬ì²´ì ì¸ particle ìˆ˜ì™€ pruning ì—¬ë¶€ ë“±ì€ ì‹¤í—˜ 2~4 ì„¸ë¶€ ì„¤ì •ì— ë”°ë¦„.

### Phase 1: ì´ˆê¸° íƒìƒ‰ì—ì„œ Particle Guidance (PG)ë¥¼ í†µí•œ "ê°•ì œì  ë‹¤ì–‘ì„±" í™•ë³´
* ê¸°ì¡´ DAPSì˜ í•œê³„: DAPSëŠ” ê°œë³„ ìƒ˜í”Œ(Chain)ì´ ë…ë¦½ì ìœ¼ë¡œ MCMCë¥¼ ìˆ˜í–‰í•œë‹¤. ìš°ì—°íˆ ì´ˆê¸°í™”ê°€ ì˜ ë˜ë©´ ì„œë¡œ ë‹¤ë¥¸ í•´ë¥¼ ì°¾ì„ ìˆ˜ë„ ìˆì§€ë§Œ, ëŒ€ë¶€ë¶„ì€ ê°€ì¥ 'ì‰¬ìš´' í•´(Dominant Mode)ë¡œ ë‹¤ ê°™ì´ ì ë ¤ë²„ë¦¬ëŠ” ê²½í–¥ì´ ìˆë‹¤.
* êµ¬ê°„: T=000 ~ 200 (ì•½ 80% êµ¬ê°„)
* ë™ì‘: LatentDAPS + Particle Guidance (Repulsive Force) - ì—¬ëŸ¬ ê°œì˜ ê¶¤ì (Particle)ì„ ë™ì‹œì— ìƒì„±í•˜ë©´ì„œ, ì…ìë“¤ë¼ë¦¬ ì„œë¡œ ë°€ì–´ë‚´ëŠ” í˜(Repulsive Force)ì„ ì ìš©. ìœ ì‚¬ë„(Similarity)ì— ëŒ€í•œ í˜ë„í‹°
* ëª©ì : parent ë‹¨ê³„ì— í•´ë‹¹í•˜ëŠ” ë‘ particleì´ ì„œë¡œ ë°€ì–´ë‚´ë©° í•´ ê³µê°„ì„ íƒìƒ‰í•©ë‹ˆë‹¤. í•˜ë‚˜ê°€ Mode 0Â°ë¡œ ê°€ë©´, ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ê°•ì œë¡œ Mode 180Â° ìª½ìœ¼ë¡œ ê°€ê²Œ ë©ë‹ˆë‹¤. í•´ ê³µê°„(Solution Space)ì„ í›¨ì”¬ ë„“ê²Œ ì»¤ë²„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* ReSample ìµœì í™”: OFF (ì´ë•Œ ìµœì í™”í•˜ë©´ Local Minimaì— ë¹ ì§‘ë‹ˆë‹¤).

ğŸ’¡ ë³´ì™„ ì œì•ˆ (Annealing the Repulsion):
* ë¬¸ì œì : Repulsive Forceê°€ ë„ˆë¬´ ëê¹Œì§€ ìœ ì§€ë˜ë©´, ë‘ ì…ìê°€ ì„œë¡œë¥¼ ë°€ì–´ë‚´ëŠë¼ ì •ì‘ ë°ì´í„° ë§¤ë‹ˆí´ë“œ(Manifold) ì •ì¤‘ì•™(ê°€ì¥ ìì—°ìŠ¤ëŸ¬ìš´ ì´ë¯¸ì§€)ì— ë„ë‹¬í•˜ì§€ ëª»í•˜ê³  ì•½ê°„ ë¹—ê²¨ë‚œ(Off-manifold) ìƒíƒœê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* í•´ê²°ì±…: TDP ë…¼ë¬¸ì—ì„œë„ ì–¸ê¸‰í•˜ë“¯, ì´ˆê¸°(High Noise)ì—ëŠ” $\alpha_p$(Particle Guidance Scale)ë¥¼ í¬ê²Œ ê°€ì ¸ê°€ì„œ í™•ì‹¤í•˜ê²Œ ê°ˆë¼ë†“ê³ , $t_{mid}$ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ $\alpha_p$ë¥¼ ì„œì„œíˆ ì¤„ì—¬ì„œ(Decay) ì…ìë“¤ì´ ê°ìì˜ Basin(ìˆ˜ë ´ ì˜ì—­) ì•ˆì°©í•˜ë„ë¡ ìœ ë„í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
* ì´ˆê¸° ë¶„ê¸°(Bifurcation)ì˜ ì¤‘ìš”ì„±: Phase Retrievalì—ì„œ 0ë„/180ë„ ê²°ì •ì€ ë…¸ì´ì¦ˆê°€ ë§¤ìš° í° ì´ˆë°˜ ë‹¨ê³„ì—ì„œ ê²°ì •ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ˆë°˜ 20~30% êµ¬ê°„ì—ì„œì˜ PG ê°•ë„ê°€ ìŠ¹íŒ¨ë¥¼ ê°€ë¥¼ ê²ƒì…ë‹ˆë‹¤.


### Phase 2:  Bi-level Tree Structureë¥¼ í†µí•œ Global Optima íƒìƒ‰ ì¤‘ ê°€ì§€ì¹˜ê¸° (Pruning)
Phase Retrievalì€ ëŒ€í‘œì ì¸ Non-convex(ë¹„ë³¼ë¡) ìµœì í™” ë¬¸ì œë¡œ, ì˜ëª»ëœ ì´ˆê¸°ê°’ì—ì„œ ì‹œì‘í•˜ë©´ Local Minimaì— ë¹ ì ¸ ì˜ì˜ ëª» ë‚˜ì˜¬ ìœ„í—˜ì´ í½ë‹ˆë‹¤.
* ê¸°ì¡´ DAPSì˜ í•œê³„: DAPSëŠ” Noise Annealingì„ í†µí•´ ì´ë¥¼ ê·¹ë³µí•˜ë ¤ í•˜ì§€ë§Œ, í•˜ë‚˜ì˜ ê¶¤ì (Sequential)ë§Œ ë”°ë¼ê°€ê¸° ë•Œë¬¸ì— ë§Œì•½ ì´ˆë°˜(tê°€ í´ ë•Œ)ì— ì˜ëª»ëœ ë°©í–¥(Local Basin)ìœ¼ë¡œ ë“¤ì–´ì„œë©´ ë˜ëŒë¦¬ê¸° ì–´ë µë‹¤.
* TDPì˜ í•´ê²°ì±… (Parent Branching & Sub-tree Expansion): TDPëŠ” "Parent Trajectory(ë¶€ëª¨ ê¶¤ì )"ë¥¼ ë¨¼ì € ë‹¤ì–‘í•˜ê²Œ ë¿Œë ¤ë†“ê³ (Exploration), ê°€ëŠ¥ì„± ìˆì–´ ë³´ì´ëŠ” ê°€ì§€ì—ì„œ "Child Trajectory(ìì‹ ê¶¤ì )"ë¥¼ ë»—ì–´ ë‚˜ê°€ë©° ì •ë°€í•˜ê²Œ ë‹¤ë“¬ëŠ”ë‹¤(Exploitation).
    * Phase Retrieval ì ìš©:
        1. Parent ë‹¨ê³„ (t: T \to t_{mid}): Particle Guidanceë¥¼ ì¼œê³  DAPSë¥¼ ìˆ˜í–‰í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ "ëŒ€ëµì ì¸ í˜•íƒœ(Coarse Structure)"ë¥¼ ê°€ì§„ ì—¬ëŸ¬ í›„ë³´êµ°ì„ í™•ë³´í•©ë‹ˆë‹¤.
        2. Child ë‹¨ê³„ (t: t_{mid} \to 0): ê° Parentì—ì„œ ê°€ì§€ë¥¼ ì³ì„œ, ì´ì œëŠ” Repulsive Forceë¥¼ ë„ê³  ê°•ë ¥í•œ Data Consistency(ì¸¡ì •ê°’ ì¼ì¹˜)ë¥¼ ì ìš©í•´ ì •ë°€í•œ ì´ë¯¸ì§€ë¥¼ ë³µì›í•©ë‹ˆë‹¤.
    * ì´ ë°©ì‹ì€ ë‹¨ìˆœíˆ í•˜ë‚˜ì˜ ê¸¸ë§Œ ê°€ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì—¬ëŸ¬ ê°€ëŠ¥ì„±ì„ ë™ì‹œì— íƒìƒ‰í•˜ë‹¤ê°€ ìœ ë§í•œ ê³³ì„ ì§‘ì¤‘ ê³µëµí•˜ë¯€ë¡œ Global Optimaë¥¼ ì°¾ì„ í™•ë¥ ì´ ë¹„ì•½ì ìœ¼ë¡œ ìƒìŠ¹í•©ë‹ˆë‹¤.
* ì‹œì : T=200 ê·¼ì²˜
* ë™ì‘: ë‘ ì…ìì˜ measurement lossë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
* ê²°ì •:
    * ë‘˜ ë‹¤ Lossê°€ ë‚®ë‹¤ë©´? ë‘˜ ë‹¤ ì‚´ë¦½ë‹ˆë‹¤ (í•˜ë‚˜ëŠ” 0Â°, í•˜ë‚˜ëŠ” 180Â°ì¼ í™•ë¥  ë†’ìŒ).
    * í•˜ë‚˜ê°€ ì••ë„ì ìœ¼ë¡œ ë‚®ë‹¤ë©´? ë‚˜ìœ ë…€ì„ì„ ë²„ë¦¬ê³  ì¢‹ì€ ë…€ì„ì„ ë³µì œí•˜ê±°ë‚˜, ì¢‹ì€ ë…€ì„ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    * ì¤‘ê°„ ë‹¨ê³„(t_{mid})ì—ì„œ "ì´ ê°€ì§€ëŠ” ê°€ë§ì´ ì—†ë‹¤(Lossê°€ ë„ˆë¬´ í¬ë‹¤)" ì‹¶ìœ¼ë©´ ê°€ì§€ì¹˜ê¸°(Pruning)ë¥¼ í•´ë²„ë¦´ ìˆ˜ ìˆë‹¤.
    * ë‚¨ëŠ” ìì›ì„ ìœ ë§í•œ ê²½ë¡œì— ì§‘ì¤‘(Child Expansion)í•  ìˆ˜ ìˆìœ¼ë‹ˆ ê³„ì‚° ë¹„ìš© ëŒ€ë¹„ ì„±ëŠ¥(ROI)ì´ í›¨ì”¬ ë†’ë‹¤.

ğŸ’¡ ë³´ì™„ ì œì•ˆ (Diversity-aware Pruning):
* ì‹œë‚˜ë¦¬ì˜¤: ë§Œì•½ ë‘ ì…ì(A, B)ê°€ ìš´ ë‚˜ì˜ê²Œ ë‘˜ ë‹¤ 0ë„ ëª¨ë“œë¡œ ìˆ˜ë ´í–ˆëŠ”ë°, Aê°€ lossê°€ ë” ë‚®ë‹¤ê³  ì¹©ì‹œë‹¤. ë‹¨ìˆœíˆ lossë§Œ ë³´ë©´ Bë¥¼ ë²„ë¦¬ê² ì§€ë§Œ, ë§Œì•½ Bê°€ 180ë„ ëª¨ë“œë¡œ ê°€ëŠ” ì¤‘ì´ì—ˆë‹¤ë©´(ì•„ì§ lossëŠ” ë†’ì§€ë§Œ), Bë¥¼ ì‚´ë¦¬ëŠ” ê²Œ ë‚˜ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
* ì „ëµ: ê°€ì§€ì¹˜ê¸°ë¥¼ í•  ë•Œ ë‹¨ìˆœíˆ Lossë§Œ ë³¼ ê²ƒì´ ì•„ë‹ˆë¼, ë‘ ì…ì ê°„ì˜ ê±°ë¦¬(Distance)ë„ í™•ì¸í•˜ì„¸ìš”.
    * Case 1: ê±°ë¦¬ê°€ ê°€ê¹ë‹¤ â†’ Lossê°€ ë‚®ì€ ë†ˆë§Œ ë‚¨ê¹€ (Local Refinement).
    * Case 2: ê±°ë¦¬ê°€ ë©€ë‹¤ â†’ Lossê°€ í—ˆìš© ë²”ìœ„ ë‚´ë¼ë©´ ë‘˜ ë‹¤ ì‚´ë¦¼ (Global Exploration ìœ ì§€).


### Phase 3: ì •ë°€ ìµœì í™” (Hard Data Consistency)
* êµ¬ê°„: T=200 ~ 0 (ë§ˆì§€ë§‰ 20% êµ¬ê°„)
* ë™ì‘: Latent Optimization ON
    * ì´ì œ Repulsive Forceë¥¼ ë•ë‹ˆë‹¤ (ì„œë¡œ ë°€ì–´ë‚¼ í•„ìš” ì—†ìŒ).
    * ëŒ€ì‹  ReSampleì˜ Latent Optimizationì„ ì¼œì„œ, í˜„ì¬ ìœ„ì¹˜(z)ë¥¼ ì¸¡ì •ê°’(y)ì— ê°•í•˜ê²Œ(Hard) ë°€ì°©ì‹œí‚µë‹ˆë‹¤. DAPSë„ ê³„ì† ì¼­ë‹ˆë‹¤.
    * ì£¼ì˜: Pixel Optimizationì€ ì ˆëŒ€ ê¸ˆì§€ (Phase Retrievalì—ì„œëŠ” ë…ì…ë‹ˆë‹¤).
* ëª©ì : DAPSê°€ ë‚¨ê¸´ ë¯¸ì„¸í•œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  PSNRì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.

ReSampleì´ í•„ìš”í•œ ìˆœê°„: "ë§ˆì§€ë§‰ í•œ ë¼˜ (Fine-tuning)"
TDPì˜ Particle Guidance(PG)ì™€ DAPSë¡œ ì—´ì‹¬íˆ íƒìƒ‰í•´ì„œ, ìš´ ì¢‹ê²Œ ì›ë³¸ ì´ë¯¸ì§€ì™€ ìœ ì‚¬í•œ í˜•íƒœ(Mode)ë¥¼ ì°¾ì•˜ë‹¤ê³  ê°€ì •í•´ ë´…ì‹œë‹¤. í•˜ì§€ë§Œ DAPSëŠ” ë³¸ì§ˆì ìœ¼ë¡œ 'ë…¸ì´ì¦ˆë¥¼ ì„ëŠ”(Annealing)' ë°©ì‹ì´ê¸° ë•Œë¬¸ì—, ìµœì¢… ê²°ê³¼ë¬¼(t=0)ì—ë„ ë¯¸ì„¸í•œ ë…¸ì´ì¦ˆê°€ ë‚¨ì•„ìˆê±°ë‚˜ ì¸¡ì •ê°’ yì™€ ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•˜ì§€ëŠ” ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ë•Œ ReSampleì˜ "Latent Optimization"ì´ ë“±ì¥í•©ë‹ˆë‹¤.
* ì—­í• : "ì´ì œ í° ê·¸ë¦¼(ìœ„ìƒ, í˜•íƒœ)ì€ ë§ì•˜ìœ¼ë‹ˆ, ë…¸ì´ì¦ˆë¥¼ ë„ê³  ë””í…Œì¼ì„ ì¸¡ì •ê°’ yì— ê°•ì œë¡œ(Hard Consistency) ë§ì¶°ë¼."
* ì•ˆì „í•œ ì´ìœ : ì´ë¯¸ DAPS+TDPê°€ 'ì •ë‹µ ê·¼ì²˜(Basin of Attraction)'ê¹Œì§€ ë°ë ¤ë‹¤ ë†“ì•˜ê¸° ë•Œë¬¸ì—, ì´ì œëŠ” ìµœì í™”ë¥¼ ê°•í•˜ê²Œ ê±¸ì–´ë„ Local Minima(ì—‰ëš±í•œ í•´)ë¡œ ë¹ ì§€ì§€ ì•Šê³  Global Optima(ì§„ì§œ ì •ë‹µ)ë¡œ ì™ ë¹¨ë ¤ ë“¤ì–´ê°‘ë‹ˆë‹¤ 
* ReSampleì—ì„œë„ local proximity(ì •ë‹µì— ê°€ê¹Œìš´ ê³³) ì•ˆì—ì„œ optimiationì„ í•¨ìœ¼ë¡œì¨ local minimaì— ë¹ ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ DDIM unconditional x0 predictionì„ optimization initial pointë¡œ ì‚¼ì•˜ë˜ ê²ƒê³¼ ë¹„ìŠ·í•œ ë§¥ë½!
ReSample ì ìš© ì‹œì : $T=200$ (Low noise) ì‹œì ì€ ì´ë¯¸ ì´ë¯¸ì§€ê°€ ê±°ì˜ ë‹¤ ë§Œë“¤ì–´ì§„ ìƒíƒœì…ë‹ˆë‹¤. ì´ë•Œ ReSampleì˜ Optimizationì„ ë„ˆë¬´ ê°•í•˜ê²Œ(Learning rateë¥¼ ë†’ê²Œ) ê±¸ë©´, ê¸°ê» DAPSê°€ ë§Œë“¤ì–´ë†“ì€ ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤ì²˜ê°€ ë§ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "Weak Optimization"ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •ë§Œ í•˜ëŠ” ê²ƒì´ ë” ì•ˆì „í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.








## ì‹¤í—˜ ê²°ê³¼

### [ì‹¤í—˜ 0] Baseline ê²°ê³¼ (LatentDAPS 4-run Independent)

#### ImageNet 10 Images (2025-12-13)
| Metric | Value | ë¹„ê³  |
|--------|-------|------|
| **Best PSNR Mean** | **17.50 dB** | ë…¼ë¬¸ 100img: 20.54 dB |
| Best PSNR Std | 3.67 | |
| Mean of Means | 15.49 dB | |
| Best SSIM Mean | 0.550 | |
| Best LPIPS Mean | 0.558 | (â†“ better) |
| Total Time | 2.5ì‹œê°„ (9,060ì´ˆ) | |
| Per Image | ~903ì´ˆ (15ë¶„) | |
| Peak VRAM | 10,161 MB | A10 GPU |

#### ì´ë¯¸ì§€ë³„ ìƒì„¸ ê²°ê³¼ (PSNR)
| Image | Sample 0 | Sample 1 | Sample 2 | Sample 3 | **Best** | Std |
|-------|----------|----------|----------|----------|----------|-----|
| 0 | 13.71 | 13.86 | 13.82 | **14.92** | 14.92 | 0.57 |
| 1 | **19.35** | 16.77 | 16.34 | 15.24 | 19.35 | 1.74 |
| 2 | **15.12** | 12.33 | 13.87 | 13.96 | 15.12 | 1.14 |
| 3 | 15.42 | **19.39** | 15.41 | 14.92 | 19.39 | 2.09 |
| 4 | **13.93** | 12.17 | 13.60 | 12.71 | 13.93 | 0.81 |
| 5 | 16.67 | **18.51** | 15.04 | 17.27 | 18.51 | 1.44 |
| 6 | 19.67 | 20.54 | 19.69 | **20.78** | 20.78 | 0.57 |
| 7 | **19.21** | 12.72 | 10.13 | 15.77 | 19.21 | 3.92 |
| 8 | 9.16 | **10.28** | 9.10 | 9.33 | 10.28 | 0.55 |
| 9 | **23.48** | 20.24 | 18.14 | 16.97 | 23.48 | 2.85 |

#### ê´€ì°°
- **4 samples ì¤‘ best idx ë¶„í¬**: sample 0 (5íšŒ), sample 1 (2íšŒ), sample 3 (3íšŒ) â†’ 4-run í•„ìš”ì„± í™•ì¸
- **ë†’ì€ std ì´ë¯¸ì§€**: img 7 (3.92), img 9 (2.85) â†’ Phase retrievalì˜ multi-modal íŠ¹ì„± ë°˜ì˜
- **ì–´ë ¤ìš´ ì´ë¯¸ì§€**: img 8 (best=10.28) â†’ ì¼ë¶€ ì´ë¯¸ì§€ì—ì„œ ì„±ëŠ¥ ì €í•˜


## í”„ë¡œì íŠ¸ ê¸°ëŒ€ ê²°ê³¼: ë³´ë‹¤ ì ì€ ì—°ì‚°ìœ¼ë¡œ ë¹„ìŠ·í•˜ê±°ë‚˜ ë” ì¢‹ì€ ì„±ëŠ¥ì„!
- DAPSì—ì„œ Phase Retrievalì˜ ë¶ˆì•ˆì •ì„±ì„ ê³ ë ¤í•˜ì—¬, 4ë²ˆì˜ independent runsì„ ìˆ˜í–‰í•œ ë’¤ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ë¥¼ ì„ íƒí•˜ì—¬ ë³´ê³ í–ˆìœ¼ë‹ˆ, ìš°ë¦¬í”Œì ì„ DAPS 4 runì´ë‘ ë¹„êµí–ˆì„ë•Œ ì‹œê°„xGPU ì‚¬ìš©ëŸ‰ì´ ë¹„ìŠ·í•˜ê±°ë‚˜ ì‘ìœ¼ë©´ì„œ ì„±ëŠ¥ì´ ë¹„ìŠ·í•˜ê±°ë‚˜ ë†’ìŒì„ ë³´ì´ë©´ ë˜ëŠ” ê²ƒ!
- ì‹¤í—˜ 2 (4 â†’ 2 Pruning)**ëŠ” ì´ë¡ ì  ìµœì ì (2 Modes)ê³¼ í˜„ì‹¤ì  ì•ˆì „ì¥ì¹˜(4 Runs) ì‚¬ì´ì˜ **"Sweet Spot"**ì„ ì°¾ëŠ” ì„¤ì •
- max ê°’ ë¿ë§Œ ì•„ë‹ˆë¼ std ë“± ë¶„í¬ë¥¼ ê°€ì§€ê³ ë„ ì˜ë¯¸ìˆëŠ” ë¶„ì„ì„ í•´ë³¼ ìˆ˜ ìˆì„ ê²ƒ.


## êµ¬í˜„ ê°€ì´ë“œ
- ëª¨ë“  Measurement Operator($\mathcal{A}$)ì™€ Loss Functionì€ (B, C, H, W) í˜•íƒœì˜ ì…ë ¥ì„ ë°›ì•„ **ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë³‘ë ¬ ì—°ì‚°(Broadcasting)**ì´ ê°€ëŠ¥í•˜ë„ë¡ ì‘ì„±ë˜ì–´ì•¼ í•œë‹¤. for ë£¨í”„ë¡œ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•˜ì§€ ë§ê³  PyTorchì˜ í…ì„œ ì—°ì‚°ì„ ì“¸ ê²ƒ!
- ìš°ë¦¬ëŠ” í•˜ë‚˜ì˜ $y$(ì¸¡ì •ê°’)ì— ëŒ€í•´ 2~4ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ $z_T$(ì´ˆê¸° ë…¸ì´ì¦ˆ)ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. Data Loaderì—ì„œ ì´ë¯¸ì§€ 1ì¥ì„ ê°€ì ¸ì˜¤ë©´, ì´ë¥¼ **batch_size=2~4ë¡œ ë³µì œ(repeat)**í•˜ë˜, ì´ˆê¸° ë…¸ì´ì¦ˆ $z_T$ëŠ” torch.randn(2~4, ...)ë¡œ ì„œë¡œ ë‹¤ë¥´ê²Œ ìƒì„±ë˜ë„ë¡ ì½”ë“œë¥¼ ì§¤ ê²ƒ!
- ~~ë³´í†µ Diffusion InferenceëŠ” with torch.no_grad(): ì•ˆì—ì„œ ë•ë‹ˆë‹¤. í•˜ì§€ë§Œ ìš°ë¦¬ëŠ” **Repulsion($\nabla_z \Phi$)**ê³¼ ReSample Optimization($\nabla_z \|y - Ax\|^2$) ë•Œë¬¸ì— ì‹¤í—˜ 1~5ì—ì„œ Gradientê°€ í•„ìš”í•  ì˜ˆì •ì´ë‹¤. ë”°ë¼ì„œ, Samplerì˜ ë©”ì¸ ë£¨í”„ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ Gradient ê³„ì‚°ì´ ê°€ëŠ¥í•˜ë„ë¡ ì—´ì–´ë‘ê³ (enable_grad), í•„ìš”í•œ ë¶€ë¶„ì—ì„œë§Œ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ no_gradë¥¼ ì“°ê±°ë‚˜, í˜¹ì€ ë°˜ëŒ€ë¡œ no_grad ë² ì´ìŠ¤ì— íŠ¹ì • ìŠ¤í…(PG, Optimization)ì—ì„œë§Œ enable_gradë¥¼ ì¼œëŠ” í† ê¸€(Toggle) êµ¬ì¡°ë¥¼ ë¯¸ë¦¬ ì‹¤í—˜ 0ì—ì„œë¶€í„° ë§Œë“¤ì–´ì•¼ í•œë‹¤!~~ â†’ **ì™„ë£Œ**: `sampler.py`ì˜ `LatentDAPS.sample()`ì—ì„œ `torch.set_grad_enabled(step_needs_grad)` êµ¬ì¡° êµ¬í˜„. `do_repulsion`ê³¼ `do_optimization` flagë¡œ stepë³„ gradient í™œì„±í™” ì œì–´. ì‹¤í—˜ 1, 2, 4 ë¡œì§ì€ TODO ì£¼ì„ìœ¼ë¡œ ì¤€ë¹„ë¨.
- ~~ì‹¤í—˜ 0~5ë¥¼ ìŠ¤í¬ë¦½íŠ¸ í•˜ë‚˜ë¡œ ì œì–´í•˜ë ¤ë©´ Flag ì„¤ê³„ê°€ ì¤‘ìš”í•˜ë‹¤. ë‹¤ìŒ Argumentë“¤ì„ ë¯¸ë¦¬ ì •ì˜í•´ ë‘˜ ê²ƒ!~~ â†’ **ì™„ë£Œ**: `configs/default.yaml`ì— ì •ì˜ë¨
    - `num_samples` (int): í•œ ë²ˆì— ìƒì„±í•  ì…ì(ì´ë¯¸ì§€)ì˜ ê°œìˆ˜ (ê¸°ì¡´ DAPSì˜ num_samplesë¥¼ ê·¸ëŒ€ë¡œ í™œìš©, particle_num ì—­í• )
    - `repulsion_scale` (float): ì…ìë¼ë¦¬ ë°€ì–´ë‚´ëŠ” í˜ì˜ ì´ˆê¸° ê°•ë„. 0.0ì´ë©´ ë…ë¦½ ì‹¤í–‰ (DAPS baseline), >0.0ì´ë©´ ì„œë¡œ ë°€ì–´ëƒ„
    - `pruning_step` (int): ê°€ì§€ì¹˜ê¸° ìˆ˜í–‰ timestep. -1ì´ë©´ pruning ì—†ìŒ
    - `optimization_step` (int): latent optimization ì‹œì‘ timestep. -1ì´ë©´ optimization ì—†ìŒ
    - `use_tpu` (bool): TPU ì‚¬ìš© ì—¬ë¶€.
    - (num_eval_imagesëŠ” data configì—ì„œ ì œì–´)
- ì‹¤í—˜ë³„ argument ì„¸íŒ… ê°€ì´ë“œ:
    Exp 0: Baseline (DAPS Replication)particle_num=4, repulsion_scale=0.0:ì´ë ‡ê²Œ ì„¤ì •í•˜ë©´ 4ê°œì˜ ì…ìê°€ ì„œë¡œ ê°„ì„­í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, DAPS ë…¼ë¬¸ì—ì„œ "1ê°œì”© 4ë²ˆ ëŒë¦° ê²ƒ(4 runs)"ê³¼ ìˆ˜í•™ì ìœ¼ë¡œ ì™„ì „íˆ ë™ì¼í•œ ê²°ê³¼ë¥¼ ëƒ…ë‹ˆë‹¤. (ì‹œë“œë§Œ ì˜ ì œì–´ëœë‹¤ë©´)ì´ê²ƒì´ ìš°ë¦¬ì˜ Reference ì„±ëŠ¥ì´ ë©ë‹ˆë‹¤.
    Exp 1: Repulsion Onlyrepulsion_scale > 0:ì´ì œ 4ê°œì˜ ì…ìê°€ ì„œë¡œ ë°€ì–´ëƒ…ë‹ˆë‹¤.ëª©í‘œ: Exp 0ë³´ë‹¤ **ë‹¤ì–‘ì„±(Std)**ì´ ë†’ê³ , **ìµœê³ ì (Max PSNR)**ì´ ë†’ê²Œ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    Exp 2: Efficiency (Pruning)pruning_step=200:ì½”ë“œëŠ” $t=200$ì´ ë˜ëŠ” ìˆœê°„, Lossì™€ Distanceë¥¼ ê³„ì‚°í•˜ì—¬ **4ê°œ ì¤‘ 2ê°œë¥¼ ë©”ëª¨ë¦¬ì—ì„œ ì‚­ì œ(ë˜ëŠ” Masking)**í•´ì•¼ í•©ë‹ˆë‹¤.ëª©í‘œ: Exp 1ê³¼ ì„±ëŠ¥ì€ ë¹„ìŠ·í•œë°, **ì‹œê°„(Time)ê³¼ ë©”ëª¨ë¦¬(VRAM)**ê°€ ì¤„ì–´ë“œëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    Exp 4: Quality (Optimization)optimization_step=200:$t=1000 \to 201$ê¹Œì§€ëŠ” Repulsionìœ¼ë¡œ íƒìƒ‰í•˜ê³ ,$t=200 \to 0$ë¶€í„°ëŠ” Repulsionì„ ë„ê³ (scale=0 ê°•ì œ ì ìš©), Latent Optimizationì„ ì¼­ë‹ˆë‹¤.ëª©í‘œ: Exp 2ë³´ë‹¤ PSNRì´ í™•ì‹¤íˆ ë” ì˜¬ë¼ê°€ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
- metric.jsonì— phaseë³„ time, gpu, optimization íšŸìˆ˜/ì‹œê°„ì„ ê¸°ë¡í•  ê²ƒ
- metric.jsonì„ Parsingí•˜ëŠ” ì½”ë“œë¥¼ ë§Œë“¤ ê²ƒ
- ì½”ë“œ ì‹¤í–‰ì„ í†µí•œ sanity checkëŠ” GPUê°€ ë‹¬ë¦° ì„œë²„ì—ì„œ ì§„í–‰í•  ì˜ˆì •! (ë¡œì»¬ ë§¥ë¶ X)
- TODOë¥¼ ì™„ë£Œí•œ ê²½ìš° ì´ PROJECT.md íŒŒì¼ì— ì·¨ì†Œì„ ì„ ê·¸ì–´ í‘œì‹œí•  ê²ƒ! ë§Œì•½ ë…¼ì˜ ê²°ê³¼ md ì„¤ëª…ë³´ë‹¤ ë” ì í•©í•œ ì„ íƒì§€ê°€ ìˆì–´ì„œ ì‹¤ì œ êµ¬í˜„ì— ì°¨ì´ê°€ ìƒê¸´ ê²½ìš° PROJECT.mdë¥¼ ì—…ë°ì´íŠ¸í•  ê²ƒ!
- git commit messageëŠ” í•œ/ì˜ í˜¼ìš© ê°€ëŠ¥, ì‹¤í—˜ ëª‡ì„ ì¤€ë¹„í•˜ê³  ìˆëŠ”ì§€ ëª…ì‹œ, í•œ ì¤„ ì´ë‚´ë¡œ ì‘ì„±. commitì€ vscode guië¡œë§Œ ì§„í–‰
- í˜„ í´ë”ëŠ” DAPS ë ˆí¬ë¥¼ ë² ì´ìŠ¤ë¡œ ìˆ˜ì • ì¤‘ì— ìˆìœ¼ë©°, TDP ë° ReSample ê´€ë ¨ ì„¸ë¶€ì‚¬í•­ì€ ì¶”í›„ í•´ë‹¹ ì‹¤í—˜ êµ¬í˜„ ë‹¨ê³„ì—ì„œ ì¶”ê°€ ì˜ˆì •
- ~~command íŒŒì¼ë“¤ì— ìƒˆë¡œìš´ argumentë“¤ ë°˜ì˜ ë° 1/10/100 imageìš© command ì¶”ê°€~~ â†’ **ì™„ë£Œ**: í´ë” êµ¬ì¡°:
    - `commands_gpu/`: GPU (CUDA) ì „ìš© ëª…ë ¹ì–´ (use_tpu=false)
    - ê° í´ë”ì— `exp0_baseline.sh` ~ `exp5_final.sh` í¬í•¨
    - ëª¨ë“  commandì— `repulsion_scale`, `pruning_step`, `optimization_step`, `data.end_id` ë°˜ì˜





ì•ˆë…•, ë‹¤ìŒì— ë”°ë¼ â€œRLSD repulsionì„ LatentDAPS(EDM Ïƒ + xâ‚€-pred)ë¡œ score-level injection ë°©ì‹ìœ¼ë¡œ ì´ì‹â€í•˜ëŠ” ì‘ì—…ì„ ê°€ì¥ ì•ˆì „í•˜ê²Œ ì§„í–‰í•´ì£¼ë¼.
(pruning/optimizationì€ ì•„ì§ ì œì™¸, ì´ ë¬¸ì„œì—ì„œ Exp1/3ë§Œ íƒ€ê²Ÿ)

You have access to two local repos:
	â€¢	Repo DAPS (target): my modified LatentDAPS / DAPS codebase, I changed vanilla DAPS so please be aware that content's different from original DAPS repo. You can refer to this PROJECT.md for concrete implementation done and planned. Though the repo name is DAPS, I am interested in LatentDAPS setting only.
	â€¢	Repo RLSD (source): RLSD (Repulsive Latent Score Distillation) official repo
	(Please ignore repo named 'DAPS_modified' since it's outdated.)

Goal: implement particle repulsion for Exp1/Exp3 (full-run 4 or 2 particles, no pruning, no optimization) by porting RLSDâ€™s repulsion module into LatentDAPS.

Context / What we know (important)

Target repo (LatentDAPS) uses:
	â€¢	EDM sigma parameterization (Ïƒ), not DDPM alpha. Evidence:
	â€¢	uses annealing_scheduler.sigma_steps[step]
	â€¢	forward diffusion like x_t = x0 + Ïƒ * Îµ
	â€¢	has sigma_max and prior_sigma
	â€¢	Prediction target is x0-prediction (denoiser), NOT eps-pred.
	â€¢	In DiffusionPFODE.derivative():
		return dst / st * xt - st * dsigma_t * sigma_t * self.model.score(xt/st, sigma=sigma_t)
	â€¢	model.score() is derived from denoiser D(x;Ïƒ) via EDM relation:
		score = (D(x;Ïƒ) - x) / Ïƒ^2

We want repulsion injection method:

Method to use: Add repulsion directly to score.
Rationale:
	â€¢	Equivalent to modifying denoiser output by + Î³ Ïƒ^2 repulsion, but simpler:
	â€¢	If score = (D-x)/Ïƒ^2, then D' = D + Î³ Ïƒ^2 repulsion â‡’ score' = score + Î³ repulsion
	â€¢	Matches DAPS update form (prior gradient + data gradient). Repulsion is an extra regularizer/guidance term added to the â€œprior gradient directionâ€.
	â€¢	We will implement sampler-loop computed repulsion, store it in pfode, and add it in DiffusionPFODE.derivative():

		# sampler loop
		repulsion = compute_repulsion(zt)
		pfode.set_repulsion(repulsion, scale=current_scale)

		# DiffusionPFODE.derivative
		score = self.model.score(...)
		if self.repulsion is not None:
			score = score + self.repulsion_scale * self.repulsion

		do not wrap model, maintain sampler -> pfode passing structure for clarity in responsibility and on/off control and debuggability.

Repulsion should be ON only for early/high-noise interval and decay to 0:
	â€¢	RLSD uses if sigma > sigma_break to enable repulsion.
	â€¢	We will implement interval on/off with alpha(Ïƒ) schedule (linear or cosine) such that alpha -> 0 as Ïƒâ†’0.
	â€¢	For now implement something simple: repulsion active for sigma > sigma_break, and within that, scale by alpha = repulsion_scale * schedule(sigma).

RLSD repulsion reference implementation (source repo):

RLSD repulsion core in rsd.py lines ~123-165 (already identified). It:
	â€¢	decodes latent to image
	â€¢	extracts DINO features
	â€¢	computes pairwise differences in feature space
	â€¢	uses RBF kernel with median heuristic bandwidth
	â€¢	computes SVGD-style repulsive gradient
	â€¢	backprops from feature space to latent using vector-Jacobian trick:
		eval_sum = torch.sum(dino_out * grad_phi.detach())
		deps_dx_backprop = torch.autograd.grad(eval_sum, latent_pred_t)[0]
	â€¢	normalizes by kernel sum

âš ï¸ Important fix:
RLSD code uses h = median(dist)^2 / log(N-1). This breaks for N=2 (log(1)=0) which is our project's Exp3 setting.
We will use a safe denominator: log(N) (or max(log(N), eps)) so that Exp3 (2 particles) works.

Deliverables / Tasks

Task 1: Identify integration points in LatentDAPS (target repo)

Find where:
	â€¢	multiple particles are represented as a batch (num_samples already exists)
	â€¢	sampler loop produces the latent state zt (or equivalent) each step
	â€¢	pfode (DiffusionPFODE) is called for derivative or stepping

We need:
	1.	A function to compute repulsion given current latent batch.
	2.	Store repulsion in pfode (or scheduler) object so derivative() can access it.
	3.	Add repulsion to score in DiffusionPFODE.derivative() before itâ€™s used in drift.

Task 2: Port RLSD repulsion computation into target repo

Implement something like:

repulsion = compute_repulsion(latents, sigma_t)

	â€¢	Use RLSDâ€™s DINO-based feature space repulsion.
	â€¢	Use decode_latents(latents) from target repo (or equivalent) to get images.
	â€¢	Use DINO-ViT model (frozen, eval mode).
	â€¢	Ensure gradients flow back to latent: need latent.requires_grad_(True) in repulsion-on steps.
	â€¢	Use vector-Jacobian trick like RLSD (avoid second-order).
	â€¢	Bandwidth: median(dist)^2 / max(log(N), eps)
	â€¢	compute pairwise distances
	â€¢	h = median(dist)**2 / max(log(N), eps)  âœ… (fix N=2)
	â€¢	Normalize by kernel sum similarly.

Task 3: Add config options (minimal)

Expose in config / args:
	â€¢	repulsion_scale (float, default 0.0)
	â€¢	sigma_break or repulsion_sigma_break (float or step index)
	â€¢	optional repulsion_schedule (linear default)

Repulsion behavior:
	â€¢	If repulsion_scale == 0, it must exactly reproduce DAPS baseline (independent chains).
	â€¢	If on, only apply when sigma > sigma_break.

Task 4: Ensure correct grad toggling and memory safety
	â€¢	Only enable autograd for repulsion steps.
	â€¢	DINO parameters must have requires_grad=False, but input must require grad.
	â€¢	DINO input should be resized to 224 (if RLSD does). Use same preprocessing as RLSD repo. But be aware that the input images' resolutions are different; DAPS handles 256x256 while RLSD handles 512x512 and latent resolution may be different across two repos. Adjust accordingly and tell me what you did. Ask me if any uncertainty. Place debugging code if needed and tell me that there are debugging code I have to check the outputs.

Task 5: Logging for sanity (Exp1/3)

Add to metrics/logging:
	â€¢	mean pairwise DINO feature distance per step
	â€¢	whether repulsion was on/off
	â€¢	optionally norm of repulsion gradient
	â€¢	time per repulsion computation
Be aware of the bugs resulting from new logging terms.

This is crucial to verify repulsion is actually acting.

Task 6: Provide a minimal test run plan - note that each sanity test may take 15+ minutes.
	â€¢	run 1 image sanity with 4 particles, repulsion on; check:
	â€¢	pairwise distance increases early
	â€¢	later decreases/stabilizes when repulsion off/decayed
	â€¢	no NaNs
	â€¢	run 1 image sanity with 2 particles; ensure no crash (bandwidth fix works).

Implementation guidance (preferred architecture)

Do NOT wrap/modify model.score() logic deeply. Keep responsibilities separated:
	â€¢	sampler computes repulsion and sets it on pfode each step:
		pfode.set_repulsion(repulsion, scale=alpha_sigma)
	â€¢	pfodeâ€™s derivative() adds it:
		score = score + self.repulsion_scale * self.repulsion
êµ¬ì¡°:
	â€¢	sampler: â€œì–¸ì œ/ì–¼ë§ˆë‚˜ repulsionâ€
	â€¢	model: â€œìˆœìˆ˜ denoisingâ€
	â€¢	pfode: â€œODE drift ê³„ì‚°â€

This allows easy on/off scheduling and debugging.

Output requirements
	â€¢	Make a clean PR-style change:
	â€¢	new module file for repulsion (e.g., repulsion.py)
	â€¢	minimal changes in sampler loop and pfode derivative
	â€¢	config updated, especially for exp1/3 sh commands file! and other files too; turn off repulsion option for baseline exp0.
	â€¢	Summarize:
	â€¢	files changed
	â€¢	key design decisions
	â€¢	how to run Exp1/Exp3
	â€¢	any assumptions or TODOs

Please proceed by:
	1.	scanning target repo to find exact insertion points and existing decoding utilities
	2.	mapping RLSD code dependencies (DINO loading, preprocessing, additionally required env setting: pip requirements, download sh, etc.)
	3.	implementing and testing quickly with 1-image run (if any additional installation or download is needed, do so, and document it inside DAPS requirements and download scripts)
	4.	report back with patch summary and instructions.