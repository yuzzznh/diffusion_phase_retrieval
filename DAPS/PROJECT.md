# My Project: LatentDAPSë¡œ Langevin Dynamics sampling + TDP-style íƒìƒ‰ìœ¼ë¡œ 0Â° 180Â° ì°¾ê¸° + ë§¨ ë§ˆì§€ë§‰ hard data consistency ì ìš©

## ì‹¤í—˜ë³„ ëª…ë ¹ì–´

``` bash
# ============================================================
# GPU (CUDA) ëª…ë ¹ì–´ - commands_gpu/ í´ë”
# ============================================================
# ì‹¤í—˜ 0
bash commands_gpu/exp0_baseline.sh --1           # 1 image sanity check -> A10ì—ì„œ 150/150W 12292MiB 100% 15min. í•˜ì§€ë§Œ ë¡œê·¸ì—” Peak Memory: 10116.54 MB ë¼ê³  ê¸°ë¡ë¨.
bash commands_gpu/exp0_baseline.sh --10          # 10 images -> A10ì—ì„œ 2.5ì‹œê°„ ê±¸ë¦´ ë“¯.
bash commands_gpu/exp0_baseline.sh --90          # 90 images (10~99) -> --10ê³¼ í•©ì³ì„œ 100ê°œ. A10ì—ì„œ 150/150W 12536MB 100% 15min per image ëœ¸. ë‹¤ ëŒë¦¬ë©´ 22.5ì‹œê°„ ì˜ˆìƒ.
bash commands_gpu/exp0_baseline.sh --1 --10      # 1 + 10 images ìˆœì°¨ ì‹¤í–‰

# ì‹¤í—˜ 1~4
bash commands_gpu/exp1_repulsion.sh --1 --10 --90
bash commands_gpu/exp2_pruning.sh --1 --10 --90
bash commands_gpu/exp3_2particle.sh --10 --90     # (1 image ì—†ìŒ)
bash commands_gpu/exp4_optimization.sh --1 --10 --90

# ì‹¤í—˜ 5
bash commands_gpu/exp5_final.sh --imagenet        # ImageNet 100
bash commands_gpu/exp5_final.sh --ffhq            # FFHQ 100
bash commands_gpu/exp5_final.sh --imagenet --ffhq # ë‘˜ ë‹¤

# ì¸ì ì—†ì´ ì‹¤í–‰í•˜ë©´ ì‚¬ìš©ë²• ì¶œë ¥:
$ bash commands_gpu/exp0_baseline.sh

# ì‚¬ìš©ë²•: 
bash exp0_baseline.sh [--1] [--10] [--90]
# --1   : 1 image sanity check (ì´ë¯¸ì§€ 0)
# --10  : 10 images main experiment (ì´ë¯¸ì§€ 0~9)
# --90  : 90 images final eval (ì´ë¯¸ì§€ 10~99, --10ê³¼ í•©ì³ì„œ 100ê°œ)
```

## ì‹¤í—˜ ì§„í–‰ ë° êµ¬í˜„ ê³¼ì • ì„¤ê³„

### [ë°ì´í„°] imagenet 10ì¥ìœ¼ë¡œ method ë¹„êµ, ë§ˆì§€ë§‰ evalì€ ffhq imagenet 100ì¥ì”©ìœ¼ë¡œ í•˜ëŠ”ê±¸ ëª©í‘œë¡œ, ì—¬ê±´ ì•ˆë˜ë©´ ffhqëŠ” ë²„ë¦¬ê¸° / ì‹œë“œ ê³ ì • (ì´ë¯¸ DAPSì—ì„œëŠ” 42)

### [ì‹¤í—˜ 0] LatentDAPS ë…¼ë¬¸ì— eval ë°ì´í„°ëŠ” 100 imageì—ë§Œ ë‚˜ì™€ìˆìœ¼ë‹ˆê¹Œ ë¹„êµë¥¼ ìœ„í•´ LatentDAPS(with Langevin Dynamic)ì˜ imagenet first 10 imageì— ëŒ€í•œ phase retrieval ì„±ëŠ¥ ì¸¡ì •.
- ~~ë‹¨, ì´ë•Œ imageë³„ë¡œ ì „ë¶€ ëŒì•„ê°„ ë’¤ ë‹¤ìŒ runì´ ì‹¤í–‰ë˜ëŠ” êµ¬ì¡°ë¡œ 4 runì´ êµ¬í˜„ë¼ìˆëŠ”ë°, ì´í›„ ì‹¤í—˜ë“¤ê³¼ì˜ ì›í™œí•œ ë¹„êµë¥¼ ìœ„í•´ eval ëª…ë ¹ì–´ë¥¼ 4 batch = 4 run êµ¬ì¡°ë¡œ ë³€ê²½í•´ì•¼ í•¨.~~
- ~~time logging: diffusion timestep Të¥¼ êµ¬ê°„ê°œìˆ˜ë¡œ í•˜ì—¬ **timestepë³„ ì†Œìš” ì‹œê°„**ì„ ì¸¡ì •. ì´í›„ ì‹¤í—˜ì—ì„œ pruning/optimization ì‹œì  ì „í›„ ì‹œê°„ ë¹„êµì— í™œìš©. sanity check ì°¨ì›ì—ì„œ 1 image 4 sample ëª…ë ¹ì–´ë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸í•  ê²ƒ.~~ â†’ **ì™„ë£Œ**: `sampler.py`ì˜ `LatentDAPS.sample()`ì—ì„œ stepë³„ ì‹œê°„ ì¸¡ì • (`self.timing_info`ì— ì €ì¥), `posterior_sample.py`ì—ì„œ ì´ë¯¸ì§€ë³„ timing ì§‘ê³„ í›„ `metrics.json`ì— ì €ì¥.
- ~~GPU VRAM logging: ì‹¤í—˜ 0ì—ì„œëŠ” phase êµ¬ë¶„ ì—†ì´ **ì „ì²´ êµ¬ê°„ì˜ peak VRAM**ë§Œ ì¸¡ì •. `torch.cuda.max_memory_allocated()` í™œìš©.~~ â†’ **ì™„ë£Œ**: `posterior_sample.py`ì—ì„œ `torch.cuda.reset_peak_memory_stats()` í›„ `torch.cuda.max_memory_allocated()` ì¸¡ì •, `metrics.json`ì˜ `metadata.gpu.peak_vram_mb`ì— ì €ì¥. (phaseë³„ êµ¬ê°„ ë¶„ë¦¬ëŠ” ì‹¤í—˜ 2, 4ì—ì„œ pruning/optimization ì¶”ê°€ ì‹œ êµ¬í˜„)
- ~~ëª…ë ¹ì–´ ìë™ê¸°ë¡ ë©”ì»¤ë‹ˆì¦˜ì´ ì´ë¯¸ ìˆëŠ”ê±¸ë¡œ ì•„ëŠ”ë°, ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ íŒŒì•…í•˜ê³ , ìš°ë¦¬ ì‹¤í—˜ 0~5ì˜ ê°ì¢… argument ì„¸íŒ…ì´ ì˜ ê¸°ë¡ë˜ë„ë¡ ì½”ë“œë¥¼ ìˆ˜ì •í•  ê²ƒ.~~ â†’ **Hydra ê¸°ë°˜ config ìë™ê¸°ë¡ í™•ì¸ ì™„ë£Œ**: `posterior_sample.py`ì—ì„œ `OmegaConf.to_container(args)`ë¥¼ í†µí•´ ëª¨ë“  configê°€ mergeëœ ìµœì¢… ê²°ê³¼ë¥¼ `results/<name>/config.yaml`ì— ìë™ ì €ì¥í•¨. sh ëª…ë ¹ì–´ì—ì„œ overrideí•œ ëª¨ë“  argumentê°€ ê¸°ë¡ë¨.

### [ì‹¤í—˜ 1] 4-Particle Full Run (Repulsion vs. Independence) â†’ **êµ¬í˜„ ì™„ë£Œ**

#### Sampler ì½”ë“œ ë¶„ì„ (ì¤€ë¹„)
| í•­ëª©         | ì´ ì½”ë“œ                  |
|--------------|--------------------------|
| íŒŒë¼ë¯¸í„°í™”   | EDM Ïƒ (sigma)            |
| ì˜ˆì¸¡ íƒ€ê²Ÿ    | xâ‚€-prediction (denoiser) |
| Îµ-prediction | âŒ ì•„ë‹˜                  |

- Forward diffusion: `x_t = x_0 + ÏƒÂ·Îµ` (EDM í˜•íƒœ)
- `DiffusionPFODE.derivative()`ì—ì„œ `model.score()` í˜¸ì¶œ â†’ score = (D(x;Ïƒ) - x) / ÏƒÂ² ë³€í™˜ ì‚¬ìš©
- ë³€ìˆ˜ëª… `x0hat`, `z0hat`ì´ xâ‚€ ì˜ˆì¸¡ì„ì„ ëª…ì‹œ

#### êµ¬í˜„ ì™„ë£Œ ì‚¬í•­ (RLSD â†’ LatentDAPS ì´ì‹)
- ~~`repulsion.py` ëª¨ë“ˆ ìƒì„±~~: **ì™„ë£Œ**
  - `DinoFeatureExtractor`: DINO-ViT ëª¨ë¸ lazy loading ë° feature ì¶”ì¶œ (`dino_vits16`, RLSDì™€ ë™ì¼)
  - `compute_repulsion_gradient()`: SVGD-style repulsion gradient ê³„ì‚° (RBF kernel + median heuristic bandwidth)
  - `RepulsionModule`: High-level repulsion ê´€ë¦¬ (scale decay, metrics tracking)
  - **N=2 ë²„ê·¸ ìˆ˜ì •**: `h = median(dist)^2 / max(log(N), eps)` ì‚¬ìš© (RLSDì˜ `log(N-1)` ëŒ€ì‹ )
- ~~`DiffusionPFODE` ìˆ˜ì •~~: **ì™„ë£Œ**
  - `set_repulsion(repulsion, scale)` ë©”ì„œë“œ ì¶”ê°€
  - `derivative()`ì—ì„œ score-level injection: `score' = score + scale * repulsion`
- ~~`LatentDAPS.sample()` ìˆ˜ì •~~: **ì™„ë£Œ**
  - Repulsion module ì´ˆê¸°í™” ë° ê° annealing stepì—ì„œ repulsion ê³„ì‚°
  - pfodeì— repulsion ì „ë‹¬ ë° metrics ìˆ˜ì§‘
- ~~Config ì—…ë°ì´íŠ¸~~: **ì™„ë£Œ**
  - `repulsion_scale`, `repulsion_sigma_break`, `repulsion_schedule`, `repulsion_dino_model`
- ~~Shell scripts ì—…ë°ì´íŠ¸~~: **ì™„ë£Œ**
  - `exp1_repulsion.sh`, `exp3_2particle.sh`ì— ìƒˆ repulsion íŒŒë¼ë¯¸í„° ë°˜ì˜

#### Hyperparameter: `repulsion_scale`ê³¼ RLSD `gamma`ì˜ ê´€ê³„

**RLSD êµ¬í˜„** (noise prediction space):
```python
noise_pred = Îµ - Î³ Â· âˆš(1-Î±_t) Â· âˆ‡Î¦   # Î³ = gamma (50~150)
```

**ìš°ë¦¬ êµ¬í˜„** (score space):
```python
score' = score + Î» Â· âˆ‡Î¦              # Î» = repulsion_scale
```

**í•µì‹¬**: Score ê³µê°„ì—ì„œ `+Î»Â·r`ì„ í•˜ë©´, Îµ ê³µê°„ì—ì„œëŠ” ì´ë¯¸ `ÏƒÂ·Î»`ê°€ ê³±í•´ì§„ íš¨ê³¼ê°€ ìƒê¸´ë‹¤ (EDM score-Îµ ë³€í™˜ ê´€ê³„).
ë”°ë¼ì„œ **Ïƒë¥¼ ì¶”ê°€ë¡œ ê³±í•˜ì§€ ì•Šê³ **, `repulsion_scale`ì„ RLSDì˜ `gamma` ìˆ˜ì¤€ìœ¼ë¡œ ì˜¬ë¦¬ë©´ ë™í˜•(equivalent)í•˜ê²Œ ë™ì‘í•œë‹¤.

| RLSD gamma | ìš°ë¦¬ repulsion_scale | ë¹„ê³  |
|------------|---------------------|------|
| 30 | 30 | Phase retrieval ê¸°ë³¸ê°’ |
| 50 | 50 | HDR ë“± ë‹¤ë¥¸ task |
| 100~150 | 100~150 | ê°•í•œ repulsion |

**ê²°ë¡ **: `repulsion_scale=0.5~1.0`ì€ RLSD ëŒ€ë¹„ ë„ˆë¬´ ì•½í•¨. 50ì€ ë„ˆë¬´ ì»¸ìŒ. 10ì€ ì ì ˆí–ˆìŒ. (KST 12/14 6:37PM ê¸°ì¤€)

#### Hyperparameter: `repulsion_sigma_break` í™œì„± êµ¬ê°„

**ìš°ë¦¬ ì„¤ì •** (EDM sigma ê¸°ì¤€):
```
sigma_max: 10
sigma_min: 0.001
repulsion_sigma_break: 1.0 (default)
```

| sigma ë²”ìœ„ | Repulsion | ë¹„ê³  |
|-----------|-----------|------|
| 1.0 ~ 10 | ON | ì „ì²´ 50 step ì¤‘ ~30 step |
| 0.001 ~ 1.0 | OFF | ë§ˆì§€ë§‰ ~20 step |

**RLSDì™€ ë¹„êµ**: RLSDëŠ” `sigma_break=999` (DDPM timestep)ë¡œ **ê±°ì˜ ì „ êµ¬ê°„ ON**.
ìš°ë¦¬ë„ ë” ì˜¤ë˜ ì¼œë‘ë ¤ë©´ `sigma_break`ë¥¼ ë‚®ì¶”ë©´ ë¨ (ì˜ˆ: 0.1 ë˜ëŠ” 0.01).

#### Shell Script ìˆ˜ì • (2025-12-14) - RLSD ìœ ì‚¬ ì„¸íŒ…

**ë°°ê²½**: ì´ì „ ì‹¤í—˜(scale=0.5, 1.0)ì—ì„œ repulsion íš¨ê³¼ê°€ ê±°ì˜ ì—†ì—ˆìŒ (pairwise distance ~32ë¡œ ë™ì¼).
RLSDì™€ ìµœëŒ€í•œ ìœ ì‚¬í•˜ê²Œ ë§ì¶”ê¸° ìœ„í•´ hyperparameter ë° ì£¼ì„ ìˆ˜ì •.

**ìˆ˜ì • ë‚´ìš©** (`exp1_repulsion.sh` ~ `exp5_final.sh` ì „ì²´):
```bash
REPULSION_SCALE=50            # RLSD gamma=50 (HDR task) ê¸°ì¤€
REPULSION_SIGMA_BREAK=1.0     # Ïƒ < 1.0ì—ì„œ OFF
REPULSION_SCHEDULE="constant" # ì¶”ê°€ decay ì—†ìŒ
```

**ì£¼ì„ ìˆ˜ì • (ì—„ë°€ì„± ê°•í™”)**:
- ~~"RLSD-ë™í˜• ì„¸íŒ…"~~ â†’ "RLSD gamma ê¸°ì¤€" (ì™„ì „ ë™í˜•ì€ ì•„ë‹˜)
- ~~"ìë™ decay"~~ â†’ "Ïƒ-decayëŠ” scoreâ†’Îµ ë³€í™˜ì—ì„œ ìì—° ë°œìƒ"
  - ì •í™•íˆëŠ”: EDM score-Îµ ë³€í™˜ ê´€ê³„ì— ì˜í•´ Îµ ê´€ì ì—ì„œ stepë³„ Ïƒê°€ ê³±í•´ì§€ëŠ” íš¨ê³¼ê°€ ë‚˜íƒ€ë‚¨
  - ì´ê²ƒì´ RLSDì˜ `gamma Ã— sqrt(1-Î±_t)` (â‰ˆ `gamma Ã— Ïƒ`)ì™€ ìœ ì‚¬í•´ì§€ëŠ” ì›ë¦¬
- `sigma_break=1.0`ì€ Ïƒ âˆˆ [1,10] êµ¬ê°„ë§Œ ON (~30/50 step)
  - RLSDëŠ” ë³´í†µ ë” ì˜¤ë˜ ì¼œë‘ . ë” ê¸´ ON ì›í•˜ë©´ 0.1 ë˜ëŠ” 0.01ë¡œ ë‚®ì¶”ê¸°

**ì˜ë„**:
1. `schedule=constant`: ì¶”ê°€ decay ì œê±° â†’ RLSDì²˜ëŸ¼ gamma ìƒìˆ˜ ìœ ì§€
2. `scale=50`: RLSD HDR task ê¸°ì¤€ê°’ìœ¼ë¡œ ì í”„ (0.5~1.0ì—ì„œ íš¨ê³¼ ì—†ì—ˆìŒ)
3. ì£¼ì„ì—ì„œ "ë™í˜•"ì´ë¼ëŠ” ê³¼ì¥ í‘œí˜„ ì œê±°, ì •í™•í•œ ë©”ì»¤ë‹ˆì¦˜ ì„¤ëª…

#### ë””ë²„ê¹… ë¡œê¹… ë° Assert ì¶”ê°€ (2025-12-14) â†’ **ì™„ë£Œ**

**ë°°ê²½**: scale=50ìœ¼ë¡œ ì˜¬ë ¤ë„ repulsionì´ ì‹¤ì œë¡œ ì ìš©ë˜ëŠ”ì§€ í™•ì‹ ì´ ì—†ì–´ì„œ, assertì™€ ìƒì„¸ ë¡œê¹… ì¶”ê°€.

**êµ¬í˜„ ë‚´ìš©**:

1. **Assert ì¶”ê°€** (`cores/scheduler.py` - `DiffusionPFODE.derivative()`):
   - ON ìƒíƒœ: `repulsion is not None`, `scale > 0`, `isfinite(score)`, `isfinite(repulsion)`, `shape ì¼ì¹˜`
   - OFF ìƒíƒœ: `scale == 0`
   - Warning: `ratio > 10`ì´ë©´ í­ì£¼ ìœ„í—˜ ê²½ê³  ì¶œë ¥

2. **repulsion.jsonl ë¡œê¹…** (metrics.jsonê³¼ ë³„ê°œ):
   - ì €ì¥ ìœ„ì¹˜: `results/<run_name>/repulsion.jsonl`
   - ìƒ˜í”Œë§ ê·œì¹™: step<50ì€ ë§¤ 5 step, step>=50ì€ ë§¤ 25 step, í•­ìƒ {0,1,2,5,10} í¬í•¨
   - í•„ë“œ:
     ```json
     {"image_idx": 0, "step": 0, "sigma": 10.0, "repulsion_on": true,
      "repulsion_scale_used": 50.0, "score_base_norm": 1234.5,
      "repulsion_norm": 0.123, "scaled_repulsion_norm": 6.15,
      "ratio_scaled_to_score": 0.005, "repulsion_cleared": false,
      "mean_pairwise_dino_dist": 32.1, "weights_mean": 0.45,
      "weights_max": 0.98, "weights_nonzero_frac": 1.0, "repulsion_time_sec": 0.23}
     ```

3. **ìˆ˜ì •ëœ íŒŒì¼**:
   - `cores/scheduler.py`: assert + `_last_score_info` + `begin/end_annealing_step()`
   - `repulsion.py`: `weights_mean`, `weights_max`, `weights_nonzero_frac` ì¶”ê°€
   - `sampler.py`: `repulsion_debug_logs` ìˆ˜ì§‘, ìƒ˜í”Œë§ ê·œì¹™ ì ìš©
   - `posterior_sample.py`: `repulsion.jsonl` ì €ì¥ ë¡œì§

#### ê²°ê³¼ ë””ë ‰í† ë¦¬ ì •ë¦¬ (2025-12-14)

ì´ì „ scale=0.1 sanity check ê²°ê³¼ë¥¼ ë³´ì¡´í•˜ê¸° ìœ„í•´ ë””ë ‰í† ë¦¬ ì´ë¦„ ë³€ê²½:
```
results/exp1_repulsion/imagenet_1img/exp1_sanity_check â†’ exp1_sanity_check_scale0.1
results/exp3_2particle/imagenet_1img/exp3_sanity_check â†’ exp3_sanity_check_scale0.1
```

#### Sanity Check ì‹¤í–‰ (2025-12-14) - scale=10 (scale=50ì—ì„œ íŠœë‹ í›„ í™•ì •)


```bash
# Exp1 (4-particle) sanity check âœ… ì™„ë£Œ
bash commands_gpu/exp1_repulsion.sh --1
# â†’ results/exp1_repulsion/imagenet_1img/exp1_sanity_check_scale10/

# Exp3 (2-particle) sanity check âœ… ì™„ë£Œ
bash commands_gpu/exp3_2particle.sh --1
# â†’ results/exp3_2particle/imagenet_1img/exp3_sanity_check_scale10/
```

**í™•ì¸í•  ê²ƒ** (Exp1 scale=10 ê¸°ì¤€):
- ~~`repulsion.jsonl`ì—ì„œ `ratio_scaled_to_score`ê°€ 0ì´ ì•„ë‹Œ ê°’ì¸ì§€~~ â†’ âœ… step 0ì—ì„œ 0.0625 (6.25%) í™•ì¸
- ~~ì´ˆë°˜ stepì—ì„œ `repulsion_on=true`ì´ê³  `repulsion_scale_used=10`ì¸ì§€~~ â†’ âœ… í™•ì¸
- ~~assert í†µê³¼ ì—¬ë¶€ (ì—ëŸ¬ ì—†ì´ ì™„ë£Œë˜ë©´ OK)~~ â†’ âœ… ì •ìƒ ì™„ë£Œ
- ~~Exp3 (N=2)ì—ì„œ bandwidth ë²„ê·¸ ìˆ˜ì •ì´ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ (NaN/crash ì—†ìŒ)~~ â†’ âœ… ì™„ë£Œ (NaN/crash ì—†ì´ ì •ìƒ ì™„ë£Œ)

* ì„¤ì •: ì…ì 4ê°œ, ì²˜ìŒë¶€í„° ëê¹Œì§€($T \to 0$) ìœ ì§€.
* ë¹„êµ: Ours (Repulsion ON) vs. DAPS Baseline (Repulsion OFF, Independent)
* í™•ì¸í•  ì§€í‘œ:
    * ~~Max PSNR: 4ê°œ ì¤‘ ê°€ì¥ ì˜ ë‚˜ì˜¨ ë†ˆì˜ ì ìˆ˜. (ìš°ë¦¬ê°€ ë” ë†’ê±°ë‚˜ ë¹„ìŠ·í•´ì•¼ í•¨)~~ â†’ âœ… **20.66 dB** (baseline 11.24 ëŒ€ë¹„ +9.42 dB)
    * ~~Std / Mode Coverage: 4ê°œê°€ 0ë„, 180ë„, í˜¹ì€ ë‹¤ë¥¸ Local Minimaë¡œ ì–¼ë§ˆë‚˜ ì˜ í©ì–´ì¡ŒëŠ”ê°€?~~ â†’ âœ… std=6.12 (ë†’ì€ ë¶„ì‚° = ë‹¤ì–‘í•œ mode íƒìƒ‰)
        * DAPS: ìš´ ë‚˜ì˜ë©´ 4ê°œ ë‹¤ 0ë„ë¡œ ì ë¦¼.
        * Ours: 0ë„, 180ë„ ê³¨ê³ ë£¨ ë‚˜ì™€ì•¼ ì„±ê³µ.
* ê¸°ëŒ€ ê²°ë¡ : "ë‹¨ìˆœíˆ ì—¬ëŸ¬ ë²ˆ ëŒë¦¬ëŠ” ê²ƒ(DAPS)ë³´ë‹¤, ì„œë¡œ ë°€ì–´ë‚´ë©° ëŒë¦¬ëŠ” ê²ƒ(Ours)ì´ ì •ë‹µ(Global Optima)ì„ ì°¾ì„ í™•ë¥ (Success Rate)ì´ í›¨ì”¬ ë†’ë‹¤."
* ~~ì—¬ê¸°ì—ì„  particle guidanceë¥¼ ì˜ ì½”ë”©í•˜ê³  repulsion ê°•ë„ ë“± hyperparameter ê°’ì„ ì ì ˆí•˜ê²Œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ê´€ê±´.~~ â†’ âœ… scale=10 í™•ì •
* ì´ì— ëŒ€í•œ sanity check ë° ê°€ì¥ ê¸°ë³¸ì ì¸ ê²½í–¥ì„± ì²´í¬ë¥¼ ìœ„í•´ 1 image 4 (particle) run ëª…ë ¹ì–´ë¥¼ ì ê·¹ í™œìš©í•œ ë’¤ ë””ë²„ê¹… ì™„ë£Œëœ ì½”ë“œë² ì´ìŠ¤ì—ì„œ í•©ë¦¬ì ì¸ hyperparameter setìœ¼ë¡œ 10 image ì‹¤í—˜ì„ ëŒë¦¬ì.
âš ï¸ ì£¼ì˜í•  ì  (Manifold):
* Repulsionì„ ìœ„í•´ z.gradë¥¼ ì¡°ì‘í•  ë•Œ, ë„ˆë¬´ ê°•í•˜ê²Œ ë°€ë©´ Latentê°€ í•™ìŠµëœ ë¶„í¬ ë°–(Off-manifold)ìœ¼ë¡œ íŠ•ê²¨ ë‚˜ê°€ ì´ë¯¸ì§€ê°€ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. â†’ scale=50ì—ì„œ í™•ì¸ë¨ (PSNR 6dBë¡œ ë¶•ê´´)
* ì´ˆë°˜ì—ëŠ” ê°•í•˜ê²Œ, í›„ë°˜($t \to 0$)ìœ¼ë¡œ ê°ˆìˆ˜ë¡ 0ì— ìˆ˜ë ´í•˜ë„ë¡ Decay Scheduleì„ ê¼­ ë„£ìœ¼ì„¸ìš”. â†’ sigma_break=1.0ìœ¼ë¡œ step 29ì—ì„œ OFF
ğŸ’¡ íŒ (Sanity Check):
* ~~1 Image ì‹¤í—˜ ì‹œ, 4ê°œì˜ Latent Vector ê°„ì˜ **í‰ê·  ê±°ë¦¬(Average Pairwise Distance)**ë¥¼ ë§¤ ìŠ¤í… ë¡œê¹…í•˜ì„¸ìš”.~~ â†’ âœ… repulsion.jsonlì— ê¸°ë¡ë¨
* ~~Baseline(ë…ë¦½ ì‹¤í–‰)ë³´ë‹¤ ì´ ê±°ë¦¬ê°€ í™•ì‹¤íˆ ì»¤ì•¼ ì„±ê³µì…ë‹ˆë‹¤.~~ â†’ âœ… 32 â†’ 71 (2.2ë°° ì¦ê°€)


### [ì‹¤í—˜ 2] 4 â†’ 2 Pruning (Efficiency Verification) â†’ **êµ¬í˜„ ì™„ë£Œ**
* ~~ì„¤ì •: 4ê°œë¡œ ì‹œì‘ $\to$ $t=200$ì—ì„œ 2ê°œë¡œ ì••ì¶• $\to$ ë.~~ â†’ **ìˆ˜ì •**: 4ê°œë¡œ ì‹œì‘ â†’ step 29 (Ïƒ < 1.0 ì „í™˜ ì§í›„)ì—ì„œ 2ê°œë¡œ ì••ì¶• â†’ ë.
* ë¹„êµ: Exp 2 (Pruning) vs. Exp 1 (Full Run)
* í™•ì¸í•  ì§€í‘œ:
    * Max PSNR ìœ ì§€ ì—¬ë¶€: Exp 1ê³¼ ê²°ê³¼ê°€ ê±°ì˜ ë˜‘ê°™ì•„ì•¼ í•¨. (ë–¨ì–´ì§€ë©´ Pruning ë¡œì§ ì‹¤íŒ¨)
    * Time / Memory: ì‹œê°„ì´ ì–¼ë§ˆë‚˜ ë‹¨ì¶•ë˜ì—ˆëŠ”ê°€? (ì´ê²Œ ë…¼ë¬¸ì˜ ì„¸ì¼ì¦ˆ í¬ì¸íŠ¸)
* ê¸°ëŒ€ ê²°ë¡ : "ì´ˆë°˜ íƒìƒ‰ í›„ ê°€ë§ ì—†ëŠ” ë†ˆì„ ë²„ë ¤ë„ ì„±ëŠ¥ ì†ì‹¤ì€ ì—†ë‹¤. ì¦‰, Exp 1ì²˜ëŸ¼ ëê¹Œì§€ 4ê°œë¥¼ ëŒê³  ê°€ëŠ” ê±´ ìì› ë‚­ë¹„ë‹¤."
* ~~pruning ì„ê³„ê°’ ë° timestepê³¼ ê°™ì€ hyperparameter ê°’ì„ ì ì ˆí•˜ê²Œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ê´€ê±´.~~ â†’ **ì™„ë£Œ**: pruning_step=29 (repulsion OFF ì§í›„), measurement loss ê¸°ì¤€ top-2 ì„ íƒ
* ì´ì— ëŒ€í•œ sanity check ë° ê°€ì¥ ê¸°ë³¸ì ì¸ ê²½í–¥ì„± ì²´í¬ë¥¼ ìœ„í•´ 1 image 4 (particle) run ëª…ë ¹ì–´ë¥¼ ì ê·¹ í™œìš©í•œ ë’¤ ë””ë²„ê¹… ì™„ë£Œëœ ì½”ë“œë² ì´ìŠ¤ì—ì„œ í•©ë¦¬ì ì¸ hyperparameter setìœ¼ë¡œ 10 image ì‹¤í—˜ì„ ëŒë¦¬ì.
âš ï¸ ì£¼ì˜í•  ì  (Indexing Hell):
* ~~ë°°ì¹˜ ì‚¬ì´ì¦ˆê°€ 4ì—ì„œ 2ë¡œ ì¤„ì–´ë“¤ ë•Œ, zë¿ë§Œ ì•„ë‹ˆë¼ optimizerì˜ state, schedulerì˜ step, measurement y ë“± ê´€ë ¨ëœ ëª¨ë“  ë³€ìˆ˜ë¥¼ ê°™ì´ ì¤„ì—¬ì•¼(Slicing) ì—ëŸ¬ê°€ ì•ˆ ë‚©ë‹ˆë‹¤.~~ â†’ **ì™„ë£Œ**: `zt, z0y, z0hat, x0y, x0hat, xt, measurement, trajectory` ëª¨ë‘ slicing êµ¬í˜„
* ~~í—·ê°ˆë¦¬ë©´ ê·¸ëƒ¥ 4ê°œ ìœ ì§€ë¥¼ í•˜ë˜, íƒˆë½í•œ 2ê°œì— ëŒ€í•´ì„œëŠ” Gradient ê³„ì‚°ì„ ë„ëŠ” ë§ˆìŠ¤í‚¹(Masking) ì²˜ë¦¬ë§Œ í•´ë„ ì—°ì‚°ëŸ‰ ì´ë“ì€ ì¦ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.~~ â†’ **ì„ íƒ**: Slicing ë°©ì‹ ì±„íƒ (ë©”ëª¨ë¦¬ ì´ë“ í™•ë³´)
~~ğŸ“Š GPU VRAM ì¸¡ì • êµ¬ê°„ ë¶„ë¦¬ (êµ¬í˜„ í•„ìš”):~~
~~* Pruning ì¶”ê°€ ì‹œ, VRAM ì¸¡ì •ì„ **pruning ì „/í›„ ë‘ êµ¬ê°„**ìœ¼ë¡œ ìª¼ê°œì•¼ í•¨.~~
~~* `torch.cuda.reset_peak_memory_stats()`ë¥¼ pruning ì‹œì ì— í˜¸ì¶œí•˜ì—¬ ê° êµ¬ê°„ë³„ peakë¥¼ ë…ë¦½ ì¸¡ì •.~~
~~* metrics.jsonì— `vram.pre_pruning_peak_mb`, `vram.post_pruning_peak_mb` í˜•íƒœë¡œ ê¸°ë¡.~~
â†’ **ì™„ë£Œ**: `sampler.py`ì—ì„œ pruning ì‹œì ì— VRAM ì¸¡ì •, `metrics.json`ì˜ `vram.segments` ê¸°ë°˜ êµ¬ì¡°ë¡œ ì €ì¥.

ğŸ“Š VRAM segments ê¸°ë°˜ êµ¬ì¡° (Exp2/Exp4 ê³µí†µ ì„¤ê³„):
```json
// Exp0/1/3 (pruning ì—†ìŒ)
"vram": {"peak_memory_mb": 10209.0, "segments": {}}

// Exp2 (pruningë§Œ)
"vram": {"peak_memory_mb": 10209.0, "segments": {"pre_pruning": 10150.0, "post_pruning": 6100.0}}

// Exp4 (pruning + optimization) - í–¥í›„ í™•ì¥ ì‹œ
"vram": {"peak_memory_mb": 10209.0, "segments": {"pre_pruning": 10150.0, "post_pruning": 6100.0, "optimization": 5800.0}}
```
â†’ ì‹¤í—˜ë³„ë¡œ í•„ìš”í•œ segmentë§Œ ì¶”ê°€í•˜ëŠ” ìœ ì—°í•œ êµ¬ì¡°ë¡œ, í–¥í›„ Exp4 optimization ì¶”ê°€ ì‹œì—ë„ ìœ ì§€ë³´ìˆ˜ ìš©ì´.

ğŸ“Œ Repulsion OFF ì „í™˜ ì‹œì  (í˜„ì¬ ì„¤ì • ê¸°ì¤€):
```
step 28: Ïƒ = 1.0482 (ON)
step 29: Ïƒ = 0.9525 (OFF) â† ì—¬ê¸°ì„œ ì²˜ìŒìœ¼ë¡œ Ïƒ < sigma_break
```
* **Repulsion ON**: step 0~28 (29 steps, 58%)
* **Repulsion OFF**: step 29~49 (21 steps, 42%)
* ì„¤ì •: `sigma_max=10`, `sigma_min=0.1`, `num_steps=50`, `timestep=poly-7`, `sigma_break=1.0`
* ~~â†’ pruning_step=25ëŠ” repulsion ON êµ¬ê°„ ë‚´ì— ìˆìŒ (Ïƒ=1.39)~~ â†’ **ìˆ˜ì •**: pruning_step=29 (repulsion OFF ì§í›„)

ğŸ“ ì €ì¥ êµ¬ì¡° (Exp 2):
```
results/<run_name>/
â”œâ”€â”€ samples/                    # ìµœì¢… ìƒ˜í”Œ (pruning í›„ 2ê°œ)
â”‚   â”œâ”€â”€ 00000_sample00.png
â”‚   â””â”€â”€ 00000_sample01.png
â”œâ”€â”€ trajectory/                 # ì‚´ì•„ë‚¨ì€ particleë“¤ì˜ ì „ì²´ trajectory (step 0~49)
â”‚   â”œâ”€â”€ 00000_sample00.png
â”‚   â””â”€â”€ 00000_sample01.png
â”œâ”€â”€ trajectory_pruned/          # íƒˆë½í•œ particleë“¤ì˜ trajectory (step 0~29ê¹Œì§€ë§Œ)
â”‚   â”œâ”€â”€ 00000_pruned00.png
â”‚   â””â”€â”€ 00000_pruned01.png
â”œâ”€â”€ pruning.jsonl              # pruning ë¡œê·¸ (losses, kept/pruned indices)
â”œâ”€â”€ repulsion.jsonl            # repulsion ë¡œê·¸
â”œâ”€â”€ metrics.json               # í‰ê°€ ê²°ê³¼ + vram.segments (pre_pruning/post_pruning)
â””â”€â”€ grid_results.png           # [gt, y, sample0, sample1] í˜•íƒœ (4ì—´)
```

### [ì‹¤í—˜ 3] 2-Particle Full Run (Justification for '4')
* ì„¤ì •: ì²˜ìŒë¶€í„° 2ê°œë§Œ ë„ì›Œì„œ ëê¹Œì§€($T \to 0$) ìœ ì§€.
* ë¹„êµ: Exp 2 (4 $\to$ 2 Pruning) vs. Exp 3 (Just 2)
* í•µì‹¬ ì§ˆë¬¸: "ê·¸ëƒ¥ ì²˜ìŒë¶€í„° 2ê°œë§Œ ëŒë¦¬ë©´ ì•ˆ ë¼? êµ³ì´ 4ê°œë¡œ ì‹œì‘í•´ì„œ ì¤„ì—¬ì•¼ í•´?" (ë¦¬ë·°ì–´ë“¤ì´ ë¬´ì¡°ê±´ ë¬¼ì–´ë³¼ ì§ˆë¬¸)
* í™•ì¸í•  ì§€í‘œ:
    * Success Rate (ì„±ê³µë¥ ): Exp 3ì€ ê°€ë” ë‘˜ ë‹¤ ì‹¤íŒ¨(Local Minima)í•˜ëŠ” ê²½ìš°ê°€ ìƒê²¨ì•¼ í•¨. ë°˜ë©´ Exp 2ëŠ” 4ê°œ ì¤‘ ê³¨ëìœ¼ë¯€ë¡œ ì„±ê³µë¥ ì´ ë” ë†’ì•„ì•¼ í•¨.
* ê¸°ëŒ€ ê²°ë¡ : "ì²˜ìŒë¶€í„° 2ê°œë§Œ ì“°ë©´(Exp 3) ë¶ˆì•ˆì •í•˜ë‹¤. 4ê°œë¡œ ë„“ê²Œ íƒìƒ‰í•˜ê³  ì¤„ì´ëŠ” ê²ƒ(Exp 2)ì´ ì•ˆì •ì„±(Stability) ì¸¡ë©´ì—ì„œ í›¨ì”¬ ìš°ì›”í•˜ë‹¤."
* ì „ëµ: ì—¬ê¸°ì„œ ì‹¤íŒ¨ ì‚¬ë¡€(0ë„/180ë„ ëª¨ë‘ ëª» ì°¾ê³  Local Minima ë¹ ì§)ê°€ ë‹¨ í•˜ë‚˜ë¼ë„ ë‚˜ì˜¤ë©´ ë‹˜ì˜ ë…¼ë¦¬ëŠ” ì™„ë²½í•´ì§‘ë‹ˆë‹¤.
* ì‚¬ì‹¤ ì—¬ê¸°ì„  ì•ì„  ì‹¤í—˜ë“¤ì—ì„œ ì¶”ê°€ë˜ëŠ” hyperparameterê°€ ì—†ìœ¼ë©°, sampleë“¤ ì¤‘ ì‹¤íŒ¨í•˜ëŠ” ê²ƒë“¤ì˜ ë¹„ìœ¨ì„ ì œëŒ€ë¡œ ì¬ëŠ” ê²ƒì´ ê´€ê±´ì´ë¯€ë¡œ 1 image ì‹¤í—˜ì´ ì˜ë¯¸ê°€ ì—†ë‹¤. ìµœì†Œí•œ 10 image, ì—¬ê±´ì´ ë˜ë©´ 100 image ì‹¤í—˜ì„ ëŒë¦¬ì.

### [ì‹¤í—˜ 4] ì‹¤í—˜ 1~3 ì¤‘ ê°€ì¥ ì˜ ë‚˜ì˜¨ ì„¸íŒ…(= 12/14 8:30PM ê¸°ì¤€ ì‹¤í—˜ 1)ì— ëŒ€í•´ ReSampleì˜ hard data consistency in latent space optimizationì„ ì¶”ê°€í•˜ì â†’ **êµ¬í˜„ ì™„ë£Œ**
- íšŸìˆ˜: ë§¨ ë§ˆì§€ë§‰ timestepì—ì„œë§Œ optimizationì„ ì§„í–‰.
- loss: ì§€ê¸ˆ pruningì—ì„œ ì“°ëŠ” ê²ƒê³¼ ë™ì¼í•œ measurement MSE: || A(decode(z)) - y ||^2
- ì–¼ë§ˆë‚˜ ê°•í•˜ê²Œ?: ReSample ë…¼ë¬¸ êµ¬í˜„ì˜ termination 2ê°€ì§€ ê¸°ì¤€ ê·¸ëŒ€ë¡œ ì‚¬ìš©
- ë‚˜ë¨¸ì§€: ReSample ë…¼ë¬¸ êµ¬í˜„ ê·¸ëŒ€ë¡œ AdamW, LR = 5e-3, eps = 1e-3, max_iters = 500
- ~~ì•ˆì „ì¥ì¹˜: accept-if-improve (measurement ê¸°ì¤€) - (ìµœì í™” ì „ loss > ìµœì í™” í›„ loss)ì¼ ë•Œë§Œ ì—…ë°ì´íŠ¸ ì±„íƒ, ì•„ë‹ˆë©´ ì›ë˜ z ìœ ì§€~~ â†’ **êµ¬í˜„ ì™„ë£Œ**: elementë³„ `final_loss < init_loss`ì¸ ê²½ìš°ì—ë§Œ ìµœì í™”ëœ z ì±„íƒ
- optimization íšŸìˆ˜ ë° ì†Œìš”ì‹œê°„ì„ ë³´ê³ í•˜ì. batch element ê°„ optimization ë° terminationì´ independentí•´ì•¼ í•¨ì— ìœ ì˜í•˜ì (ReSample ê³µì‹ ë ˆí¬ëŠ” ê·¸ë ‡ì§€ ì•Šì•˜ìŒ!)
- ~~GPU VRAM ì¸¡ì • êµ¬ê°„ ë¶„ë¦¬ (êµ¬í˜„ ì˜ˆì • - segments ê¸°ë°˜):~~
* ~~Optimization ì¶”ê°€ ì‹œ, VRAM ì¸¡ì •ì„ **optimization ì „/í›„ ë‘ êµ¬ê°„**ìœ¼ë¡œ ë¶„ë¦¬í•´ì•¼ í•¨.~~
* ~~`torch.cuda.reset_peak_memory_stats()`ë¥¼ optimization ì‹œì‘ ì‹œì ì— í˜¸ì¶œí•˜ì—¬ ê° êµ¬ê°„ë³„ peakë¥¼ ë…ë¦½ ì¸¡ì •.~~
* ~~metrics.jsonì— `vram.pre_optimization_peak_mb`, `vram.optimization_peak_mb` í˜•íƒœë¡œ ê¸°ë¡.~~
* ~~ë§Œì•½ ì‹¤í—˜ 2ì˜ pruningê³¼ í•¨ê»˜ ì‚¬ìš© ì‹œ, 3êµ¬ê°„ìœ¼ë¡œ ë¶„ë¦¬: `pre_pruning`, `post_pruning_pre_optimization`, `optimization`.~~
â†’ **êµ¬í˜„ ì™„ë£Œ**: `vram.segments['optimization']`ì— optimization êµ¬ê°„ peak VRAM ê¸°ë¡

#### êµ¬í˜„ ì™„ë£Œ ì‚¬í•­ (2025-12-14)

**1. `_latent_optimization()` ë©”ì„œë“œ êµ¬í˜„** (`sampler.py`):
- ReSample-style latent space optimization with **batch-independent termination**
- Loss: `|| A(decode(z)) - y ||^2` (measurement MSE)
- Termination criteria (per-element):
  1. Loss threshold: `cur_loss < epsÂ²` (1e-6 for eps=1e-3)
  2. Loss plateau: After 200 iterations, if `init_loss < cur_loss`, stop
- **Accept-if-improve ë¡œì§** (ì•ˆì „ì¥ì¹˜):
  - elementë³„ë¡œ `final_loss < init_loss`ì¸ ê²½ìš°ì—ë§Œ ìµœì í™”ëœ z ì±„íƒ
  - ìµœì í™”ê°€ ì˜¤íˆë ¤ ì•…í™”ì‹œí‚¨ ê²½ìš° â†’ ì›ë˜ z ìœ ì§€
  - logging: `accepted_mask`, `num_accepted`, `num_rejected`
- **í•µì‹¬ ì°¨ë³„ì **: ê° batch elementê°€ **ë…ë¦½ì ìœ¼ë¡œ** termination + accept/reject íŒë‹¨
  - ReSample ê³µì‹ ë ˆí¬: ì „ì²´ ë°°ì¹˜ í‰ê·  lossë¡œ termination â†’ ì¼ë¶€ elementê°€ ë‹¤ë¥¸ elementì— ì˜í–¥ ë°›ìŒ
  - ìš°ë¦¬ êµ¬í˜„: elementë³„ init_loss/cur_loss ì¶”ì , terminated maskë¡œ gradient ì°¨ë‹¨, accept_maskë¡œ ì±„íƒ ì—¬ë¶€ ê²°ì •

**2. `sample()` ìˆ˜ì •**:
- Diffusion loop ì™„ë£Œ í›„ ë§ˆì§€ë§‰ì— optimization ìˆ˜í–‰ (`hard_data_consistency == 1`ì¼ ë•Œ)
- `zt` (final latent)ë¥¼ ì§ì ‘ ì‚¬ìš© (encode ê³¼ì • ë¶ˆí•„ìš”)
- `optimization.jsonl` ë¡œê¹… ì¶”ê°€

**3. Config & Shell Script ì—…ë°ì´íŠ¸**:
- `default.yaml`: `hard_data_consistency` (-1=off, 1=on), `optimization_lr`, `optimization_eps`, `optimization_max_iters` ì¶”ê°€
- `exp4_optimization.sh`: ì‹¤í—˜ 1 ì„¸íŒ… (scale=10) + `hard_data_consistency=1`ë¡œ optimization í™œì„±í™”

**4. Logging**:
- `optimization.jsonl`: ì´ë¯¸ì§€ë³„ optimization ìƒì„¸ ë¡œê·¸
  ```json
  {"image_idx": 0, "init_losses": [...], "final_losses": [...],
   "final_iters": [...], "termination_reasons": [...],
   "accepted_mask": [true, true, false, true], "num_accepted": 3, "num_rejected": 1,
   "total_time_seconds": 12.3, "lr": 0.005, "eps": 0.001, "max_iters": 500}
  ```
- `metrics.json`: `metadata.optimization` ì„¹ì…˜ì— summary ì €ì¥
- `vram.segments['optimization']`: optimization êµ¬ê°„ peak VRAM ê¸°ë¡

**5. Exp 1~3ì—ì„œ Optimization ë¹„í™œì„±í™” í™•ì¸**:
- `exp0_baseline.sh`: `hard_data_consistency=-1` âœ…
- `exp1_repulsion.sh`: `hard_data_consistency=-1` âœ…
- `exp2_pruning.sh`: `hard_data_consistency=-1` âœ…
- `exp3_2particle.sh`: `hard_data_consistency=-1` âœ…
- **Exp 4ì—ì„œë§Œ** `HARD_DATA_CONSISTENCY=1`ë¡œ í™œì„±í™”

**6. íŒŒì¼ ë³€ê²½ ëª©ë¡**:
- `sampler.py`: `_latent_optimization()`, `sample()` ìˆ˜ì •
- `posterior_sample.py`: optimization kwargs ì „ë‹¬, `optimization.jsonl` ì €ì¥, `metrics.json` ì—…ë°ì´íŠ¸
- `configs/default.yaml`: optimization hyperparameters ì¶”ê°€
- `commands_gpu/exp4_optimization.sh`: ì‹¤í—˜ ì„¤ì • ì—…ë°ì´íŠ¸

**ëª…ë ¹ì–´**:
```bash
bash commands_gpu/exp4_optimization.sh --1    # 1 image sanity check
bash commands_gpu/exp4_optimization.sh --10   # 10 images main experiment
```

**VRAM segments êµ¬ì¡°** (ìµœì¢…):
```json
// Exp4 (optimization only, pruning OFF)
"vram": {
  "peak_memory_mb": 10209.0,
  "segments": {
    "optimization": 5800.0
  }
}

// Exp4 + Exp2 ì¡°í•© (pruning + optimization) - í–¥í›„ í™•ì¥
"vram": {
  "peak_memory_mb": 10209.0,
  "segments": {
    "pre_pruning": 10150.0,
    "post_pruning": 6100.0,
    "optimization": 5800.0
  }
}
```

#### (ì°¸ê³ ) ReSample ì›ë³¸ ë ˆí¬ ì½”ë“œ ë¶„ì„

Diffusion Timesteps

- ì´ timesteps: 1000 (configs/latent-diffusion/ffhq-ldm-vq-4.yaml:9)
- ddim_use_original_steps=Trueë¡œ ì„¤ì •ë˜ì–´ ìˆì–´ ì „ì²´ 1000 ìŠ¤í… ì‚¬ìš©

---
Optimization íšŸìˆ˜

ì½”ë“œì—ì„œ splits = 3ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆì–´ 1000 ìŠ¤í…ì„ 3ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤:
- index_split = 1000 // 3 â‰ˆ 333

Pixel Space Optimization (ddim.py:337-370)

- ì¡°ê±´: index >= 333 (ì¦‰, timestep 333~666 êµ¬ê°„)
- í˜¸ì¶œ ë¹ˆë„: ë§¤ 10 ìŠ¤í…ë§ˆë‹¤ (index % 10 == 0)
- í˜¸ì¶œ íšŸìˆ˜: ì•½ 33ë²ˆ
- ê° í˜¸ì¶œë‹¹ max iterations: 2000 (early stopping ìˆìŒ)
- optimizer: AdamW, lr=1e-2

Latent Space Optimization (ddim.py:373-432)

- ì¡°ê±´: index < 333 (ì¦‰, timestep 0~332 êµ¬ê°„)
- í˜¸ì¶œ ë¹ˆë„: ë§¤ 10 ìŠ¤í…ë§ˆë‹¤ (index % 10 == 0)
- í˜¸ì¶œ íšŸìˆ˜: ì•½ 33ë²ˆ + ë§ˆì§€ë§‰ì— 1ë²ˆ ì¶”ê°€ (line 329-332)
- ê° í˜¸ì¶œë‹¹ max iterations: 500 (early stopping ìˆìŒ)
- optimizer: AdamW, lr=5e-3

---
ìš”ì•½

| êµ¬ë¶„         | Timestep ë²”ìœ„  | í˜¸ì¶œ íšŸìˆ˜ | Max Iterations/í˜¸ì¶œ |
|--------------|----------------|-----------|---------------------|
| Pixel Space  | 333~666        | ~33íšŒ     | 2000                |
| Latent Space | 0~332 + ë§ˆì§€ë§‰ | ~34íšŒ     | 500                 |

  Latent Space Optimization Hyperparameters

  | Hyperparameter     | ê°’      | ìœ„ì¹˜                 |
  |--------------------|---------|----------------------|
  | max_iters          | 500     | line 373 (í•¨ìˆ˜ ì¸ì) |
  | lr (learning rate) | 5e-3    | line 394             |
  | eps (tolerance)    | 1e-3    | line 373 (í•¨ìˆ˜ ì¸ì) |
  | optimizer          | AdamW   | line 399             |
  | loss function      | MSELoss | line 398             |

  Early Stopping ì¡°ê±´ (2ê°€ì§€)

  1. Loss threshold: cur_loss < epsÂ² (1e-6) â†’ line 428
  2. Loss plateau: 200 iteration ì´í›„, ì´ˆê¸° lossë³´ë‹¤ í˜„ì¬ lossê°€ í¬ë©´ ì¢…ë£Œ â†’ line 419-426

  ì½”ë“œ ë°œì·Œ

  def latent_optimization(self, measurement, z_init, operator_fn, eps=1e-3, max_iters=500, lr=None):
      if lr is None:
          lr_val = 5e-3
      else:
          lr_val = lr.item()

      loss = torch.nn.MSELoss()
      optimizer = torch.optim.AdamW([z_init], lr=lr_val)

      # Early stopping logic
      if itr < 200:
          losses.append(cur_loss)
      else:
          if losses[0] < cur_loss:
              break


### [ì‹¤í—˜ 5] ê²°ê³¼ë¥¼ ë³´ê³  ì œì¼ ì˜ ë‚˜ì˜¨ ì„¸íŒ…ì— ëŒ€í•´ 100 image ì‹¤í—˜ì„ ëŒë¦¬ì. 
- ì´í›„ particle guidance, ìœ ì „ì•Œê³ ë¦¬ì¦˜ì  ê´€ì ì˜ ì„¤ëª…, phase retrieval with 2 oversamplingì´ë¼ëŠ” 2-mode task ìì²´ì˜ íŠ¹ìˆ˜ì„±, DAPSì™€ ReSampleê³¼ì˜ ì‹¤í–‰ì‹œê°„ ë° GPU ë° ì—°ì‚°ëŸ‰ ë¹„êµ
- ëª‡ particleì´ í•„ìš”í–ˆê³  pruning ë° hard data consistency optimizationì´ ì–¼ë§ˆë‚˜ ë„ì›€ì´ ëëŠ”ì§€ì— ëŒ€í•œ ë³´ê³ 
- ê°€ëŠ¥í•˜ë©´ ffhq 100 imageì— ëŒ€í•´ì„œë„ evalì„ ì§„í–‰í•˜ì—¬ table ë§Œë“¤ê¸°.
* FFHQ 100ì¥: ì‹œê°„ì´ ë‚¨ìœ¼ë©´ ëŒë¦¬ë˜, ì•ˆ ë˜ë©´ "ImageNetì´ ë” ìƒìœ„ í˜¸í™˜(Superset) ë¬¸ì œì´ë¯€ë¡œ ìƒëµí–ˆë‹¤"ê³  í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤.
* ìŠ¤í† ë¦¬í…”ë§: "ìœ ì „ ì•Œê³ ë¦¬ì¦˜ì  ê´€ì "ê³¼ "TDPì˜ Planning ê´€ì "ì„ ì„ì–´ì„œ ì„¤ëª…í•˜ë©´, ë‹¨ìˆœí•œ ì—”ì§€ë‹ˆì–´ë§ì´ ì•„ë‹ˆë¼ **'ìƒì„± ëª¨ë¸ì„ ìœ„í•œ íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜ ì œì•ˆ'**ìœ¼ë¡œ ê²©ìƒë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.



## êµ¬í˜„ ì˜ˆì‹œ. êµ¬ì²´ì ì¸ particle ìˆ˜ì™€ pruning ì—¬ë¶€ ë“±ì€ ì‹¤í—˜ 2~4 ì„¸ë¶€ ì„¤ì •ì— ë”°ë¦„.

### Phase 1: ì´ˆê¸° íƒìƒ‰ì—ì„œ Particle Guidance (PG)ë¥¼ í†µí•œ "ê°•ì œì  ë‹¤ì–‘ì„±" í™•ë³´
* ê¸°ì¡´ DAPSì˜ í•œê³„: DAPSëŠ” ê°œë³„ ìƒ˜í”Œ(Chain)ì´ ë…ë¦½ì ìœ¼ë¡œ MCMCë¥¼ ìˆ˜í–‰í•œë‹¤. ìš°ì—°íˆ ì´ˆê¸°í™”ê°€ ì˜ ë˜ë©´ ì„œë¡œ ë‹¤ë¥¸ í•´ë¥¼ ì°¾ì„ ìˆ˜ë„ ìˆì§€ë§Œ, ëŒ€ë¶€ë¶„ì€ ê°€ì¥ 'ì‰¬ìš´' í•´(Dominant Mode)ë¡œ ë‹¤ ê°™ì´ ì ë ¤ë²„ë¦¬ëŠ” ê²½í–¥ì´ ìˆë‹¤.
* êµ¬ê°„: T=000 ~ 200 (ì•½ 80% êµ¬ê°„)
* ë™ì‘: LatentDAPS + Particle Guidance (Repulsive Force) - ì—¬ëŸ¬ ê°œì˜ ê¶¤ì (Particle)ì„ ë™ì‹œì— ìƒì„±í•˜ë©´ì„œ, ì…ìë“¤ë¼ë¦¬ ì„œë¡œ ë°€ì–´ë‚´ëŠ” í˜(Repulsive Force)ì„ ì ìš©. ìœ ì‚¬ë„(Similarity)ì— ëŒ€í•œ í˜ë„í‹°
* ëª©ì : parent ë‹¨ê³„ì— í•´ë‹¹í•˜ëŠ” ë‘ particleì´ ì„œë¡œ ë°€ì–´ë‚´ë©° í•´ ê³µê°„ì„ íƒìƒ‰í•©ë‹ˆë‹¤. í•˜ë‚˜ê°€ Mode 0Â°ë¡œ ê°€ë©´, ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ê°•ì œë¡œ Mode 180Â° ìª½ìœ¼ë¡œ ê°€ê²Œ ë©ë‹ˆë‹¤. í•´ ê³µê°„(Solution Space)ì„ í›¨ì”¬ ë„“ê²Œ ì»¤ë²„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* ReSample ìµœì í™”: OFF (ì´ë•Œ ìµœì í™”í•˜ë©´ Local Minimaì— ë¹ ì§‘ë‹ˆë‹¤).

ğŸ’¡ ë³´ì™„ ì œì•ˆ (Annealing the Repulsion):
* ë¬¸ì œì : Repulsive Forceê°€ ë„ˆë¬´ ëê¹Œì§€ ìœ ì§€ë˜ë©´, ë‘ ì…ìê°€ ì„œë¡œë¥¼ ë°€ì–´ë‚´ëŠë¼ ì •ì‘ ë°ì´í„° ë§¤ë‹ˆí´ë“œ(Manifold) ì •ì¤‘ì•™(ê°€ì¥ ìì—°ìŠ¤ëŸ¬ìš´ ì´ë¯¸ì§€)ì— ë„ë‹¬í•˜ì§€ ëª»í•˜ê³  ì•½ê°„ ë¹—ê²¨ë‚œ(Off-manifold) ìƒíƒœê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* í•´ê²°ì±…: TDP ë…¼ë¬¸ì—ì„œë„ ì–¸ê¸‰í•˜ë“¯, ì´ˆê¸°(High Noise)ì—ëŠ” $\alpha_p$(Particle Guidance Scale)ë¥¼ í¬ê²Œ ê°€ì ¸ê°€ì„œ í™•ì‹¤í•˜ê²Œ ê°ˆë¼ë†“ê³ , $t_{mid}$ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ $\alpha_p$ë¥¼ ì„œì„œíˆ ì¤„ì—¬ì„œ(Decay) ì…ìë“¤ì´ ê°ìì˜ Basin(ìˆ˜ë ´ ì˜ì—­) ì•ˆì°©í•˜ë„ë¡ ìœ ë„í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
* ì´ˆê¸° ë¶„ê¸°(Bifurcation)ì˜ ì¤‘ìš”ì„±: Phase Retrievalì—ì„œ 0ë„/180ë„ ê²°ì •ì€ ë…¸ì´ì¦ˆê°€ ë§¤ìš° í° ì´ˆë°˜ ë‹¨ê³„ì—ì„œ ê²°ì •ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ˆë°˜ 20~30% êµ¬ê°„ì—ì„œì˜ PG ê°•ë„ê°€ ìŠ¹íŒ¨ë¥¼ ê°€ë¥¼ ê²ƒì…ë‹ˆë‹¤.


### Phase 2:  Bi-level Tree Structureë¥¼ í†µí•œ Global Optima íƒìƒ‰ ì¤‘ ê°€ì§€ì¹˜ê¸° (Pruning)
Phase Retrievalì€ ëŒ€í‘œì ì¸ Non-convex(ë¹„ë³¼ë¡) ìµœì í™” ë¬¸ì œë¡œ, ì˜ëª»ëœ ì´ˆê¸°ê°’ì—ì„œ ì‹œì‘í•˜ë©´ Local Minimaì— ë¹ ì ¸ ì˜ì˜ ëª» ë‚˜ì˜¬ ìœ„í—˜ì´ í½ë‹ˆë‹¤.
* ê¸°ì¡´ DAPSì˜ í•œê³„: DAPSëŠ” Noise Annealingì„ í†µí•´ ì´ë¥¼ ê·¹ë³µí•˜ë ¤ í•˜ì§€ë§Œ, í•˜ë‚˜ì˜ ê¶¤ì (Sequential)ë§Œ ë”°ë¼ê°€ê¸° ë•Œë¬¸ì— ë§Œì•½ ì´ˆë°˜(tê°€ í´ ë•Œ)ì— ì˜ëª»ëœ ë°©í–¥(Local Basin)ìœ¼ë¡œ ë“¤ì–´ì„œë©´ ë˜ëŒë¦¬ê¸° ì–´ë µë‹¤.
* TDPì˜ í•´ê²°ì±… (Parent Branching & Sub-tree Expansion): TDPëŠ” "Parent Trajectory(ë¶€ëª¨ ê¶¤ì )"ë¥¼ ë¨¼ì € ë‹¤ì–‘í•˜ê²Œ ë¿Œë ¤ë†“ê³ (Exploration), ê°€ëŠ¥ì„± ìˆì–´ ë³´ì´ëŠ” ê°€ì§€ì—ì„œ "Child Trajectory(ìì‹ ê¶¤ì )"ë¥¼ ë»—ì–´ ë‚˜ê°€ë©° ì •ë°€í•˜ê²Œ ë‹¤ë“¬ëŠ”ë‹¤(Exploitation).
    * Phase Retrieval ì ìš©:
        1. Parent ë‹¨ê³„ (t: T \to t_{mid}): Particle Guidanceë¥¼ ì¼œê³  DAPSë¥¼ ìˆ˜í–‰í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ "ëŒ€ëµì ì¸ í˜•íƒœ(Coarse Structure)"ë¥¼ ê°€ì§„ ì—¬ëŸ¬ í›„ë³´êµ°ì„ í™•ë³´í•©ë‹ˆë‹¤.
        2. Child ë‹¨ê³„ (t: t_{mid} \to 0): ê° Parentì—ì„œ ê°€ì§€ë¥¼ ì³ì„œ, ì´ì œëŠ” Repulsive Forceë¥¼ ë„ê³  ê°•ë ¥í•œ Data Consistency(ì¸¡ì •ê°’ ì¼ì¹˜)ë¥¼ ì ìš©í•´ ì •ë°€í•œ ì´ë¯¸ì§€ë¥¼ ë³µì›í•©ë‹ˆë‹¤.
    * ì´ ë°©ì‹ì€ ë‹¨ìˆœíˆ í•˜ë‚˜ì˜ ê¸¸ë§Œ ê°€ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì—¬ëŸ¬ ê°€ëŠ¥ì„±ì„ ë™ì‹œì— íƒìƒ‰í•˜ë‹¤ê°€ ìœ ë§í•œ ê³³ì„ ì§‘ì¤‘ ê³µëµí•˜ë¯€ë¡œ Global Optimaë¥¼ ì°¾ì„ í™•ë¥ ì´ ë¹„ì•½ì ìœ¼ë¡œ ìƒìŠ¹í•©ë‹ˆë‹¤.
* ì‹œì : T=200 ê·¼ì²˜
* ë™ì‘: ë‘ ì…ìì˜ measurement lossë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
* ê²°ì •:
    * ë‘˜ ë‹¤ Lossê°€ ë‚®ë‹¤ë©´? ë‘˜ ë‹¤ ì‚´ë¦½ë‹ˆë‹¤ (í•˜ë‚˜ëŠ” 0Â°, í•˜ë‚˜ëŠ” 180Â°ì¼ í™•ë¥  ë†’ìŒ).
    * í•˜ë‚˜ê°€ ì••ë„ì ìœ¼ë¡œ ë‚®ë‹¤ë©´? ë‚˜ìœ ë…€ì„ì„ ë²„ë¦¬ê³  ì¢‹ì€ ë…€ì„ì„ ë³µì œí•˜ê±°ë‚˜, ì¢‹ì€ ë…€ì„ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    * ì¤‘ê°„ ë‹¨ê³„(t_{mid})ì—ì„œ "ì´ ê°€ì§€ëŠ” ê°€ë§ì´ ì—†ë‹¤(Lossê°€ ë„ˆë¬´ í¬ë‹¤)" ì‹¶ìœ¼ë©´ ê°€ì§€ì¹˜ê¸°(Pruning)ë¥¼ í•´ë²„ë¦´ ìˆ˜ ìˆë‹¤.
    * ë‚¨ëŠ” ìì›ì„ ìœ ë§í•œ ê²½ë¡œì— ì§‘ì¤‘(Child Expansion)í•  ìˆ˜ ìˆìœ¼ë‹ˆ ê³„ì‚° ë¹„ìš© ëŒ€ë¹„ ì„±ëŠ¥(ROI)ì´ í›¨ì”¬ ë†’ë‹¤.

ğŸ’¡ ë³´ì™„ ì œì•ˆ (Diversity-aware Pruning):
* ì‹œë‚˜ë¦¬ì˜¤: ë§Œì•½ ë‘ ì…ì(A, B)ê°€ ìš´ ë‚˜ì˜ê²Œ ë‘˜ ë‹¤ 0ë„ ëª¨ë“œë¡œ ìˆ˜ë ´í–ˆëŠ”ë°, Aê°€ lossê°€ ë” ë‚®ë‹¤ê³  ì¹©ì‹œë‹¤. ë‹¨ìˆœíˆ lossë§Œ ë³´ë©´ Bë¥¼ ë²„ë¦¬ê² ì§€ë§Œ, ë§Œì•½ Bê°€ 180ë„ ëª¨ë“œë¡œ ê°€ëŠ” ì¤‘ì´ì—ˆë‹¤ë©´(ì•„ì§ lossëŠ” ë†’ì§€ë§Œ), Bë¥¼ ì‚´ë¦¬ëŠ” ê²Œ ë‚˜ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
* ì „ëµ: ê°€ì§€ì¹˜ê¸°ë¥¼ í•  ë•Œ ë‹¨ìˆœíˆ Lossë§Œ ë³¼ ê²ƒì´ ì•„ë‹ˆë¼, ë‘ ì…ì ê°„ì˜ ê±°ë¦¬(Distance)ë„ í™•ì¸í•˜ì„¸ìš”.
    * Case 1: ê±°ë¦¬ê°€ ê°€ê¹ë‹¤ â†’ Lossê°€ ë‚®ì€ ë†ˆë§Œ ë‚¨ê¹€ (Local Refinement).
    * Case 2: ê±°ë¦¬ê°€ ë©€ë‹¤ â†’ Lossê°€ í—ˆìš© ë²”ìœ„ ë‚´ë¼ë©´ ë‘˜ ë‹¤ ì‚´ë¦¼ (Global Exploration ìœ ì§€).


### Phase 3: ì •ë°€ ìµœì í™” (Hard Data Consistency)
* êµ¬ê°„: T=200 ~ 0 (ë§ˆì§€ë§‰ 20% êµ¬ê°„)
* ë™ì‘: Latent Optimization ON
    * ì´ì œ Repulsive Forceë¥¼ ë•ë‹ˆë‹¤ (ì„œë¡œ ë°€ì–´ë‚¼ í•„ìš” ì—†ìŒ).
    * ëŒ€ì‹  ReSampleì˜ Latent Optimizationì„ ì¼œì„œ, í˜„ì¬ ìœ„ì¹˜(z)ë¥¼ ì¸¡ì •ê°’(y)ì— ê°•í•˜ê²Œ(Hard) ë°€ì°©ì‹œí‚µë‹ˆë‹¤. DAPSë„ ê³„ì† ì¼­ë‹ˆë‹¤.
    * ì£¼ì˜: Pixel Optimizationì€ ì ˆëŒ€ ê¸ˆì§€ (Phase Retrievalì—ì„œëŠ” ë…ì…ë‹ˆë‹¤).
* ëª©ì : DAPSê°€ ë‚¨ê¸´ ë¯¸ì„¸í•œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  PSNRì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.

ReSampleì´ í•„ìš”í•œ ìˆœê°„: "ë§ˆì§€ë§‰ í•œ ë¼˜ (Fine-tuning)"
TDPì˜ Particle Guidance(PG)ì™€ DAPSë¡œ ì—´ì‹¬íˆ íƒìƒ‰í•´ì„œ, ìš´ ì¢‹ê²Œ ì›ë³¸ ì´ë¯¸ì§€ì™€ ìœ ì‚¬í•œ í˜•íƒœ(Mode)ë¥¼ ì°¾ì•˜ë‹¤ê³  ê°€ì •í•´ ë´…ì‹œë‹¤. í•˜ì§€ë§Œ DAPSëŠ” ë³¸ì§ˆì ìœ¼ë¡œ 'ë…¸ì´ì¦ˆë¥¼ ì„ëŠ”(Annealing)' ë°©ì‹ì´ê¸° ë•Œë¬¸ì—, ìµœì¢… ê²°ê³¼ë¬¼(t=0)ì—ë„ ë¯¸ì„¸í•œ ë…¸ì´ì¦ˆê°€ ë‚¨ì•„ìˆê±°ë‚˜ ì¸¡ì •ê°’ yì™€ ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•˜ì§€ëŠ” ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ë•Œ ReSampleì˜ "Latent Optimization"ì´ ë“±ì¥í•©ë‹ˆë‹¤.
* ì—­í• : "ì´ì œ í° ê·¸ë¦¼(ìœ„ìƒ, í˜•íƒœ)ì€ ë§ì•˜ìœ¼ë‹ˆ, ë…¸ì´ì¦ˆë¥¼ ë„ê³  ë””í…Œì¼ì„ ì¸¡ì •ê°’ yì— ê°•ì œë¡œ(Hard Consistency) ë§ì¶°ë¼."
* ì•ˆì „í•œ ì´ìœ : ì´ë¯¸ DAPS+TDPê°€ 'ì •ë‹µ ê·¼ì²˜(Basin of Attraction)'ê¹Œì§€ ë°ë ¤ë‹¤ ë†“ì•˜ê¸° ë•Œë¬¸ì—, ì´ì œëŠ” ìµœì í™”ë¥¼ ê°•í•˜ê²Œ ê±¸ì–´ë„ Local Minima(ì—‰ëš±í•œ í•´)ë¡œ ë¹ ì§€ì§€ ì•Šê³  Global Optima(ì§„ì§œ ì •ë‹µ)ë¡œ ì™ ë¹¨ë ¤ ë“¤ì–´ê°‘ë‹ˆë‹¤ 
* ReSampleì—ì„œë„ local proximity(ì •ë‹µì— ê°€ê¹Œìš´ ê³³) ì•ˆì—ì„œ optimiationì„ í•¨ìœ¼ë¡œì¨ local minimaì— ë¹ ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ DDIM unconditional x0 predictionì„ optimization initial pointë¡œ ì‚¼ì•˜ë˜ ê²ƒê³¼ ë¹„ìŠ·í•œ ë§¥ë½!
ReSample ì ìš© ì‹œì : $T=200$ (Low noise) ì‹œì ì€ ì´ë¯¸ ì´ë¯¸ì§€ê°€ ê±°ì˜ ë‹¤ ë§Œë“¤ì–´ì§„ ìƒíƒœì…ë‹ˆë‹¤. ì´ë•Œ ReSampleì˜ Optimizationì„ ë„ˆë¬´ ê°•í•˜ê²Œ(Learning rateë¥¼ ë†’ê²Œ) ê±¸ë©´, ê¸°ê» DAPSê°€ ë§Œë“¤ì–´ë†“ì€ ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤ì²˜ê°€ ë§ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "Weak Optimization"ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •ë§Œ í•˜ëŠ” ê²ƒì´ ë” ì•ˆì „í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.








## ì‹¤í—˜ ê²°ê³¼

### [ì‹¤í—˜ 0] Baseline ê²°ê³¼ (LatentDAPS 4-run Independent)

#### ImageNet 10 Images (2025-12-13 KST)
| Metric | Value | ë¹„ê³  |
|--------|-------|------|
| **Best PSNR Mean** | **17.50 dB** | ë…¼ë¬¸ 100img: 20.54 dB |
| Best PSNR Std | 3.67 | |
| Mean of Means | 15.49 dB | |
| Best SSIM Mean | 0.550 | |
| Best LPIPS Mean | 0.558 | (â†“ better) |
| Total Time | 2.5ì‹œê°„ (9,060ì´ˆ) | |
| Per Image | ~903ì´ˆ (15ë¶„) | |
| Peak VRAM | 10,161 MB | A10 GPU |

#### ì´ë¯¸ì§€ë³„ ìƒì„¸ ê²°ê³¼ (PSNR)
| Image | Sample 0 | Sample 1 | Sample 2 | Sample 3 | **Best** | Std |
|-------|----------|----------|----------|----------|----------|-----|
| 0 | 13.71 | 13.86 | 13.82 | **14.92** | 14.92 | 0.57 |
| 1 | **19.35** | 16.77 | 16.34 | 15.24 | 19.35 | 1.74 |
| 2 | **15.12** | 12.33 | 13.87 | 13.96 | 15.12 | 1.14 |
| 3 | 15.42 | **19.39** | 15.41 | 14.92 | 19.39 | 2.09 |
| 4 | **13.93** | 12.17 | 13.60 | 12.71 | 13.93 | 0.81 |
| 5 | 16.67 | **18.51** | 15.04 | 17.27 | 18.51 | 1.44 |
| 6 | 19.67 | 20.54 | 19.69 | **20.78** | 20.78 | 0.57 |
| 7 | **19.21** | 12.72 | 10.13 | 15.77 | 19.21 | 3.92 |
| 8 | 9.16 | **10.28** | 9.10 | 9.33 | 10.28 | 0.55 |
| 9 | **23.48** | 20.24 | 18.14 | 16.97 | 23.48 | 2.85 |

#### ê´€ì°°
- **4 samples ì¤‘ best idx ë¶„í¬**: sample 0 (5íšŒ), sample 1 (2íšŒ), sample 3 (3íšŒ) â†’ 4-run í•„ìš”ì„± í™•ì¸
- **ë†’ì€ std ì´ë¯¸ì§€**: img 7 (3.92), img 9 (2.85) â†’ Phase retrievalì˜ multi-modal íŠ¹ì„± ë°˜ì˜
- **ì–´ë ¤ìš´ ì´ë¯¸ì§€**: img 8 (best=10.28) â†’ ì¼ë¶€ ì´ë¯¸ì§€ì—ì„œ ì„±ëŠ¥ ì €í•˜

### [ì‹¤í—˜ 1] Repulsion Sanity Check (2025-12-13 KST) - scale=0.1

#### Exp0 vs Exp1 Sanity Check ë¹„êµ (1 Image, scale=0.1)
| Metric | Exp0 (Baseline) | Exp1 (Repulsion, s=0.1) | ì°¨ì´ |
|--------|-----------------|------------------|------|
| PSNR samples | [8.46, 7.83, 8.44, 11.25] | [8.46, 7.82, 8.40, 11.24] | ~ë™ì¼ |
| **Best PSNR** | 11.25 | 11.24 | -0.01 |
| Mean PSNR | 8.99 | 8.98 | -0.01 |
| Std PSNR | 1.53 | 1.53 | ë™ì¼ |
| Best SSIM | 0.565 | 0.565 | ë™ì¼ |
| Best LPIPS | 0.495 | 0.495 | ë™ì¼ |
| **Time** | 900ì´ˆ | 910ì´ˆ | +10ì´ˆ (+1.1%) |
| **Peak VRAM** | 10,117 MB | 10,209 MB | +92 MB (+0.9%) |

#### Exp1 Repulsion ì„¤ì • (scale=0.1)
| Parameter | Value |
|-----------|-------|
| repulsion_scale | 0.1 |
| repulsion_sigma_break | 1.0 |
| repulsion_schedule | linear |
| repulsion_dino_model | dino_vits16 |
| repulsion_active_steps | 30/50 steps |
| repulsion_total_time | 11.4ì´ˆ |
| mean_pairwise_distance | 32.13 |

#### ê´€ì°° ë° ë¶„ì„
- **PSNR ê±°ì˜ ë™ì¼**: repulsionì´ ì¼œì¡ŒìŒì—ë„ ê²°ê³¼ê°€ baselineê³¼ ê±°ì˜ ê°™ìŒ
- **ê°€ëŠ¥í•œ ì›ì¸**:
  1. `repulsion_scale=0.1`ì´ ë„ˆë¬´ ì•½í•  ìˆ˜ ìˆìŒ â†’ scale ì¦ê°€ ì‹¤í—˜ í•„ìš”
  2. 1 imageë§Œìœ¼ë¡œëŠ” í†µê³„ì  ì˜ë¯¸ ë¶€ì¡± â†’ 10 image ì‹¤í—˜ í•„ìš”
  3. ê°™ì€ seed ì‚¬ìš©ìœ¼ë¡œ trajectoryê°€ ë¹„ìŠ·í•˜ê²Œ ìˆ˜ë ´í–ˆì„ ê°€ëŠ¥ì„±
- **Overhead ë¯¸ë¯¸**: ì‹œê°„ +1.1%, VRAM +0.9% â†’ repulsion ì—°ì‚° ë¹„ìš© ë‚®ìŒ
- **ë‹¤ìŒ ë‹¨ê³„**: `repulsion_scale` ì¡°ì • ë˜ëŠ” 10 image ì‹¤í—˜ìœ¼ë¡œ íš¨ê³¼ ê²€ì¦ í•„ìš”

### [ì‹¤í—˜ 1/3] Overnight Scale Grid Search (2025-12-14 KST) - scale=0.5, 1.0

#### í•µì‹¬ ìš”ì•½
| ì‹¤í—˜ | Particles | Scale | Best PSNR | Time | VRAM |
|------|-----------|-------|-----------|------|------|
| Exp0 Baseline | 4 | 0.0 | 17.50 | 100% | 100% |
| **Exp1-B (Best)** | 4 | 1.0 | **17.68** (+0.18) | +1.4% | +0.9% |
| Exp3-B | 2 | 1.0 | 17.35 (-0.15) | **-48%** | **-40%** |

#### ì‹¤í—˜ ì„¤ì •
| ì‹¤í—˜ | num_samples | repulsion_scale | ëª©í‘œ |
|------|-------------|-----------------|------|
| Exp1-A | 4 | 0.5 | scale ì¦ê°€ íš¨ê³¼ í™•ì¸ |
| Exp1-B | 4 | 1.0 | ë” ê°•í•œ repulsion íš¨ê³¼ í™•ì¸ |
| Exp3-A | 2 | 0.5 | 2-particle baseline |
| Exp3-B | 2 | 1.0 | 2-particle + ê°•í•œ repulsion |

#### ì „ì²´ ë¹„êµ ê²°ê³¼ (10 Images)

| ì‹¤í—˜ | Particles | Scale | Best PSNR â†‘ | Std | Mean of Means | Time (ì´ˆ) | VRAM (MB) |
|------|-----------|-------|-------------|-----|---------------|-----------|-----------|
| **Exp0 Baseline** | 4 | 0.0 | **17.50** | 3.67 | 15.49 | 9,060 | 10,161 |
| Exp1-A | 4 | 0.5 | 17.54 (+0.04) | 3.45 | 15.50 | 9,182 (+1.3%) | 10,252 |
| **Exp1-B** | 4 | 1.0 | **17.68 (+0.18)** | 3.20 | 15.56 | 9,191 (+1.4%) | 10,252 |
| Exp3-A | 2 | 0.5 | 17.20 (-0.30) | 3.67 | 16.04 | 4,694 (-48%) | 6,100 |
| Exp3-B | 2 | 1.0 | 17.35 (-0.15) | 3.68 | 16.14 | 4,694 (-48%) | 6,100 |

#### SSIM / LPIPS ë¹„êµ

| ì‹¤í—˜ | Best SSIM â†‘ | Best LPIPS â†“ |
|------|-------------|--------------|
| Exp0 Baseline | 0.550 | 0.558 |
| Exp1-A (s=0.5) | 0.550 | 0.562 |
| Exp1-B (s=1.0) | 0.552 | 0.556 |
| Exp3-A (s=0.5) | 0.535 | 0.568 |
| Exp3-B (s=1.0) | 0.544 | 0.563 |

#### Repulsion Metrics

| ì‹¤í—˜ | Mean Pairwise Distance | Repulsion Time (ì´ˆ) |
|------|------------------------|---------------------|
| Exp1-A (4p, s=0.5) | 32.09 | 10.9 |
| Exp1-B (4p, s=1.0) | 32.22 | 10.8 |
| Exp3-A (2p, s=0.5) | 32.21 | 5.6 |
| Exp3-B (2p, s=1.0) | 32.22 | 5.6 |

#### ì´ë¯¸ì§€ë³„ Best PSNR ìƒì„¸ ë¹„êµ

| Img | Exp0 (4p,s=0) | Exp1-A (4p,s=0.5) | Exp1-B (4p,s=1.0) | Exp3-A (2p,s=0.5) | Exp3-B (2p,s=1.0) |
|-----|---------------|-------------------|-------------------|-------------------|-------------------|
| 0 | 14.92 | 14.92 | 14.92 | 13.86 | 13.86 |
| 1 | 19.35 | 19.38 | 19.35 | 19.35 | 19.38 |
| 2 | 15.12 | **15.85** | 14.41 | 14.52 | **15.91** |
| 3 | 19.39 | **19.57** | 18.97 | 19.27 | 19.32 |
| 4 | 13.93 | 13.72 | **13.68** | 12.20 | **13.95** |
| 5 | 18.51 | 18.47 | **18.89** | 18.45 | 18.47 |
| 6 | 20.78 | 20.55 | **20.99** | 20.53 | 20.55 |
| 7 | 19.21 | 18.38 | 18.39 | **18.49** | 18.40 |
| 8 | 10.28 | **11.10** | **13.65** â­ | 11.87 | 10.15 |
| 9 | 23.48 | 23.48 | **23.55** | 23.47 | 23.47 |

- â­ **Image 8**: Exp1-B(s=1.0)ì—ì„œ +3.37 dB ê·¹ì  ê°œì„  (10.28â†’13.65)
- Bold: í•´ë‹¹ rowì—ì„œ ìµœê³  ì„±ëŠ¥

#### ê´€ì°° ë° ë¶„ì„

**1. Scale íš¨ê³¼ (Exp1)**
- scale=0.5 â†’ scale=1.0ìœ¼ë¡œ ì¦ê°€ ì‹œ Best PSNR +0.14 dB ê°œì„  (17.54 â†’ 17.68)
- Baseline ëŒ€ë¹„ scale=1.0ì—ì„œ +0.18 dB ê°œì„ 
- **ê²°ë¡ **: Repulsionì´ ì•½ê°„ì˜ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í•˜ë‚˜, íš¨ê³¼ê°€ í¬ì§€ ì•ŠìŒ

**2. Particle ìˆ˜ íš¨ê³¼ (Exp1 vs Exp3)**
- 4 particle â†’ 2 particle ê°ì†Œ ì‹œ Best PSNR ~0.3 dB í•˜ë½
- í•˜ì§€ë§Œ **Mean of MeansëŠ” 2 particleì´ ë” ë†’ìŒ** (16.04~16.14 vs 15.50~15.56)
  - ì´ëŠ” 4 particle ì¤‘ ì¼ë¶€ê°€ ë‚®ì€ ì ìˆ˜ë¥¼ ê¸°ë¡í•˜ê¸° ë•Œë¬¸
- ì‹œê°„/ë©”ëª¨ë¦¬ ì•½ ì ˆë°˜ìœ¼ë¡œ ì ˆì•½ (~48% ì‹œê°„, ~60% VRAM)

**3. Mean Pairwise Distance ì´ìŠˆ** âš ï¸
- **ëª¨ë“  ì‹¤í—˜ì—ì„œ ~32ë¡œ ê±°ì˜ ë™ì¼**
- scale=0.5ë‚˜ 1.0ì´ë‚˜ pairwise distance ì°¨ì´ ì—†ìŒ
- **ê°€ëŠ¥í•œ ì›ì¸**:
  1. DINO feature spaceì—ì„œ ì´ë¯¸ ì¶©ë¶„íˆ ë¶„ë¦¬ë˜ì–´ ìˆìŒ
  2. Repulsion gradientê°€ ì‹¤ì œ latent trajectoryì— í° ì˜í–¥ì„ ì£¼ì§€ ëª»í•¨
  3. Scaleì´ ì—¬ì „íˆ ë¶€ì¡±í•˜ê±°ë‚˜, score injection ë°©ì‹ì˜ í•œê³„

**4. íš¨ìœ¨ì„± ë¶„ì„**
- Exp3 (2 particle): ì‹œê°„ 48%, VRAM 60%ë¡œ ì„±ëŠ¥ ìœ ì§€ ê°€ëŠ¥
- Best PSNRì€ 4 particleì´ ìš°ì„¸í•˜ì§€ë§Œ, íš¨ìœ¨ì„± ë©´ì—ì„œ 2 particleë„ ê³ ë ¤ ê°€ëŠ¥

#### ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ

1. **Scale ì¶”ê°€ ì‹¤í—˜**: scale=2.0, 5.0 ë“± ë” í° ê°’ìœ¼ë¡œ repulsion íš¨ê³¼ í™•ì¸
2. **Sigma break ì¡°ì •**: repulsionì´ ë” ì˜¤ë˜ ìœ ì§€ë˜ë„ë¡ sigma_break ë‚®ì¶”ê¸° (0.5, 0.1)
3. **Pairwise distance ë””ë²„ê¹…**: repulsionì´ ì‹¤ì œë¡œ latent separationì„ ìœ ë„í•˜ëŠ”ì§€ stepë³„ ë¡œê¹… ê°•í™”
4. **ì‹¤í—˜ 2 (Pruning) ì§„í–‰**: 4â†’2 pruningìœ¼ë¡œ íš¨ìœ¨ì„± + ì„±ëŠ¥ ì–‘ë¦½ ê²€ì¦

### [ì‹¤í—˜ 1] Sanity Check (2025-12-14 KST) - scale=50 âš ï¸ ì‹¤íŒ¨

#### ëª©ì 
- scale=0.1~1.0ì—ì„œ pairwise distanceê°€ ~32ë¡œ ë³€í™” ì—†ì–´ì„œ, RLSD gamma=50 ìˆ˜ì¤€ìœ¼ë¡œ ëŒ€í­ ìƒí–¥
- repulsionì´ ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸

#### ê²°ê³¼: Repulsion ì‘ë™ í™•ì¸ âœ…, í•˜ì§€ë§Œ ë„ˆë¬´ ê°•í•¨ ğŸ’€

| Metric | scale=0.1 | scale=50 | ë³€í™” |
|--------|-----------|----------|------|
| **Best PSNR** | 11.24 | **6.49** | **-4.75 dB** ğŸ’€ |
| Best SSIM | 0.565 | 0.074 | -0.49 |
| Best LPIPS | 0.495 | 0.690 | +0.20 (worse) |
| **Pairwise Dist** | 32.13 | **128.94** | **+4x** âœ… |

#### Repulsion ë¡œê·¸ ë¶„ì„ (`repulsion.jsonl`)

| Step | Ïƒ | ratio_scaled_to_score | Pairwise Dist | í•´ì„ |
|------|-----|----------------------|---------------|------|
| 0 | 10.0 | **1.53 (153%)** | 21.1 | âš ï¸ repulsion > score |
| 1 | 9.3 | 0.65 (65%) | 44.5 | ì—¬ì „íˆ ë§¤ìš° ê°•í•¨ |
| 2 | 8.7 | 0.12 (12%) | 105.6 | ì…ì ë¶„ë¦¬ ì‹œì‘ |
| 5 | 7.1 | 0.046 (4.6%) | 129.3 | ì ì • ìˆ˜ì¤€ |
| 10 | 4.9 | 0.026 (2.6%) | 136.9 | |
| 30+ | <1.0 | 0.0 (OFF) | - | sigma_breakë¡œ OFF |

#### ë¶„ì„

**Good**:
- Repulsionì´ **í™•ì‹¤íˆ ì‘ë™í•¨**: pairwise distance 32 â†’ 129 (4ë°° ì¦ê°€)
- ë””ë²„ê¹… ë¡œê¹…(`repulsion.jsonl`)ì´ ì •ìƒ ì‘ë™

**Bad**:
- step 0ì—ì„œ `ratio_scaled_to_score=1.53` â†’ repulsionì´ scoreì˜ **153%**
- Diffusion process ìì²´ê°€ ë¶•ê´´ â†’ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨ (PSNR 6 dB)

#### ê²°ë¡  ë° ë‹¤ìŒ ë‹¨ê³„

- **scale=0.1~1.0**: íš¨ê³¼ ì—†ìŒ (ratio < 0.01, pairwise dist ~32)
- **scale=50**: ë„ˆë¬´ ê°•í•¨ (ratio > 1.0, ì´ë¯¸ì§€ ìƒì„± ë¶•ê´´)
- **ì ì • ë²”ìœ„ ì¶”ì •**: ratio_scaled_to_scoreê°€ **0.05~0.2** ì •ë„ ë˜ë ¤ë©´ **scale=5~15** í•„ìš” => 10 í•´ë³¸ ë’¤ 5 / 15 íƒ1 ì¶”ê°€ ì§„í–‰ ì˜ˆì •!
- ê²°ê³¼ í´ë”: `results/exp1_repulsion/imagenet_1img/exp1_sanity_check_scale50/`

#### Scale íŠœë‹ ì „ëµ (2025-12-14)

1. **scale=10 ë¨¼ì € ì‹œë„**
2. step 0ì—ì„œ `ratio_scaled_to_score` í™•ì¸:
   - ratioê°€ **0.3~0.5 ì´ìƒ**ì´ë©´ â†’ **scale=5**ë¡œ ë‚´ë¦¬ê¸°
   - ratioê°€ **0.05 ë¯¸ë§Œ**ìœ¼ë¡œ íš¨ê³¼ ì•½í•˜ë©´ â†’ **scale=15**ë¡œ ì˜¬ë¦¬ê¸°
3. ëª©í‘œ: ratioê°€ **0.1~0.3** ë²”ìœ„, pairwise distance ì¦ê°€í•˜ë©´ì„œ PSNR ìœ ì§€

### [ì‹¤í—˜ 1] Sanity Check (2025-12-14 KST) - scale=10 âœ… ì„±ê³µ

#### ê²°ê³¼ ìš”ì•½: Scale=10ì´ Sweet Spot!

| Scale | Best PSNR | Mean PSNR | Std | Pairwise Dist | step 0 ratio | íŒì • |
|-------|-----------|-----------|-----|---------------|--------------|------|
| 0.1 | 11.24 | 8.98 | 1.53 | 32.13 | ~0.001 | âŒ íš¨ê³¼ ì—†ìŒ |
| **10** | **20.66** | 12.83 | 6.12 | **70.70** | **0.0625** | âœ… **Sweet Spot** |
| 50 | 6.49 | 5.84 | 0.55 | 128.94 | 1.53 | ğŸ’€ ë¶•ê´´ |

#### PSNR ìƒì„¸ (scale=10)
- samples: [14.75, 20.66, 8.13, 7.80]
- **Best: 20.66** (sample 1) - baseline ëŒ€ë¹„ **+9.42 dB**
- Std: 6.12 (ë†’ì€ ë¶„ì‚° = ë‹¤ì–‘í•œ mode íƒìƒ‰ ì„±ê³µ)

#### Repulsion ë¡œê·¸ ë¶„ì„ (scale=10)

| Step | Ïƒ | ratio_scaled_to_score | Pairwise Dist | í•´ì„ |
|------|-----|----------------------|---------------|------|
| 0 | 10.0 | **0.0625 (6.25%)** | 21.1 | âœ… ì ì ˆí•œ ê°•ë„ |
| 5 | 7.1 | 0.024 (2.4%) | 67.2 | ë¶„ë¦¬ ì§„í–‰ |
| 10 | 4.9 | 0.028 (2.8%) | 60.4 | |
| 25 | 1.5 | 0.016 (1.6%) | 91.3 | ì•ˆì •í™” |
| 30+ | <1.0 | 0.0 (OFF) | - | sigma_break |

#### ë¶„ì„

**Good**:
- **Best PSNR 20.66**: baseline(11.24) ëŒ€ë¹„ +9.42 dB í–¥ìƒ
- **ratio ì ì ˆ**: 0.02~0.06 ë²”ìœ„ (score ì••ë„ X, íš¨ê³¼ O)
- **Pairwise dist ì¦ê°€**: 32 â†’ 71 (2.2ë°°) - ì‹¤ì œë¡œ ì…ì ë¶„ë¦¬ë¨
- **Sample ë‹¤ì–‘ì„±**: std=6.12ë¡œ ë†’ìŒ (0Â°/180Â° ë‹¤ë¥¸ mode íƒìƒ‰ ê°€ëŠ¥ì„±)

**ê²°ë¡ : scale=10 í™•ì • (Sweet Spot)** âœ…

step 0 ratioê°€ **0.0625 (6.25%)** ë¡œ ì•ˆì „+ì˜í–¥ ìˆëŠ” êµ¬ê°„(0.05~0.2)ì— ë”± ë“¤ì–´ì™”ê³ , PSNRì´ 20.66ê¹Œì§€ íŠ„ ê±´ repulsionì´ **'ì œëŒ€ë¡œ'** ë¬¸ì œë¥¼ í‘¼ ì‹ í˜¸.

**scale=5 / 15 ì¶”ê°€ íŠœë‹ì€ ìš°ì„ ìˆœìœ„ ë‚®ìŒ**:
- ì´ë¯¸ scale=10ì´ ratio ì ì • + PSNR í¬ê²Œ ê°œì„  + pairwise dist ìƒìŠ¹
- "ë” íŠœë‹í•´ì„œ +0.2dB ì–»ì" ë‹¨ê³„ê°€ ì•„ë‹˜
- ì§€ê¸ˆì€ **ì¬í˜„ì„±(10 images)** ê³¼ **2p ì•ˆì •ì„±(Exp3)** ì´ ë” ì¤‘ìš”

**ì˜ˆì™¸: scale=5 ë¹„êµê°€ í•„ìš”í•œ ê²½ìš°**:
- Exp1 10 imagesì—ì„œ PSNRì´ ì´ë¯¸ì§€ë§ˆë‹¤ ë“¤ì­‰ë‚ ì­‰í•˜ê±°ë‚˜
- ëª‡ ì¥ì—ì„œ ë¶•ê´´/ì•„í‹°íŒ©íŠ¸ê°€ ë³´ì´ë©´
- â†’ scale=5ë¥¼ "ì•ˆì „ ë²„ì „"ìœ¼ë¡œ ë¹„êµ (15ëŠ” ì˜¤íˆë ¤ ìœ„í—˜ ìª½)


#### ì§„í–‰ ìƒí™© (2025-12-14)

| ìˆœì„œ | ì‹¤í—˜ | ìƒíƒœ | ëª©ì  |
|------|------|------|------|
| 0 | Exp1 sanity check (4p, scale=10) | âœ… **ì™„ë£Œ** | scale íŠœë‹ â†’ **Sweet Spot í™•ì •** |
| 1 | Exp3 sanity check (2p, scale=10) | âœ… **ì™„ë£Œ** | N=2 ì•ˆì •ì„± í™•ì¸ â†’ **2p ëŒ€ë¹„ 4p ìš°ìœ„ í™•ì¸! (ê°€ì„¤ëŒ€ë¡œ)** |
| 2 | Exp1 10 images (4p, scale=10) | â³ ëŒ€ê¸° | ì¬í˜„ì„± ê²€ì¦ |
| 3 | Exp3 10 images (2p, scale=10) | â³ ëŒ€ê¸° | 2p vs 4p ë¹„êµ |

- Exp1 ê²°ê³¼ í´ë”: `results/exp1_repulsion/imagenet_1img/exp1_sanity_check_scale10/`
- Exp3 ê²°ê³¼ í´ë”: `results/exp3_2particle/imagenet_1img/exp3_sanity_check_scale10/`

### [ì‹¤í—˜ 3] Sanity Check (2025-12-14 KST) - scale=10, 2-particle âœ… ì™„ë£Œ

#### Exp1 vs Exp3 ë¹„êµ (1 Image, scale=10)

| Metric | Exp1 (4p) | Exp3 (2p) | ì°¨ì´ |
|--------|-----------|-----------|------|
| **Best PSNR** | **20.66** | 10.59 | **-10.07 dB** |
| Mean PSNR | 12.83 | 9.20 | -3.63 dB |
| Pairwise Dist | 70.70 | 59.27 | -11.43 |
| step 0 ratio | 0.0625 | 0.034 | ë” ì•½í•¨ |
| Time | 911ì´ˆ | 464ì´ˆ | **-49%** âœ… |
| VRAM | 10,209 MB | 6,066 MB | **-41%** âœ… |

#### Repulsion ë¡œê·¸ ë¶„ì„ (2-particle)

| Step | Ïƒ | ratio_scaled_to_score | Pairwise Dist | weights_mean |
|------|-----|----------------------|---------------|--------------|
| 0 | 10.0 | 0.034 (3.4%) | 20.8 | 0.5 âœ… |
| 5 | 7.1 | 0.019 | 51.9 | 0.5 |
| 25 | 1.5 | 0.011 | 73.9 | 0.5 |

#### ë¶„ì„ ë° í•µì‹¬ ë°œê²¬

**Good**:
- âœ… **N=2 bandwidth ë²„ê·¸ ìˆ˜ì • í™•ì¸**: NaN/crash ì—†ì´ ì •ìƒ ì™„ë£Œ
- âœ… **weights_mean = 0.5**: N=2ì—ì„œ ì •ìƒ ì‘ë™ (kernel weight ì •ê·œí™” OK)
- âœ… **ì‹œê°„ 49%, VRAM 41% ì ˆì•½**: íš¨ìœ¨ì„± ì¸¡ë©´ì—ì„œ ê¸°ëŒ€ëŒ€ë¡œ

**Critical Observation**:
- **4pê°€ 2pë³´ë‹¤ PSNR í›¨ì”¬ ë†’ìŒ** (20.66 vs 10.59, -10 dB)
- ì´ê±´ ë‹¨ìˆœ ë…¸ì´ì¦ˆ ì°¨ì´ê°€ ì•„ë‹ˆë¼, **2pê°€ ì¢‹ì€ ëª¨ë“œ(0Â°/180Â° ì¤‘ í•˜ë‚˜)ë¥¼ ëª» ì¡ê³  ë‹¤ë¥¸ basinìœ¼ë¡œ ë¹ ì§„ ì¼€ì´ìŠ¤**ì¼ í™•ë¥ ì´ ë†’ìŒ
- ratioê°€ 2pì—ì„œ ë” ì•½í•œ ê²ƒë„ ìì—°ìŠ¤ëŸ¬ì›€ (ì…ì ìˆ˜ê°€ ì ìœ¼ë©´ repulsion gradient êµ¬ì„± ìì²´ê°€ ë‹¬ë¼ì§€ê³ , "ì¢‹ì€ í›„ë³´ë¥¼ í¬í•¨í•  í™•ë¥ "ì´ ë–¨ì–´ì§)

#### ê²°ê³¼ í•´ì„: Exp2 Pruning ì „ëµì˜ ì„¤ë“ë ¥ â­

> **"2pëŠ” ì‹¸ì§€ë§Œ ë¶ˆì•ˆì •. 4pë¡œ íƒìƒ‰ í›„ 2ë¡œ ì¤„ì´ëŠ” pruningì´ sweet spot."**

ì´ ìŠ¤í† ë¦¬ê°€ **1ì¥ sanityì—ì„œë„ ë°”ë¡œ ë“œëŸ¬ë‚¬ë‹¤**:
- 4p Best PSNR 20.66 vs 2p 10.59 (-10 dB) ì°¨ì´ëŠ” **"í†µê³„ ë¶€ì¡±"ì„ ê°ì•ˆí•´ë„ ì‹ í˜¸ê°€ ë„ˆë¬´ ê°•í•´ì„œ ë°©í–¥ì„±ì´ ê±°ì˜ í™•ì •ì **
- 2pëŠ” ì²˜ìŒë¶€í„° 2ê°œë§Œ ë„ìš°ë©´ ë‘˜ ë‹¤ Local Minimaì— ë¹ ì§ˆ ìœ„í—˜ì´ ë†’ìŒ
- 4pëŠ” ì´ˆë°˜ì— ë„“ê²Œ íƒìƒ‰í•˜ì—¬ ì¢‹ì€ ëª¨ë“œë¥¼ ì°¾ì„ í™•ë¥ ì´ ë†’ìŒ

**â†’ Exp2 (4â†’2 Pruning)ì˜ ë…¼ë¦¬ê°€ í›¨ì”¬ ì„¤ë“ë ¥ ìˆì–´ì¡Œë‹¤:**
- 4ê°œë¡œ ì‹œì‘í•´ì„œ ë‹¤ì–‘í•œ basin íƒìƒ‰ â†’ ì¤‘ê°„ì— ìœ ë§í•œ 2ê°œë§Œ ë‚¨ê¹€
- **"ì²˜ìŒë¶€í„° 2ê°œë§Œ ì“°ë©´ ì•ˆ ë˜ë‚˜ìš”?"** ë¼ëŠ” ë¦¬ë·°ì–´ ì§ˆë¬¸ì— ëŒ€í•œ ê°•ë ¥í•œ ë°˜ë°• ê·¼ê±° í™•ë³´
- 1ì¥ sanity checkì—ì„œ ì´ë¯¸ -10 dB ì°¨ì´ â†’ **10 images ë¹„êµ ì—†ì´ë„ ë°©í–¥ì„± í™•ì •**

#### ë‹¤ìŒ ë‹¨ê³„

- 10 images ë¹„êµë¡œ í†µê³„ì  ìœ ì˜ì„± í™•ë³´ (optional, ì´ë¯¸ ê°•í•œ ì‹ í˜¸)
- **Exp2 (4â†’2 Pruning) êµ¬í˜„ ìš°ì„ ìˆœìœ„ ìƒìŠ¹** - pruningì´ ì •ë§ sweet spotì¸ì§€ ê²€ì¦

### [ì‹¤í—˜ 2] Sanity Check (2025-12-14 19:18 KST) - 4â†’2 Pruning âœ… ì™„ë£Œ

#### ì‹¤í—˜ ì„¤ì •
| Parameter | Value |
|-----------|-------|
| Particles | 4 â†’ 2 (pruning) |
| Repulsion Scale | 10 |
| Sigma Break | 1.0 |
| Pruning Step | 29 (Ïƒ ì „í™˜ ì§í›„) |
| Images | 1 (sanity check) |

#### ëª…ë ¹ì–´
```bash
bash commands_gpu/exp2_pruning.sh --1
```

#### ê²°ê³¼

| Metric | Exp1 (4p, no prune) | Exp2 (4â†’2 prune) | ì°¨ì´ |
|--------|---------------------|------------------|------|
| **Best PSNR** | **20.66 dB** | **14.71 dB** | **-5.95 dB âš ï¸** |
| Mean PSNR | 12.83 dB | 11.46 dB | -1.37 dB |
| Best SSIM | 0.757 | 0.641 | -0.116 |
| Best LPIPS | 0.413 | 0.501 | +0.088 (â†‘worse) |
| Time | 911ì´ˆ | 734ì´ˆ | **-19% ì ˆì•½** |
| VRAM (pre_pruning) | 10209 MB | 10209 MB | ë™ì¼ |
| VRAM (post_pruning) | - | 6068 MB | **-40% ì ˆì•½** |

#### Pruning ë¡œê·¸ ë¶„ì„ (`pruning.jsonl`)
```json
{
  "prune_step": 29,
  "prev_sigma": 1.105, "curr_sigma": 1.007,
  "losses": [1280.43, 1292.52, 1317.48, 1288.60],
  "kept_indices": [0, 3],
  "pruned_indices": [1, 2],
  "batch_before": 4, "batch_after": 2,
  "did_prune": true, "mode": "slicing"
}
```

#### í•µì‹¬ ê²€ì¦ í¬ì¸íŠ¸
- [x] pruning ì‹œì (step 29)ì—ì„œ ì •í™•íˆ 4â†’2 ì „í™˜ë˜ëŠ”ì§€ âœ…
- [x] VRAM segments ì •ìƒ ê¸°ë¡ (`pre_pruning: 10209`, `post_pruning: 6068`) âœ…
- [x] trajectory_pruned/ í´ë”ì— íƒˆë½ particle ì €ì¥ âœ… (`00000_pruned00.png`, `00000_pruned01.png`)
- [x] Time/VRAM ì ˆì•½ëŸ‰ ì¸¡ì • âœ… (ì‹œê°„ -19%, VRAM -40%)
- [ ] **Best PSNRì´ Exp1ê³¼ ìœ ì‚¬í•œì§€** â†’ âŒ **-5.95 dB í•˜ë½ (ì‹¬ê°)**

#### âš ï¸ ì¤‘ìš” ë°œê²¬: Pruningì´ Best Sampleì„ ì œê±°í•¨

**Exp1 4ê°œ ìƒ˜í”Œ ìµœì¢… PSNR**:
| Sample | PSNR | Pruning Loss | ê²°ê³¼ |
|--------|------|--------------|------|
| 0 | 14.75 dB | 1280.43 | âœ… kept |
| **1** | **20.66 dB** | 1292.52 | âŒ **pruned** |
| 2 | 8.13 dB | 1317.48 | âŒ pruned |
| 3 | 7.80 dB | 1288.60 | âœ… kept |

â†’ **step 29ì—ì„œ measurement lossê°€ ë‘ ë²ˆì§¸ë¡œ ë†’ì•˜ë˜ sample[1]ì´ pruningë˜ì—ˆì§€ë§Œ, ì´ ìƒ˜í”Œì´ ì‹¤ì œ ìµœê³  PSNR(20.66 dB)ì„ ê¸°ë¡í•  ìƒ˜í”Œì´ì—ˆìŒ.**

**ë¶„ì„**: Pruning ì‹œì (step 29)ì˜ measurement lossê°€ ìµœì¢… PSNRì„ ì™„ë²½í•˜ê²Œ ì˜ˆì¸¡í•˜ì§€ ëª»í•¨.
- ì´ëŠ” phase retrievalì˜ íŠ¹ì„±ìƒ ì¤‘ê°„ ë‹¨ê³„ lossì™€ ìµœì¢… í’ˆì§ˆì´ ë¹„ì„ í˜• ê´€ê³„ì¼ ìˆ˜ ìˆìŒì„ ì‹œì‚¬
- ë‹¤ë§Œ 1 ì´ë¯¸ì§€ sanity checkì´ë¯€ë¡œ, **10 image ì‹¤í—˜ì—ì„œ í†µê³„ì  ê²€ì¦ í•„ìš”**

#### Repulsion ë™ì‘ í™•ì¸ (`repulsion.jsonl`)
- step 0~29: repulsion ON (scale=10, ratio ~3~6%)
- step 30~: repulsion OFF (sigma < sigma_break=1.0)
- mean_pairwise_dino_dist: 21 â†’ 90 (ì¶©ë¶„íˆ í¼ì§) âœ…

#### ê²°ê³¼ í´ë”
- `results/exp2_pruning/imagenet_1img/exp2_sanity_check_scale10_prune29/`

#### ë‹¤ìŒ ë‹¨ê³„
- **10 image ì‹¤í—˜ ì§„í–‰**: 1 ì´ë¯¸ì§€ì—ì„œëŠ” ìš´ ë‚˜ì˜ê²Œ best sampleì´ pruningë¨. í†µê³„ì ìœ¼ë¡œ pruningì´ í‰ê· ì ìœ¼ë¡œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ”ì§€ ê²€ì¦ í•„ìš”.
- **Pruning ê¸°ì¤€ ê°œì„  ê²€í† **: measurement loss ì™¸ ë‹¤ë¥¸ ê¸°ì¤€ (diversity-aware pruning ë“±) ê³ ë ¤ ê°€ëŠ¥

## í”„ë¡œì íŠ¸ ê¸°ëŒ€ ê²°ê³¼: ë³´ë‹¤ ì ì€ ì—°ì‚°ìœ¼ë¡œ ë¹„ìŠ·í•˜ê±°ë‚˜ ë” ì¢‹ì€ ì„±ëŠ¥ì„!
- DAPSì—ì„œ Phase Retrievalì˜ ë¶ˆì•ˆì •ì„±ì„ ê³ ë ¤í•˜ì—¬, 4ë²ˆì˜ independent runsì„ ìˆ˜í–‰í•œ ë’¤ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ë¥¼ ì„ íƒí•˜ì—¬ ë³´ê³ í–ˆìœ¼ë‹ˆ, ìš°ë¦¬í”Œì ì„ DAPS 4 runì´ë‘ ë¹„êµí–ˆì„ë•Œ ì‹œê°„xGPU ì‚¬ìš©ëŸ‰ì´ ë¹„ìŠ·í•˜ê±°ë‚˜ ì‘ìœ¼ë©´ì„œ ì„±ëŠ¥ì´ ë¹„ìŠ·í•˜ê±°ë‚˜ ë†’ìŒì„ ë³´ì´ë©´ ë˜ëŠ” ê²ƒ!
- ì‹¤í—˜ 2 (4 â†’ 2 Pruning)**ëŠ” ì´ë¡ ì  ìµœì ì (2 Modes)ê³¼ í˜„ì‹¤ì  ì•ˆì „ì¥ì¹˜(4 Runs) ì‚¬ì´ì˜ **"Sweet Spot"**ì„ ì°¾ëŠ” ì„¤ì •
- max ê°’ ë¿ë§Œ ì•„ë‹ˆë¼ std ë“± ë¶„í¬ë¥¼ ê°€ì§€ê³ ë„ ì˜ë¯¸ìˆëŠ” ë¶„ì„ì„ í•´ë³¼ ìˆ˜ ìˆì„ ê²ƒ.


## êµ¬í˜„ ê°€ì´ë“œ

### ì‹¤í—˜1 Repulsion
- ëª¨ë“  Measurement Operator($\mathcal{A}$)ì™€ Loss Functionì€ (B, C, H, W) í˜•íƒœì˜ ì…ë ¥ì„ ë°›ì•„ **ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë³‘ë ¬ ì—°ì‚°(Broadcasting)**ì´ ê°€ëŠ¥í•˜ë„ë¡ ì‘ì„±ë˜ì–´ì•¼ í•œë‹¤. for ë£¨í”„ë¡œ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•˜ì§€ ë§ê³  PyTorchì˜ í…ì„œ ì—°ì‚°ì„ ì“¸ ê²ƒ!
- ìš°ë¦¬ëŠ” í•˜ë‚˜ì˜ $y$(ì¸¡ì •ê°’)ì— ëŒ€í•´ 2~4ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ $z_T$(ì´ˆê¸° ë…¸ì´ì¦ˆ)ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. Data Loaderì—ì„œ ì´ë¯¸ì§€ 1ì¥ì„ ê°€ì ¸ì˜¤ë©´, ì´ë¥¼ **batch_size=2~4ë¡œ ë³µì œ(repeat)**í•˜ë˜, ì´ˆê¸° ë…¸ì´ì¦ˆ $z_T$ëŠ” torch.randn(2~4, ...)ë¡œ ì„œë¡œ ë‹¤ë¥´ê²Œ ìƒì„±ë˜ë„ë¡ ì½”ë“œë¥¼ ì§¤ ê²ƒ!
- ~~ë³´í†µ Diffusion InferenceëŠ” with torch.no_grad(): ì•ˆì—ì„œ ë•ë‹ˆë‹¤. í•˜ì§€ë§Œ ìš°ë¦¬ëŠ” **Repulsion($\nabla_z \Phi$)**ê³¼ ReSample Optimization($\nabla_z \|y - Ax\|^2$) ë•Œë¬¸ì— ì‹¤í—˜ 1~5ì—ì„œ Gradientê°€ í•„ìš”í•  ì˜ˆì •ì´ë‹¤. ë”°ë¼ì„œ, Samplerì˜ ë©”ì¸ ë£¨í”„ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ Gradient ê³„ì‚°ì´ ê°€ëŠ¥í•˜ë„ë¡ ì—´ì–´ë‘ê³ (enable_grad), í•„ìš”í•œ ë¶€ë¶„ì—ì„œë§Œ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ no_gradë¥¼ ì“°ê±°ë‚˜, í˜¹ì€ ë°˜ëŒ€ë¡œ no_grad ë² ì´ìŠ¤ì— íŠ¹ì • ìŠ¤í…(PG, Optimization)ì—ì„œë§Œ enable_gradë¥¼ ì¼œëŠ” í† ê¸€(Toggle) êµ¬ì¡°ë¥¼ ë¯¸ë¦¬ ì‹¤í—˜ 0ì—ì„œë¶€í„° ë§Œë“¤ì–´ì•¼ í•œë‹¤!~~ â†’ **ì™„ë£Œ**: `sampler.py`ì˜ `LatentDAPS.sample()`ì—ì„œ `torch.set_grad_enabled(step_needs_grad)` êµ¬ì¡° êµ¬í˜„. `do_repulsion`ê³¼ `do_optimization` flagë¡œ stepë³„ gradient í™œì„±í™” ì œì–´. ì‹¤í—˜ 1, 2, 4 ë¡œì§ì€ TODO ì£¼ì„ìœ¼ë¡œ ì¤€ë¹„ë¨.
- ~~ì‹¤í—˜ 0~5ë¥¼ ìŠ¤í¬ë¦½íŠ¸ í•˜ë‚˜ë¡œ ì œì–´í•˜ë ¤ë©´ Flag ì„¤ê³„ê°€ ì¤‘ìš”í•˜ë‹¤. ë‹¤ìŒ Argumentë“¤ì„ ë¯¸ë¦¬ ì •ì˜í•´ ë‘˜ ê²ƒ!~~ â†’ **ì™„ë£Œ**: `configs/default.yaml`ì— ì •ì˜ë¨
    - `num_samples` (int): í•œ ë²ˆì— ìƒì„±í•  ì…ì(ì´ë¯¸ì§€)ì˜ ê°œìˆ˜ (ê¸°ì¡´ DAPSì˜ num_samplesë¥¼ ê·¸ëŒ€ë¡œ í™œìš©, particle_num ì—­í• )
    - `repulsion_scale` (float): ì…ìë¼ë¦¬ ë°€ì–´ë‚´ëŠ” í˜ì˜ ì´ˆê¸° ê°•ë„. 0.0ì´ë©´ ë…ë¦½ ì‹¤í–‰ (DAPS baseline), >0.0ì´ë©´ ì„œë¡œ ë°€ì–´ëƒ„
    - `pruning_step` (int): ê°€ì§€ì¹˜ê¸° ìˆ˜í–‰ timestep. -1ì´ë©´ pruning ì—†ìŒ
    - `hard_data_consistency` (int): latent optimization ì‹œì‘ timestep. -1ì´ë©´ optimization ì—†ìŒ
    - `use_tpu` (bool): TPU ì‚¬ìš© ì—¬ë¶€.
    - (num_eval_imagesëŠ” data configì—ì„œ ì œì–´)
- ì‹¤í—˜ë³„ argument ì„¸íŒ… ê°€ì´ë“œ:
    Exp 0: Baseline (DAPS Replication)particle_num=4, repulsion_scale=0.0:ì´ë ‡ê²Œ ì„¤ì •í•˜ë©´ 4ê°œì˜ ì…ìê°€ ì„œë¡œ ê°„ì„­í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, DAPS ë…¼ë¬¸ì—ì„œ "1ê°œì”© 4ë²ˆ ëŒë¦° ê²ƒ(4 runs)"ê³¼ ìˆ˜í•™ì ìœ¼ë¡œ ì™„ì „íˆ ë™ì¼í•œ ê²°ê³¼ë¥¼ ëƒ…ë‹ˆë‹¤. (ì‹œë“œë§Œ ì˜ ì œì–´ëœë‹¤ë©´)ì´ê²ƒì´ ìš°ë¦¬ì˜ Reference ì„±ëŠ¥ì´ ë©ë‹ˆë‹¤.
    Exp 1: Repulsion Onlyrepulsion_scale > 0:ì´ì œ 4ê°œì˜ ì…ìê°€ ì„œë¡œ ë°€ì–´ëƒ…ë‹ˆë‹¤.ëª©í‘œ: Exp 0ë³´ë‹¤ **ë‹¤ì–‘ì„±(Std)**ì´ ë†’ê³ , **ìµœê³ ì (Max PSNR)**ì´ ë†’ê²Œ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    Exp 2: Efficiency (Pruning)pruning_step=200:ì½”ë“œëŠ” $t=200$ì´ ë˜ëŠ” ìˆœê°„, Lossì™€ Distanceë¥¼ ê³„ì‚°í•˜ì—¬ **4ê°œ ì¤‘ 2ê°œë¥¼ ë©”ëª¨ë¦¬ì—ì„œ ì‚­ì œ(ë˜ëŠ” Masking)**í•´ì•¼ í•©ë‹ˆë‹¤.ëª©í‘œ: Exp 1ê³¼ ì„±ëŠ¥ì€ ë¹„ìŠ·í•œë°, **ì‹œê°„(Time)ê³¼ ë©”ëª¨ë¦¬(VRAM)**ê°€ ì¤„ì–´ë“œëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    Exp 4: Quality (Optimization)hard_data_consistency=200:$t=1000 \to 201$ê¹Œì§€ëŠ” Repulsionìœ¼ë¡œ íƒìƒ‰í•˜ê³ ,$t=200 \to 0$ë¶€í„°ëŠ” Repulsionì„ ë„ê³ (scale=0 ê°•ì œ ì ìš©), Latent Optimizationì„ ì¼­ë‹ˆë‹¤.ëª©í‘œ: Exp 2ë³´ë‹¤ PSNRì´ í™•ì‹¤íˆ ë” ì˜¬ë¼ê°€ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
- metric.jsonì— phaseë³„ time, gpu, optimization íšŸìˆ˜/ì‹œê°„ì„ ê¸°ë¡í•  ê²ƒ
- metric.jsonì„ Parsingí•˜ëŠ” ì½”ë“œë¥¼ ë§Œë“¤ ê²ƒ
- ì½”ë“œ ì‹¤í–‰ì„ í†µí•œ sanity checkëŠ” GPUê°€ ë‹¬ë¦° ì„œë²„ì—ì„œ ì§„í–‰í•  ì˜ˆì •! (ë¡œì»¬ ë§¥ë¶ X)
- TODOë¥¼ ì™„ë£Œí•œ ê²½ìš° ì´ PROJECT.md íŒŒì¼ì— ì·¨ì†Œì„ ì„ ê·¸ì–´ í‘œì‹œí•  ê²ƒ! ë§Œì•½ ë…¼ì˜ ê²°ê³¼ md ì„¤ëª…ë³´ë‹¤ ë” ì í•©í•œ ì„ íƒì§€ê°€ ìˆì–´ì„œ ì‹¤ì œ êµ¬í˜„ì— ì°¨ì´ê°€ ìƒê¸´ ê²½ìš° PROJECT.mdë¥¼ ì—…ë°ì´íŠ¸í•  ê²ƒ!
- git commit messageëŠ” í•œ/ì˜ í˜¼ìš© ê°€ëŠ¥, ì‹¤í—˜ ëª‡ì„ ì¤€ë¹„í•˜ê³  ìˆëŠ”ì§€ ëª…ì‹œ, í•œ ì¤„ ì´ë‚´ë¡œ ì‘ì„±. commitì€ vscode guië¡œë§Œ ì§„í–‰
- í˜„ í´ë”ëŠ” DAPS ë ˆí¬ë¥¼ ë² ì´ìŠ¤ë¡œ ìˆ˜ì • ì¤‘ì— ìˆìœ¼ë©°, TDP ë° ReSample ê´€ë ¨ ì„¸ë¶€ì‚¬í•­ì€ ì¶”í›„ í•´ë‹¹ ì‹¤í—˜ êµ¬í˜„ ë‹¨ê³„ì—ì„œ ì¶”ê°€ ì˜ˆì •
- ~~command íŒŒì¼ë“¤ì— ìƒˆë¡œìš´ argumentë“¤ ë°˜ì˜ ë° 1/10/100 imageìš© command ì¶”ê°€~~ â†’ **ì™„ë£Œ**: í´ë” êµ¬ì¡°:
    - `commands_gpu/`: GPU (CUDA) ì „ìš© ëª…ë ¹ì–´ (use_tpu=false)
    - ê° í´ë”ì— `exp0_baseline.sh` ~ `exp5_final.sh` í¬í•¨
    - ëª¨ë“  commandì— `repulsion_scale`, `pruning_step`, `hard_data_consistency`, `data.end_id` ë°˜ì˜





ì•ˆë…•, ë‹¤ìŒì— ë”°ë¼ â€œRLSD repulsionì„ LatentDAPS(EDM Ïƒ + xâ‚€-pred)ë¡œ score-level injection ë°©ì‹ìœ¼ë¡œ ì´ì‹â€í•˜ëŠ” ì‘ì—…ì„ ê°€ì¥ ì•ˆì „í•˜ê²Œ ì§„í–‰í•´ì£¼ë¼.
(pruning/optimizationì€ ì•„ì§ ì œì™¸, ì´ ë¬¸ì„œì—ì„œ Exp1/3ë§Œ íƒ€ê²Ÿ)

You have access to two local repos:
	â€¢	Repo DAPS (target): my modified LatentDAPS / DAPS codebase, I changed vanilla DAPS so please be aware that content's different from original DAPS repo. You can refer to this PROJECT.md for concrete implementation done and planned. Though the repo name is DAPS, I am interested in LatentDAPS setting only.
	â€¢	Repo RLSD (source): RLSD (Repulsive Latent Score Distillation) official repo
	(Please ignore repo named 'DAPS_modified' since it's outdated.)

Goal: implement particle repulsion for Exp1/Exp3 (full-run 4 or 2 particles, no pruning, no optimization) by porting RLSDâ€™s repulsion module into LatentDAPS.

Context / What we know (important)

Target repo (LatentDAPS) uses:
	â€¢	EDM sigma parameterization (Ïƒ), not DDPM alpha. Evidence:
	â€¢	uses annealing_scheduler.sigma_steps[step]
	â€¢	forward diffusion like x_t = x0 + Ïƒ * Îµ
	â€¢	has sigma_max and prior_sigma
	â€¢	Prediction target is x0-prediction (denoiser), NOT eps-pred.
	â€¢	In DiffusionPFODE.derivative():
		return dst / st * xt - st * dsigma_t * sigma_t * self.model.score(xt/st, sigma=sigma_t)
	â€¢	model.score() is derived from denoiser D(x;Ïƒ) via EDM relation:
		score = (D(x;Ïƒ) - x) / Ïƒ^2

We want repulsion injection method:

Method to use: Add repulsion directly to score.
Rationale:
	â€¢	Equivalent to modifying denoiser output by + Î³ Ïƒ^2 repulsion, but simpler:
	â€¢	If score = (D-x)/Ïƒ^2, then D' = D + Î³ Ïƒ^2 repulsion â‡’ score' = score + Î³ repulsion
	â€¢	Matches DAPS update form (prior gradient + data gradient). Repulsion is an extra regularizer/guidance term added to the â€œprior gradient directionâ€.
	â€¢	We will implement sampler-loop computed repulsion, store it in pfode, and add it in DiffusionPFODE.derivative():

		# sampler loop
		repulsion = compute_repulsion(zt)
		pfode.set_repulsion(repulsion, scale=current_scale)

		# DiffusionPFODE.derivative
		score = self.model.score(...)
		if self.repulsion is not None:
			score = score + self.repulsion_scale * self.repulsion

		do not wrap model, maintain sampler -> pfode passing structure for clarity in responsibility and on/off control and debuggability.

Repulsion should be ON only for early/high-noise interval and decay to 0:
	â€¢	RLSD uses if sigma > sigma_break to enable repulsion.
	â€¢	We will implement interval on/off with alpha(Ïƒ) schedule (linear or cosine) such that alpha -> 0 as Ïƒâ†’0.
	â€¢	For now implement something simple: repulsion active for sigma > sigma_break, and within that, scale by alpha = repulsion_scale * schedule(sigma).

RLSD repulsion reference implementation (source repo):

RLSD repulsion core in rsd.py lines ~123-165 (already identified). It:
	â€¢	decodes latent to image
	â€¢	extracts DINO features
	â€¢	computes pairwise differences in feature space
	â€¢	uses RBF kernel with median heuristic bandwidth
	â€¢	computes SVGD-style repulsive gradient
	â€¢	backprops from feature space to latent using vector-Jacobian trick:
		eval_sum = torch.sum(dino_out * grad_phi.detach())
		deps_dx_backprop = torch.autograd.grad(eval_sum, latent_pred_t)[0]
	â€¢	normalizes by kernel sum

âš ï¸ Important fix:
RLSD code uses h = median(dist)^2 / log(N-1). This breaks for N=2 (log(1)=0) which is our project's Exp3 setting.
We will use a safe denominator: log(N) (or max(log(N), eps)) so that Exp3 (2 particles) works.

Deliverables / Tasks

Task 1: Identify integration points in LatentDAPS (target repo)

Find where:
	â€¢	multiple particles are represented as a batch (num_samples already exists)
	â€¢	sampler loop produces the latent state zt (or equivalent) each step
	â€¢	pfode (DiffusionPFODE) is called for derivative or stepping

We need:
	1.	A function to compute repulsion given current latent batch.
	2.	Store repulsion in pfode (or scheduler) object so derivative() can access it.
	3.	Add repulsion to score in DiffusionPFODE.derivative() before itâ€™s used in drift.

Task 2: Port RLSD repulsion computation into target repo

Implement something like:

repulsion = compute_repulsion(latents, sigma_t)

	â€¢	Use RLSDâ€™s DINO-based feature space repulsion.
	â€¢	Use decode_latents(latents) from target repo (or equivalent) to get images.
	â€¢	Use DINO-ViT model (frozen, eval mode).
	â€¢	Ensure gradients flow back to latent: need latent.requires_grad_(True) in repulsion-on steps.
	â€¢	Use vector-Jacobian trick like RLSD (avoid second-order).
	â€¢	Bandwidth: median(dist)^2 / max(log(N), eps)
	â€¢	compute pairwise distances
	â€¢	h = median(dist)**2 / max(log(N), eps)  âœ… (fix N=2)
	â€¢	Normalize by kernel sum similarly.

Task 3: Add config options (minimal)

Expose in config / args:
	â€¢	repulsion_scale (float, default 0.0)
	â€¢	sigma_break or repulsion_sigma_break (float or step index)
	â€¢	optional repulsion_schedule (linear default)

Repulsion behavior:
	â€¢	If repulsion_scale == 0, it must exactly reproduce DAPS baseline (independent chains).
	â€¢	If on, only apply when sigma > sigma_break.

Task 4: Ensure correct grad toggling and memory safety
	â€¢	Only enable autograd for repulsion steps.
	â€¢	DINO parameters must have requires_grad=False, but input must require grad.
	â€¢	DINO input should be resized to 224 (if RLSD does). Use same preprocessing as RLSD repo. But be aware that the input images' resolutions are different; DAPS handles 256x256 while RLSD handles 512x512 and latent resolution may be different across two repos. Adjust accordingly and tell me what you did. Ask me if any uncertainty. Place debugging code if needed and tell me that there are debugging code I have to check the outputs.

Task 5: Logging for sanity (Exp1/3)

Add to metrics/logging:
	â€¢	mean pairwise DINO feature distance per step
	â€¢	whether repulsion was on/off
	â€¢	optionally norm of repulsion gradient
	â€¢	time per repulsion computation
Be aware of the bugs resulting from new logging terms.

This is crucial to verify repulsion is actually acting.

Task 6: Provide a minimal test run plan - note that each sanity test may take 15+ minutes.
	â€¢	run 1 image sanity with 4 particles, repulsion on; check:
	â€¢	pairwise distance increases early
	â€¢	later decreases/stabilizes when repulsion off/decayed
	â€¢	no NaNs
	â€¢	run 1 image sanity with 2 particles; ensure no crash (bandwidth fix works).

Implementation guidance (preferred architecture)

Do NOT wrap/modify model.score() logic deeply. Keep responsibilities separated:
	â€¢	sampler computes repulsion and sets it on pfode each step:
		pfode.set_repulsion(repulsion, scale=alpha_sigma)
	â€¢	pfodeâ€™s derivative() adds it:
		score = score + self.repulsion_scale * self.repulsion
êµ¬ì¡°:
	â€¢	sampler: â€œì–¸ì œ/ì–¼ë§ˆë‚˜ repulsionâ€
	â€¢	model: â€œìˆœìˆ˜ denoisingâ€
	â€¢	pfode: â€œODE drift ê³„ì‚°â€

This allows easy on/off scheduling and debugging.

Output requirements
	â€¢	Make a clean PR-style change:
	â€¢	new module file for repulsion (e.g., repulsion.py)
	â€¢	minimal changes in sampler loop and pfode derivative
	â€¢	config updated, especially for exp1/3 sh commands file! and other files too; turn off repulsion option for baseline exp0.
	â€¢	Summarize:
	â€¢	files changed
	â€¢	key design decisions
	â€¢	how to run Exp1/Exp3
	â€¢	any assumptions or TODOs

Please proceed by:
	1.	scanning target repo to find exact insertion points and existing decoding utilities
	2.	mapping RLSD code dependencies (DINO loading, preprocessing, additionally required env setting: pip requirements, download sh, etc.)
	3.	implementing and testing quickly with 1-image run (if any additional installation or download is needed, do so, and document it inside DAPS requirements and download scripts)
	4.	report back with patch summary and instructions.

### ì‹¤í—˜2 Pruning

0) exp2 sh ìˆ˜ì • (í•„ìˆ˜)
	â€¢	commands_gpu/exp2_pruning.shì—ì„œ pruning_step=29ë¡œ ë³€ê²½í•´ì¤˜.
	â€¢	ì£¼ì„ìœ¼ë¡œ ì•„ë˜ ê·¼ê±°ë¥¼ ëª…í™•íˆ ì¨ì¤˜ (ìš°ë¦¬ ì„¸íŒ… ê¸°ì¤€, zero-indexed):
	â€¢	repulsion_sigma_break=1.0ì¼ ë•Œ Ïƒ ì „í™˜:
	â€¢	step 28: Ïƒ=1.0482 (Repulsion ON ë§ˆì§€ë§‰)
	â€¢	step 29: Ïƒ=0.9525 (Repulsion OFF ì „í™˜ ì§í›„, ì²˜ìŒ Ïƒ<1.0)
	â€¢	ë”°ë¼ì„œ â€œrepulsionì´ ëë‚œ ì§í›„ pruningâ€ì„ í•˜ë ¤ë©´ pruning_step=29ê°€ ë§ìŒ

ë‹¨, ì½”ë“œ êµ¬í˜„ì€ step=29 í•˜ë“œì½”ë”©ë§Œ ì˜ì¡´í•˜ì§€ ë§ê³ 
prev_sigma >= break && curr_sigma < break ì „í™˜ ê°ì§€ + did_prune í”Œë˜ê·¸ë¡œ ì •í™•íˆ 1íšŒë§Œ ìˆ˜í–‰ë˜ê²Œ í•´ì¤˜.

â¸»

1) Pruning ìˆ˜í–‰ ì¡°ê±´ (ì •í™•íˆ 1íšŒ)

íŠ¸ë¦¬ê±° ì •ì˜(ë‘˜ ë‹¤ ë§Œì¡±í•´ì•¼ prune ì‹¤í–‰)
	â€¢	pruning_step != -1 ì¼ ë•Œë§Œ í™œì„±í™”
	â€¢	ì•„ë˜ ì¤‘ í•˜ë‚˜ë¥¼ ë§Œì¡±í•˜ë©´ pruning ì‹¤í–‰(ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒí•´ì„œ êµ¬í˜„í•´ë„ OK):
	1.	step ê¸°ë°˜(ì‹¤í—˜ í†µì œìš©): annealing_step == pruning_step
	2.	ì „í™˜ ê¸°ë°˜(ì•ˆì „ì¥ì¹˜): prev_sigma >= repulsion_sigma_break and curr_sigma < repulsion_sigma_break
	â€¢	ê·¸ë¦¬ê³  did_pruneê°€ falseì¼ ë•Œë§Œ ì‹¤í–‰ (ì‹¤í–‰ í›„ trueë¡œ ê³ ì •)

ê¶Œì¥: (1) + (2)ë¥¼ ë‘˜ ë‹¤ ë„£ê³ , ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ë§Œì¡±í•˜ë©´ prune ì‹¤í–‰í•˜ë˜, did_pruneë¡œ 1íšŒë§Œ ë³´ì¥.

â¸»

2) Pruning ê¸°ì¤€ (loss-only)
	â€¢	ê¸°ì¤€ì€ measurement lossë¡œë§Œ (DINO distance ê¸°ë°˜ ê¸°ì¤€ì€ ì´ë²ˆ êµ¬í˜„ì—ì„œ ì œì™¸)
	â€¢	pruning ì‹œì ì— batch=4 particles ê°ê°ì— ëŒ€í•´ measurement lossë¥¼ ê³„ì‚°í•˜ê³ ,
	â€¢	lossê°€ ê°€ì¥ ì‘ì€ 2ê°œ(top-2)ë¥¼ keep
	â€¢	êµ¬í˜„ ë””í…Œì¼:
	â€¢	loss shapeëŠ” (B,)ê°€ ë˜ë„ë¡(=particleë³„ ìŠ¤ì¹¼ë¼)
	â€¢	ì •ë ¬ì€ torch.topk(loss, k=2, largest=False) í˜•íƒœ ê¶Œì¥

â¸»

3) Pruning êµ¬í˜„ ë°©ì‹ (ê¸°ë³¸ slicing + fallback masking)

3.1 ê¸°ë³¸: slicing (ì§„ì§œ 4â†’2ë¡œ batch ì¤„ì´ê¸°)

pruning í›„ì—ëŠ” batchê°€ ì‹¤ì œë¡œ 2ê°€ ë˜ì–´ì•¼ í•¨.
	â€¢	slicing ëŒ€ìƒ(ìµœì†Œ):
	â€¢	latent/state í…ì„œë“¤(zt, x_t, x0hat, z0hat ë“± sampler loopì—ì„œ ì´í›„ stepì— ê³„ì† ì“°ì´ëŠ” ëª¨ë“  batch í…ì„œ)
	â€¢	measurement yê°€ batch repeat ë˜ì–´ ìˆìœ¼ë©´ yë„ ê°™ì´
	â€¢	measurement operator ì…ë ¥ì— ì“°ì´ëŠ” í…ì„œë“¤ë„ ê°™ì´
	â€¢	trajectory ì €ì¥ ë²„í¼/ì¤‘ê°„ ê²°ê³¼ ìºì‹œê°€ batch dimensionì„ ê°–ê³  ìˆìœ¼ë©´ ê°™ì´
	â€¢	seed/idx í…ì„œ(ìˆë‹¤ë©´)

êµ¬í˜„ íŒ: â€œprune ì§í›„ì˜ ì‚´ì•„ë‚¨ì€ batch sizeâ€ëŠ” í•­ìƒ z.shape[0]ë¡œ ë‹¤ì‹œ ì½ê³ , í•˜ë“œì½”ë”©ëœ num_samples(=4)ì„ ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ.

3.2 fallback: masking ì˜µì…˜(ì•ˆì „ì¥ì¹˜)

slicingì´ ì–´ë ¤ì›Œì„œ ì—ëŸ¬ê°€ ë‚˜ë©´:
	â€¢	â€œíƒˆë½ particleì€ update/grad/ODE stepì„ ìˆ˜í–‰í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ carryâ€í•˜ëŠ” masking ê²½ë¡œë¥¼ ì˜µì…˜ìœ¼ë¡œ ë‚¨ê²¨ì¤˜.
	â€¢	ë‹¨, ê¸°ë³¸ ê²½ë¡œëŠ” slicingì´ì–´ì•¼ í•¨.

â¸»

4) ë¡œê¹… / í¬ë§· ì œí•œ (ì—„ê²©)
	â€¢	ì‹¤í–‰ ì¤‘ metrics.json í¬ë§·ì€ ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ˆ (ìƒˆ í•„ë“œ ì¶”ê°€ë„ ê¸ˆì§€)
	â€¢	ì‹¤í–‰ ì¤‘ repulsion.jsonl í¬ë§·ë„ ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ˆ
	â€¢	pruning ë¡œê·¸ëŠ” ìƒˆ íŒŒì¼ë¡œ:
	â€¢	run ë””ë ‰í† ë¦¬ ì•„ë˜ pruning.jsonl ìƒì„±/append
	â€¢	â€œevent 1ì¤„ JSONâ€ë¡œ ê¸°ë¡ (repulsion.jsonlê³¼ ë™ì¼ ìŠ¤íƒ€ì¼):
	â€¢	image_idx
	â€¢	prune_step (annealing step index, ì˜ˆ: 29)
	â€¢	prev_sigma, curr_sigma, repulsion_sigma_break
	â€¢	losses (len=4 float list, prune ì§ì „ ê¸°ì¤€)
	â€¢	kept_indices (len=2 int list, prune ì§ì „ batch index ê¸°ì¤€)
	â€¢	(ì„ íƒ) kept_losses (len=2)

ì¶”ê°€ë¡œ ìœ ìš©í•˜ì§€ë§Œ optional:
	â€¢	did_prune (true)
	â€¢	batch_before=4, batch_after=2
	â€¢	mode="slicing" ë˜ëŠ” "masking"

â¸»

5) ë°°ì¹˜ ê°ì†Œë¡œ ì¸í•œ ì €ì¥/trajectory/metric ì—ëŸ¬ ë°©ì§€ (í•„ìˆ˜ ì£¼ì˜)

pruning ì´í›„ batchê°€ 2ê°€ ë˜ë¯€ë¡œ ì•„ë˜ê°€ ê¹¨ì§€ê¸° ì‰¬ì›€:
	â€¢	ìƒ˜í”Œ/trajectory ì €ì¥ ë£¨í”„ê°€ range(num_samples) ê°™ì€ ê³ ì •ê°’ ì‚¬ìš©
	â€¢	ì´ë¯¸ì§€ ì €ì¥ì—ì„œ íŒŒì¼ëª…/ì¸ë±ìŠ¤ê°€ particle 0..3ì„ ê°€ì •
	â€¢	metric ì§‘ê³„ì—ì„œ â€œí•­ìƒ 4ê°œ ê²°ê³¼â€ë¥¼ ê¸°ëŒ€

ë”°ë¼ì„œ:
	â€¢	ì €ì¥/ì§‘ê³„ì—ì„œ **í•­ìƒ í˜„ì¬ batch size = tensor.shape[0]**ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë™ì‘í•˜ë„ë¡ ìˆ˜ì •í•´ì¤˜.
	â€¢	ë‹¨, pruningì´ ì—†ëŠ” ì‹¤í—˜(Exp0/1/3 ë“±)ì€ ë™ì‘/ê²°ê³¼ê°€ ê¸°ì¡´ê³¼ ì™„ì „íˆ ë™ì¼í•´ì•¼ í•¨:
	â€¢	pruning_step=-1ì´ë©´ ì½”ë“œ ê²½ë¡œê°€ ì‚¬ì‹¤ìƒ ì™„ì „íˆ bypass ë˜ì–´ì•¼ í•¨.

â¸»

6) ë¬¸ì„œí™” (PROJECT.md í•„ìˆ˜ ì—…ë°ì´íŠ¸)

PROJECT.mdì˜ Exp2 ì¤€ë¹„ ì„¹ì…˜ì— ë‹¤ìŒì„ ë°˜ë“œì‹œ ì¶”ê°€í•´ì¤˜:
	â€¢	pruning íŠ¸ë¦¬ê±°: ì™œ step29ì¸ì§€(Ïƒ ì „í™˜ ê·¼ê±° í¬í•¨)
	â€¢	pruningì´ â€œë”± 1íšŒâ€ ì‹¤í–‰ë˜ë„ë¡ í•œ ë¡œì§(did_prune, ì „í™˜ê°ì§€)
	â€¢	loss ê³„ì‚° ë°©ì‹(ì–´ë–¤ measurement lossë¥¼ ì‚¬ìš©í–ˆê³  ì…ë ¥/shapeê°€ ë¬´ì—‡ì¸ì§€)
	â€¢	slicing ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸(ì–´ë–¤ í…ì„œë“¤ì„ í•¨ê»˜ ì¤„ì˜€ëŠ”ì§€)
	â€¢	fallback masking ì˜µì…˜(ìˆë‹¤ë©´ ì–¸ì œ ì‚¬ìš©ë˜ëŠ”ì§€)
	â€¢	pruning.jsonl ì €ì¥ ìœ„ì¹˜ì™€ í¬ë§·
	â€¢	metrics.json/repulsion.jsonl í¬ë§· ë¶ˆë³€ ë³´ì¥

â¸»

7) êµ¬í˜„ í›„ â€œë°˜ë“œì‹œ í™•ì¸í•  ì²´í¬ë¦¬ìŠ¤íŠ¸â€(ê°„ë‹¨)
	â€¢	pruning.jsonlì— ê° ì´ë¯¸ì§€ë‹¹ 1ì¤„ë§Œ ê¸°ë¡ë˜ëŠ”ì§€(did_prune true 1íšŒ)
	â€¢	prune_stepì´ 29ì¸ì§€
	â€¢	batchê°€ ì‹¤ì œë¡œ 4â†’2ë¡œ ì¤„ì–´ë“œëŠ”ì§€(batch_after==2)
	â€¢	pruning ì´í›„ ì €ì¥/metricì—ì„œ shape mismatch ì—ëŸ¬ê°€ ì—†ëŠ”ì§€
	â€¢	Exp0/Exp1/Exp3 ì‹¤í–‰ ì‹œ ê²°ê³¼/ë¡œê·¸ í¬ë§·ì´ ë³€í•˜ì§€ ì•ŠëŠ”ì§€

â¸»

8) ì• ë§¤í•œ ê²°ì •ì‚¬í•­ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì§ˆë¬¸

ì˜ˆ: measurement lossë¥¼ ê³„ì‚°í•  í•¨ìˆ˜ ìœ„ì¹˜/ì…ë ¥, prune ì‹œì ì— ì ‘ê·¼ ê°€ëŠ¥í•œ í…ì„œ ëª©ë¡, ì €ì¥ ë¡œì§ì´ particle idë¥¼ ì–´ë–»ê²Œ ìœ ì§€í•˜ëŠ”ì§€ ë“±

#### ì‹¤í—˜2 Pruning êµ¬í˜„ ì™„ë£Œ (2025-12-14) â†’ **ì™„ë£Œ**

##### 1. Pruning íŠ¸ë¦¬ê±° (step 29)
```
ìš°ë¦¬ ì„¸íŒ… ê¸°ì¤€ (zero-indexed, num_steps=50, sigma_break=1.0):
  step 28: Ïƒ=1.0482 (Repulsion ON ë§ˆì§€ë§‰)
  step 29: Ïƒ=0.9525 (Repulsion OFF ì „í™˜ ì§í›„, ì²˜ìŒ Ïƒ < sigma_break)
```
- **ì „ëµ**: "repulsionì´ ëë‚œ ì§í›„ pruning" â†’ step 29
- **íŠ¸ë¦¬ê±° ì¡°ê±´** (ë‘˜ ì¤‘ í•˜ë‚˜ ë§Œì¡± + did_prune=False):
  1. **step ê¸°ë°˜**: `step == pruning_step` (ì‹¤í—˜ í†µì œìš©)
  2. **ì „í™˜ ê¸°ë°˜**: `prev_sigma >= sigma_break and curr_sigma < sigma_break` (ì•ˆì „ì¥ì¹˜)

##### 2. 1íšŒë§Œ ì‹¤í–‰ ë³´ì¥ ë¡œì§
- `did_prune` í”Œë˜ê·¸: ì´ˆê¸°ê°’ `False`, pruning ìˆ˜í–‰ í›„ `True`ë¡œ ì„¤ì •
- ì¡°ê±´ ì²´í¬: `if self.pruning_step >= 0 and not did_prune:`
- í•œ ë²ˆ ì‹¤í–‰ë˜ë©´ ì´í›„ stepì—ì„œëŠ” ì¡°ê±´ì´ ë§Œì¡±ë˜ì–´ë„ ë¬´ì‹œë¨

##### 3. Loss ê³„ì‚° ë°©ì‹
- **í•¨ìˆ˜**: `warpped_operator.loss(z0y, measurement)`
  - `warpped_operator`ëŠ” `LatentWrapper(operator, model)` ì¸ìŠ¤í„´ìŠ¤
  - ë‚´ë¶€ì ìœ¼ë¡œ latentë¥¼ decodeí•œ í›„ phase retrieval operator ì ìš©
- **ì…ë ¥**: `z0y` (í˜„ì¬ stepì˜ denoised latent), `measurement` (y batch)
- **ì¶œë ¥ shape**: `(B,)` - ê° particleë³„ ìŠ¤ì¹¼ë¼ loss

##### 4. Slicing ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸
Pruning í›„ ì•„ë˜ í…ì„œë“¤ì„ ëª¨ë‘ `kept_indices`ë¡œ ìŠ¬ë¼ì´ì‹±:
- `zt` (noisy latent)
- `z0y` (denoised latent after MCMC)
- `z0hat` (denoised latent from diffusion)
- `x0y` (decoded image after MCMC)
- `x0hat` (decoded image from diffusion)
- `xt` (decoded noisy image)
- `measurement` (y batch)
- `trajectory.tensor_data[*]` (ì´ì „ step ê¸°ë¡ë“¤ë„ í•¨ê»˜ ìŠ¬ë¼ì´ì‹± â†’ compile ì‹œ shape í†µì¼)

##### 5. Fallback Masking (ë¯¸êµ¬í˜„)
- í˜„ì¬ slicingë§Œ êµ¬í˜„ë¨
- í•„ìš”ì‹œ masking ê²½ë¡œ ì¶”ê°€ ê°€ëŠ¥ (íƒˆë½ particleì€ update ì—†ì´ carry)

##### 6. pruning.jsonl ì €ì¥ ìœ„ì¹˜ ë° í¬ë§·
- **ìœ„ì¹˜**: `results/<run_name>/pruning.jsonl`
- **í¬ë§·** (ì´ë¯¸ì§€ë‹¹ 1ì¤„):
```json
{
  "image_idx": 0,
  "prune_step": 29,
  "prev_sigma": 1.0482,
  "curr_sigma": 0.9525,
  "repulsion_sigma_break": 1.0,
  "losses": [0.1234, 0.5678, 0.2345, 0.3456],
  "kept_indices": [0, 2],
  "kept_losses": [0.1234, 0.2345],
  "batch_before": 4,
  "batch_after": 2,
  "did_prune": true,
  "trigger": "step",
  "mode": "slicing"
}
```

##### 7. metrics.json / repulsion.jsonl í¬ë§· ë¶ˆë³€ ë³´ì¥
- **metrics.json**: ê¸°ì¡´ í¬ë§· ìœ ì§€ + `vram` í•„ë“œ ì¶”ê°€ (segments ê¸°ë°˜ êµ¬ê°„ë³„ VRAM)
  ```json
  "vram": {
    "peak_memory_mb": 10209.0,
    "segments": {
      "pre_pruning": 10150.0,
      "post_pruning": 6100.0
    }
  }
  ```
  â†’ í–¥í›„ Exp4 optimization ì¶”ê°€ ì‹œ `segments.optimization` ì¶”ê°€í•˜ë©´ ë¨
- **repulsion.jsonl**: ê¸°ì¡´ í¬ë§· ìœ ì§€, ìƒˆ í•„ë“œ ì¶”ê°€ ì—†ìŒ
- **pruning ë¡œê·¸**: ë³„ë„ íŒŒì¼ `pruning.jsonl`ë¡œ ë¶„ë¦¬

##### 8. ì €ì¥ êµ¬ì¡°
```
results/<run_name>/
â”œâ”€â”€ samples/                    # ìµœì¢… ìƒ˜í”Œ (pruning í›„ 2ê°œ)
â”‚   â”œâ”€â”€ 00000_sample00.png
â”‚   â””â”€â”€ 00000_sample01.png
â”œâ”€â”€ trajectory/                 # ì‚´ì•„ë‚¨ì€ particleë“¤ì˜ ì „ì²´ trajectory (step 0~49)
â”‚   â”œâ”€â”€ 00000_sample00.png
â”‚   â””â”€â”€ 00000_sample01.png
â”œâ”€â”€ trajectory_pruned/          # íƒˆë½í•œ particleë“¤ì˜ trajectory (step 0~29ê¹Œì§€ë§Œ)
â”‚   â”œâ”€â”€ 00000_pruned00.png
â”‚   â””â”€â”€ 00000_pruned01.png
â”œâ”€â”€ pruning.jsonl              # pruning ë¡œê·¸
â”œâ”€â”€ repulsion.jsonl            # repulsion ë¡œê·¸
â””â”€â”€ metrics.json               # í‰ê°€ ê²°ê³¼
```

##### 9. ìˆ˜ì •ëœ íŒŒì¼
- `commands_gpu/exp2_pruning.sh`: PRUNING_STEP=29, ì£¼ì„ ì¶”ê°€
- `sampler.py`: pruning ë¡œì§ êµ¬í˜„ (íŠ¸ë¦¬ê±°, loss ê³„ì‚°, slicing, ë¡œê¹…, pruned_trajectory ì €ì¥, VRAM êµ¬ê°„ ì¸¡ì •)
- `posterior_sample.py`: pruning.jsonl ì €ì¥, ë™ì  ë°°ì¹˜ í¬ê¸° ëŒ€ì‘, pruned trajectory ì €ì¥, VRAM ì •ë³´ metrics.jsonì— ê¸°ë¡

##### 10. êµ¬í˜„ í›„ ì²´í¬ë¦¬ìŠ¤íŠ¸ âœ… ì™„ë£Œ (2025-12-14 19:30 KST)
- [x] pruning.jsonlì— ê° ì´ë¯¸ì§€ë‹¹ 1ì¤„ë§Œ ê¸°ë¡ë˜ëŠ”ì§€(did_prune true 1íšŒ) âœ…
- [x] prune_stepì´ 29ì¸ì§€ âœ…
- [x] batchê°€ ì‹¤ì œë¡œ 4â†’2ë¡œ ì¤„ì–´ë“œëŠ”ì§€(batch_after==2) âœ…
- [x] pruning ì´í›„ ì €ì¥/metricì—ì„œ shape mismatch ì—ëŸ¬ê°€ ì—†ëŠ”ì§€ âœ…
- [x] Exp0/Exp1/Exp3 ì‹¤í–‰ ì‹œ ê²°ê³¼/ë¡œê·¸ í¬ë§·ì´ ë³€í•˜ì§€ ì•ŠëŠ”ì§€ âœ… (ì½”ë“œ êµ¬ì¡° ìœ ì§€ë¨)
- [x] trajectory_pruned/ í´ë”ì— íƒˆë½í•œ particle trajectoryê°€ ì €ì¥ë˜ëŠ”ì§€ âœ…
- [x] metrics.jsonì— vram.segments.pre_pruning, vram.segments.post_pruningì´ ê¸°ë¡ë˜ëŠ”ì§€ âœ…
- [x] segments.post_pruning < segments.pre_pruning (ë©”ëª¨ë¦¬ ì ˆì•½ í™•ì¸) âœ… (6068 < 10209, **40% ì ˆì•½**)

##### 11. ì‹¤í–‰ ëª…ë ¹ì–´
```bash
# Exp2 (4â†’2 Pruning) sanity check
bash commands_gpu/exp2_pruning.sh --1
# â†’ results/exp2_pruning/imagenet_1img/exp2_sanity_check_scale10_prune29/
```